{"/jekyll/2025-09-13-Big_Number_Multiplication.html": {
    "title": "Algorithm | Big Number Multiplication",
    "keywords": "algorithm leetcode Jekyll",
    "url": "/jekyll/2025-09-13-Big_Number_Multiplication.html",
    "body": "Algorithm | 大數乘法 在演算法題目中，「大數乘法（Big Number Multiplication）」一直是經典卻讓人頭痛的練習題。 雖然數學上的乘法概念人人熟悉，但一旦限制不能用內建的大數類別（像 Java 的 BigInteger 或 Go 的 math/big），就只能乖乖用字串模擬直式乘法。 尤其是 LLM 可以使用之後，更沒耐心去寫這種繁瑣的程式碼了，趕快複習一下。 實作的題目可以找 Hackerrank. Extra Long Factorials, 43. Multiply Strings 來練習。 Why Care About Big Number Multiplication 程式語言內建型別有位元上限，面試或競賽題常要求你在不使用內建大數的情況下處理超大整數。 最基本的做法就是使用直式乘法搬到字串或陣列上來做：用低位對低位、進位處理，最後組合結果。 對於大數乘法有更多的演算法（Karatsuba, Toom-Cook, FFT-based），但這些通常超出面試範圍，且實作複雜。 【算法】大数乘法问题及其高效算法 Problem Statement 輸入兩個非負整數的字串 num1 和 num2，回傳它們相乘的結果字串。 不允許使用直接把字串轉成大數、或使用語言提供的大數類別。 限制要點： 輸入大小可能超過原生整數範圍 要處理零與前導零情況（”0” × 任意 = “0”） Basic Idea 把每個字元轉成整數，從低位（字串尾）開始模擬逐位相乘。 兩位數相乘的結果加到對應的結果陣列位置，再處理進位。 最後把結果陣列轉回字串並去除前導零。 時間複雜度：O(m * n)，m 和 n 分別為兩個輸入字串長度。 空間複雜度：O(m + n)（用來存放結果的陣列）。 Algorithm Steps 23958233 × 5830 ——————————————— 00000000 ( = 23,958,233 × 0) # 1. Multiplying by 0 71874699 ( = 23,958,233 × 3) # 2. Multiplying by 3 191665864 ( = 23,958,233 × 8) # 3. Multiplying by 8 + 119791165 ( = 23,958,233 × 5) # 4. Multiplying by 5 ——————————————— 139676498390 ( = 139,676,498,390 ) # Final result maximum M + N digits 我們以直立式乘法為例，可以看到被拆成以下階段： 準備乘數與被乘數 逐次乘以被乘數的每一位，並將結果依位數對齊 被乘數有 N 位數，就會產生 N 次計算 將所有部分和相加得到最終結果 最後的值會有最多 M + N 位數（需要使用另一個陣列來存放） Algorithm: 若其中一個字串為 “0”，直接回傳 “0”。 建立長度為 m + n 的整數陣列 result，初始化為 0。 反向遍歷 num1，對每個位 i： 反向遍歷 num2，對每個位 j： product = (num1[i] - ‘0’) * (num2[j] - ‘0’) sum = product + result[i + j + 1]（+ 可能的進位） result[i + j + 1] = sum % 10 result[i + j] += sum / 10 把 result 陣列轉成字串，跳過開頭的零，並回傳。 這裡 i + j + 1 與 i + j 的索引對應到「從高位到低位」排列的陣列格局。 Pitfalls and Tips 索引錯誤（i + j、i + j + 1）是常見 bug 根源。 千萬不要把字元直接當數字相加（記得減 ‘0’）。 處理前導零：例如 “000123” × “045” 的結果需正確處理。 大量的字串拼接會影響效能，使用陣列或 StringBuilder（Java）更好。 Implementation in Python def multiply(num1: str, num2: str) -&gt; str: if num1 == \"0\" or num2 == \"0\": return \"0\" m, n = len(num1), len(num2) res = [0] * (m + n) for i in range(m - 1, -1, -1): a = ord(num1[i]) - ord('0') for j in range(n - 1, -1, -1): b = ord(num2[j]) - ord('0') prod = a * b sum_ = prod + res[i + j + 1] res[i + j + 1] = sum_ % 10 res[i + j] += sum_ // 10 # skip leading zeros idx = 0 while idx &lt; len(res) and res[idx] == 0: idx += 1 return ''.join(str(d) for d in res[idx:]) if idx &lt; len(res) else \"0\" Last Edit 09-14-2025 01:50"
  },"/jekyll/2025-09-12-Binary_Search_Tricks_and_Hacks.html": {
    "title": "Algorithm | Binary Search Tricks and Hacks",
    "keywords": "algorithm leetcode Jekyll",
    "url": "/jekyll/2025-09-12-Binary_Search_Tricks_and_Hacks.html",
    "body": "Algorithm | Binary Search 的奇淫巧技 Binary Search 是一個對於計算機科學學生來說非常基礎的演算法，但是在實際的程式競賽或是面試中，Binary Search 往往會被用在一些比較特別的情境中， 這些情境往往會讓人感到困惑，這篇文章會介紹一些 Binary Search 的技巧和應用，並且透過一些範例來說明這些技巧和應用。 Last Edit 09-14-2025 01:50"
  },"/jekyll/2024-10-17-KLEE_symbolic_execution.html": {
    "title": "Paper | KLEE: Introducing Symbolic Execution",
    "keywords": "software software_qualitiy symbolic_execution Jekyll",
    "url": "/jekyll/2024-10-17-KLEE_symbolic_execution.html",
    "body": "Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. 2008. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. In 8th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2008, December 8–10, 2008, San Diego, California, USA, Proceedings, Richard Draves and Robbert van Renesse (Eds.). USENIX Association, 209–224. http://www.usenix.org/events/osdi08/tech/full_papers/cadar/cadar.pdf KLEE 是在符號執行 (Symbolic Execution) 領域中的成功案例，這裡介紹 2008 年 KLEE 的原始論文，以此來了解符號執行的基本概念。 1. Introduction Program Analysis Tools : 軟體分析工具具體可以分為兩類，Dynamic Analysis 和 Static Analysis，簡單可以理解為： Dynmaic Analysis: 需要具體的執行 Binary code，透過執行過程中的資訊來進行分析 例如：gdb, valgrind Static Analysis: 只需要具體的 Source code，不需要執行 Binary code 例如：pylint, clang-tidy, clang-format Symbolic execution 也是靜態分析的一種，透過對變數的符號化來進行分析該變數的可能值 Why we need Symbolic execution? 首先 Testing 對於大部分程式開發過程中都是一個 Pain point，並且編寫 Testcase 相較於 Development 來說往往讓人感到 ennui。 並且手動測試往往是不夠的，如果一個 Bug 早就已經知道他是存在的，那就應該不叫做 Bug。 即使手動編寫 Unit Test，也往往無法達到足夠的覆蓋率，到目前也沒有一種良好的規範化的方法來產生 Testcase， 因此我們需要一種 Automation 的方法來產生 Testcase，這就是 Symbolic execution 的目的。 並且在 C 中 assert 往往可能在 -o2 -o3 的編譯器優化中被去除，導致這些 assert 在正式的環境中是無效的 Symbolic Execution Advantages: 對於一個程式覆蓋所有可能的分支路徑 檢查可能產生危險操作的分支路徑, e.g. buffer overflow, null pointer dereference 自動產生的測試案例 不需要實際運行程式 History of Symbolic Execution: Symbolic execution 這個概念最早是由 King 在 1976 年提出，但當時受限於 SMT Solver 還無法實際應用在實際的程式中。 直到 2000 年左右各種 SMT Solver 開始發展，Symbolic execution 才開始有了實際的應用。 KLEE 是在 2008 年發表於 OSDI，但在之後的數十年中，KLEE 是在該領域中的一個成功案例，並且舉辦了自己的 Workshop。 2. KLEE Example 1 : void expand(char *arg, unsigned char*buffer) { 8 2 : int i, ac; 9 3 : while (*arg) { 4 : if (*arg == ’\\\\’) { 10* 11* 5 : arg++; 6 : i = ac = 0; 7 : if (*arg &gt;= ’0’ &amp;&amp; *arg &lt;= ’7’) { 8 : do { 9 : ac = (ac &lt;&lt; 3) + *arg++− ’0’; 10: i++; 11: } while (i&lt;4 &amp;&amp; *arg&gt;=’0’ &amp;&amp; *arg&lt;=’7’); 12: *buffer++ = ac; 13: } else if (*arg != ’\\0’) 14: *buffer++ = *arg++; 15: } else if (*arg == ’[’) { 12* 16: arg++; 13 17: i = *arg++; 14 18: if (*arg++ != ’-’) { 15! 19: *buffer++ = ’[’; 20: arg−= 2; 21: continue; 22: } 23: ac = *arg++; 24: while (i &lt;= ac) *buffer++ = i++; 25: arg++; /* Skip ’]’ */ 26: } else 27: *buffer++ = *arg++; 28: } 29: } 30: . . . 31: int main(int argc, char* argv[ ]) { 1 32: int index = 1; 2 33: if (argc &gt; 1 &amp;&amp; argv[index][0] ==’-’) { 3* 34: . . . 4 35: } 5 36: . . . 6 37: expand(argv[index++], index); 7 38: . . . 39: } TBC … Last Edit 10-26-2024 19:48"
  },"/jekyll/2024-07-28-parallel_and_distributed_systems_introduction.html": {
    "title": "PDS | Introduction of Parallel and Distributed Systems",
    "keywords": "PDS Jekyll",
    "url": "/jekyll/2024-07-28-parallel_and_distributed_systems_introduction.html",
    "body": "這邊紀錄一系列的筆記關於 Parallel and Distributed Systems (PDS) 的課程，主要是希望自己更加了解這個領域，並且在暑假期間能做出一個小的 Demo 來練習。 1. Introduction 分散式系統的具體學習目標，從理論到中間軟體再到實際應用 隨著軟體規模、性能需求的提升，透過低成本多節點的方式來提升系統的效能已經成為一個趨勢。並且分散式在降低硬體成本的同時，也提升了軟體的可靠性、擴展性等等， 但也因此分散式要處裡許多新的技術問題。 分散式系統需要分散式理論跟演算法作為基礎: CAP theorem BASE Paxos Algorithm Two-phase commit protocol Three-phase commit protocol 如果不能理解這些基礎的理論，將會對架構跟開發工作帶來困擾 Technical solutions(技術解決方案): Distributed Lock Distributed Transaction Service Discovery Service Protection Service Gateway Middleware(中間軟體): Distributed Coordination Middleware: Zookeeper, etc. Service Governance Middleware: Dubbo, Eureka, etc. 如果對於這些 Middleware(中間軟體)的功能或者實現原理不了解，同樣會增加開發的困難度 1.2 Application History 1.2.1 Single Application 1.2.2 Cluster Application 1.2.3 Narrowly Distributed Applications 1.2.1 Single Application 最初的應用形式即是部屬在一台機器的單一應用，其中可以包含很多很多 Module，內部的 Module 之間是高度耦合的，但在開發、測試、部屬的成本低廉。 隨著功能與併發數量的提升，通常會帶來兩個挑戰: Hardware 上龐大的單體應用需要更多的資源，因此單一主機的成本會提升 Software 應用內部的 module 之間耦合度高，隨著功能的增加，會導致開發維護變得困難 因此當應用的功能與併發數量提升到一定程度時，就要考慮拆分應用，就演變成了 Cluster Application。 1.2.2 Cluster Application 使用 Cluster 那就要面對在 Cluster 中的節點之間的協作問題，這也是分散式系統的核心問題 Cluster Application 可以對應用的併發與容量進行分散，這裡的 Cluster 包含多個「同質」(Homogeneous) 的應用節點， 這裡會使用「同質」是因為這些節點的功能是一樣的，執行相同的程式、相同的設定，就像是一個副本複製出來的一樣。 例如透過反向代理去做 Load Balance，把外部請求分發到不同的節點上。 上圖是一個 Kubernetes 的架構，透過 Load Balance 來分發請求到不同的 Pod 就可以提升應用的併發量。 但 Homogeneous Cluster 帶來最明顯的問題是，同一個使用者發出的請求可能會被不同的節點處理，導致服務可能不連貫。 為了解決這種問，演化出了幾種方案。 Stateless Cluster Nodes Stateless cluster nodes(無狀態叢集節點) 是指任何請求的結果都和該節點之前所處理的請求無關。 例如簡單的 Web Server，每次都回傳相同的 Page，這樣就不需要考慮請求的連貫性，對於使用者來說這個頁面是 stateless。 但是即使是 Stateless 也需要考慮協作，例如: Stateless cluster node 需要每天早上發送一封 Mail 給外部的使用者， 我們希望的是只有一個節點發送這封 Mail，如果沒有考慮到這個問題，可能會導致每個 Node 都發送一封 Mail 給使用者。 可能的解法: 設計一個外部的請求，由該外部請求來請求發送，這樣就可以保證只有一個節點被分配到這個任務 Stateless node 設計簡單，可以很方便的擴充，但是也因為要是 Stateless 所以會有很大的侷限性。 Single Cluster Node 雖然 Stateless 很好用，但是畢竟有很多服務是需要狀態的，例如: 聊天室、購物車… Key words: Fixed allocation, Session affinity 這些服務在 Single application 是很好實現的，但在 Cluster 中就變得複雜起來，一個最簡單的辦法就是在使用的 Node 和 User 之間建立聯繫。 User 可以訪問固定的 Node，這樣就可以保證 User 的狀態是連續的。 任意 User 都有一個對應的 Node，在 Node 上保存使用者的 Context information User 的請求總是被分配到對應的 Node 上 想要實現這些功能的方法有很多，例如: 線上遊戲由使用者自己選擇 Server、透過帳號來分配 Server 等等 … 當然這樣的方法在各個 Node 之間是互相隔離的，因此在容錯上會比較差。 Shared Information Cluster Node Shared Information Cluster Node (共用資訊叢集節點) 是為了解決資訊共享的問題，如果有一個共享的資訊庫，那麼所有的 Node 都可以訪問這個資訊庫， 這樣就不用去擔心 Node 崩潰後資訊的丟失，例如下圖: 例如: 可以使用 Redis 來做共享記憶體，儲存使用者的 Session 等等。 但是同樣的共享資訊池也會成為一個瓶頸，因為所有的 Node 都要訪問這個資訊池。 Consistent Information Cluster Nodes 為了避免資訊池成為瓶僅，那就橫向擴展資訊池，這樣就可以降低資訊池的壓力，但同樣就會有 Consistent(一致性) 的問題， 例如部分 Node 共用一個 Information Pool，Pool 之間去做一致性。 部分 Node 共用同個 Pool，或者乾脆 Pool 加入 Node，在 Node 之間做 Consistent 要注意 Consistent 的實施成本非常巨大，你可以想像如果有 1000 個 Pool，那麼每次更新都要通知 1000 個 Pool，這樣的成本是非常高的。 因此 Information-consistent clusters 適合用在讀多寫少的場景。 1.2.3 Narrowly Distributed Applications Cluster Application 是為了解決併發數量和使用量，並不能減少程式自身的問題 狹義上的分散式系統是指原本是一個 Single application，但是隨著規模的擴大遭遇以下問題: 硬體成本, 應用性能, 業務邏輯複雜, 變更維護複雜, 可靠性變差 這些問題都不能透過將 Single application 轉變為 Cluster application 解決時，去把 Single application 拆分成多個獨立的 Sub-application， 讓每個 Sub-application 都是可以獨立運行的，這樣就把 Single application 變成了 Narrowly distributed applications。 因此每個 Node 上運行的 Application 都是 Heterogeneous 的，可以獨立的開發、部屬、升級、維護。 把這些 Sub-Application 擴展成 Cluster 這樣我們也可以把這些 Sub-Applcation 部署到不同的機器上，擴展成一個更大的 Cluster，也就是 Distrubuted System， 也可以依照 Sub-Application 的需求進行 Scaling，更為靈活與高效。 1.2.4 Microservices 在 Narrowly Distributed Applications 的基礎上，Sub-Application 之間有嚴格的從屬關係，這種關係可能會造成資源的浪費。 這裡舉個浪費的例子: 存在 Application A, 包含 3 個 Sub-Application: A1, A2, A3，有可能 A 的某個功能只需要 A1, A2， 但是 A3 就會被閒置，這樣就會造成資源的浪費。 Microservices 是一種 Architecture patte，也就是每個 Microservices 都是完備的，可以獨立對外提供服務， 這樣就可以針對每個 Microservices 進行 Scaling、Resource Allocation。 1.3 Distributed System Introduction 如果所有的服務都只能作為系統的一部分聯合起來對外提供服務，那麼這樣的系統就是 Narrowly Distributed Applications， 如果這些服務可以獨立對外提供服務，那麼這樣的系統就是 Microservices。 1.3.2 Distributed System Consistency Problem 最簡單的一致性問題如下，假如 Request 是有序的，那麼我們應該要確保 R1 修改完 a 之後 R2 讀取的 a 是修改後的值。 如果一個系統會發生上面的問題，那麼這個系統就是不一致的(至少是線性不一致 Linearizability Inconsistency)， 關於一致性的分級會在後續章節提到，這裡只是提一下。 1.3.3 Distributed System Node 在 Distributed System 中，Node 可能是同質的，也可能是異質的。 在同質節點中，當系統發生變更時，所有的節點的變更是一樣的，例如: Zookeeper cluster 收到 Client 發送建立 znode 的請求後， 每個節點都需要建立 znode。 異質節點中，當應用發生變更時，每個節點的變更是不一樣的，例如: 一個網購平台的系統，訂單節點需要建立訂單，庫存節點需要減少庫存。 這也是一種 Consistent 的問題，要確保整個系統的 Consistent。 1.4 Distributed System Advantages &amp; Disadvantages 1.4.1 Advantages Reduced Cost: 降低系統的實施成本是分散式系統的發展與最初動力，對於硬體強大的大型主機，可以用多個小型叢集上取代 Improved Availability: 提升系統的可用性，即使某個節點失效，也不會影響整個系統 Improved Performance: 提升系統的併發與容量，提升系統的性能 Reduced Maintenance: 由於分散式系統的多個節點，可以降低系統的維護成本，例如: 可以對某個節點進行維護，而不影響整個系統 並且因為節點之間是獨立的，可以獨立的平行開發與模組化 1.4.2 Disadvantages Consistency: 一致性問題是分散式系統的核心問題，要保證系統的一致性是非常困難的，所以在下個章節會先介紹一致性問題 Node discovery problem: 在有許多動態變化的節點時，怎麼發現系統中的可用節點是一個問題 Node call problem: 當系統中有許多節點時，在節點之間的呼叫是經常發生的，但它們也有與之對應的成本 Node coworking problem: 當系統中有許多節點時，節點之間的協作是一個問題，例如: 如何在同質節點中保證一個 Task 只被一個節點執行 異質節點也需要解決 Producer-Consumer 問題，例如: 訂單節點需要建立訂單，庫存節點需要減少庫存 這張主要是 Introduction 所以就簡單介紹 Distributed System 的優缺點與發展歷程，後續章節會更深入的介紹 Distributed System。 Last Edit 7-30-2024 19:48"
  },"/jekylls/2024-06-21-competitive_paging_algorithm.html": {
    "title": "Algorithm | Competitive Paging Algorithm",
    "keywords": "Algorithm Jekylls",
    "url": "/jekylls/2024-06-21-competitive_paging_algorithm.html",
    "body": "Notes for Marking algorithms Analysis, Reference: [1] Fiat, A., Karp, R. M., Luby, M., McGeoch, L. A., Sleator, D. D., &amp; Young, N. E. (1991). Competitive paging algorithms. [2] Achlioptas, D., Chrobak, M., &amp; Noga, J. (2000). Competitive analysis of randomized paging algorithms. 1. Introduction 這篇介紹 Competitive Paging Algorithm，Marking Algorithm 是一種 Online Paging Algorithm，並且是 Randomized Algorithm， 主要目的是用來說明分析這些 Online Paging Algorithm 的工具。詳細的原始論文可以參考 [1] 和 [2]。 所以 Marking Alogrithm 並不是一個實際在 OS 中被廣泛使用的演算法，但透過這對於 Marking Algorithm 的分析，可以更好的了解 Competitive Algorithm 的特性。 例如: LRU(Last Recently Used) 就是一種 Marking Algorithm，FIFO(First In First Out) 則不是。 [1] 是首次提出 Competitive Paging Algorithm 與 Marking Algorithm 分析的論文 [2] 則在前三章有對於 Marking Algorithm 的更詳細與緻密的分析 2. Paging Problem 從最開始的 Paging Problem Definition 開始到 Offline Optimal Algorithm, Marking Algorithm 的介紹。 Paging Problem Definition: A two-level memory system, capable of holding $K$ items in the Cache. At each time step, a request to an item is issued. If the item $p$ exists in the Cache, then the cost is 0. If the item $p$ does not exist in the Cache: Choose an item $q$ to replace it, and the cost is 1. 這個問題是一個 Online Problem，因為沒有辦法知道未來的 Request，所以只能根據目前的 Request 來做決策。 但是我們可以先從 Offline Algorithm 開始，來了解這個問題的 Optimal。 2.1 Optimal Algorithm 目前被證明的 Belady’s Optimal Algorithm 被證明是 Optimal Algorithm Belady’s Optimal Algorithm 也被稱為 Longest Forward Distance(LFD) Algorithm 簡單來說就是把未來最久才會被 Request 的 Item replace Example: Cache Size = 3, Request Seq = { 1,2,3,4,1,2,5,1,2,3,4,5 } 關於 Optimal Algorithm 的證明可以參考 stack overflow:Proof for optimal page replacement (OPT)，使用 Contradiction 來證明 Optimal Algorithm 是最佳的。 但是在現實中我們無法得知一個 item 多久才會被 Request，這個演算法也被稱為 clairvoyant replacement algorithm，因為要做到 Optimal 需要透視未來的 Request。 2.2 Marking Algorithm 這邊我們先給出一個基本的 Marking Algorithm，這是從 [1] 中提出的。 假設有一個 Cache，跟大小相同的 Marking Bit Array，用來記錄 p 是否在 Cache 中 發生 Request 時，如果 p 已經被 Marked，則 Cost = 0 如果 p 沒有被 Marked，則 Cost = 1 Total marked item &lt; k，則 Mark p Total marked item = k，則 Unmark all items，然後再 Mark p Example: Cache Size = 3, Request Seq = { 1,2,3,4,1,2,5,1,2,3,4,5 } 這個 Seq 是故意設計的最糟情況，可以看到 Marking Algorithm 不斷填滿 Array，然後清空。注意我們的重點在於對 Marking Algorithm 的分析，而不是實際的使用。 3. Preliminaries 在分析上會以 [2] 為主要的論文，相較於 [1] 在 1991 剛被提出，到了 [2] 2000 年已經有更多的分析工具。 Throughout this node: $k$ denote the size of the cache $ALG(𝛼)$ denote the online algorithm Definition. An online algorithm $ALG$ is $c-competitive$ if, for any instance $I$, $ALG(I) \\leq c \\cdot OPT(I)$. 先定義 $c-competitive$ 的概念，$OPT(I)$ 是最佳的，而 $ALG(I)$ 是 Online Algorithm $\\text{If c = 1, ALG is equivalent to OPT}$ 3.1 Bounds k-competitive 首先我們先證明任何 Online Algorithm 都是 $k-competitive$，證明其最糟情況下至少是 OPT 的 $k$ 倍。 Theorem. For any $k$ and any deterministic online algorithm $ALG$, the competitive ratio of $ALG \\geq k$. A always request the page that is not currently in the cache, This causes a page fault in every access. The total cost of $ALG$ is $|𝛼|$ The total cost of $OPT$ is at most $|𝛼|/k$ Becase $OPT$ can only a single page fault in any $k$ accesses The base competitive ration is $k$ 簡單來說有一個無論如何都會產生 Page fault 的輸入，這樣就可以證明任何 $\\text{ALG}$ 最糟情況下都是 $k-competitive$。 3.2 Potential Method 在 Randomized online algorithm 的分析中常常會使用 Amortized Analysis，Paging Problem 也是如此， [2] 中就使用了 Potential Method 來分析 Marking Algorithm 的 Upper Bound。 Each operation has state $w \\text{(work function), } A\\text{(ALG conguration), } \\text{r(request)}$ $Δcost_A$ is the cost of $ALG$ $Δopt$ is the cost of $OPT$ $Δϕ$ is the potential function change 首先定義每次的操作都會有狀態 $w, A, \\text{r}$，然後定義 $Δcost_A, Δopt, Δϕ$ 來分析每次操作的成本。這樣我們就能給出公式: \\[Δcost_A + Δϕ \\leq c \\cdot Δopt\\] 這個公式就是 Competitive Ratio 的定義，每次操作的成本加上 Potential Function 的變化都應該小於等於 $c$ 倍的 $OPT$。 3.3 Work Function Work Function 是一個用來描述每次操作的狀態或者是成本的函數，這裡定義 $w(A)$ 來描述 $A$ 的狀態 Lemma. Every offset function is coned up from the set of configurations for which its value is zero. Moreover, if $\\omega$ is the current offset function and $r$ is the last request, then there is a sequence of sets $L_1, L_2, \\ldots, L_k$, with $L_1 = r$, such that $\\omega(X) = 0$ if and only if $|X \\cap \\bigcup_{i \\le j} L_i| \\ge j$ for all $1 \\le j \\le k$. 這裡的 $L$ 代表的是一組 Request 的集合，稱作 Layer，例如: $w(X)=0$ 代表之前的輸入都在 Cache 中，所以 Cost = 0，同樣也有該輸入的 item 數量一定比 $k$ 小。 這裡定義三種集合: $V(w) \\text{ is mean valid, no cost occurs so the cost is 0}$ $S(w)=⋃_{i \\leq j}L_i \\text{ is the set of all requests for this work function}$ $N(w) \\text{ is non-revealed item in } S(w)$ After Request 然後是在 Request 之後的狀態變化，這裡定義 $\\omega’$ 來描述 Request 之後的狀態變化。 \\[Let \\ \\omega = (L_1 | \\cdots | L_k) \\text{, If} \\ r \\text{ is a new request.}\\] \\[\\omega' = \\begin{cases} (r | L_1 | \\cdots | L_{j-1} | L_j \\cup L_{j+1} - r | L_{j+2} | \\cdots | L_k) &amp; \\text{if } r \\in L_j \\text{ and } j &lt; k, \\\\ (r | L_1 | \\cdots | L_{k-1}) &amp; \\text{if } r \\in L_k, \\\\ (r | L_1 | L_2 | L_3 | \\cdots | L_k) &amp; \\text{if } r \\notin S(\\omega). \\end{cases}\\] $r \\in L_j$ 中，則將 $r$ 移到最前面，而 $L_j$ 之後的 Layer 都移除 $r$ 並且加入到 $L_{j+1}$ $r \\in L_k$ 中，則將 $r$ 移到最前面，並且移除 $L_k$ $r \\notin S(\\omega)$，則將 $r$ 移到最前面 在之後的分析中，我們會使用這個 Work Function 來描述每次操作的狀態變化，下面會給出一個例子。 這裡也能看處 Request 實際上就 3 種情況 example: $\\text{Let } k = 3,\\text{ {a,b,c} in cache, Request = {d,e,b}}$ \\[(a|b|c) \\rightarrow (d|a,b|c) \\rightarrow (e|a,b,d|c) \\rightarrow (b|e|a,c,d) \\\\\\] $\\text{The optimal cost is 2.}$ 4. Analysis 首先我們已經清楚了 Marking Algorithm 的過程，所以有兩個 $Fact$: Fact 1. The cache contains only marked items and active items. All marked items are in the cache. If there are $m$ marked items and $v$ active items then each active item is in the cache with probability $(k-m)/v$. Fact 2. Let $w = (L_1|L_2|…|L_k)$ If $L_i$ contains a marked item then all item in $⋃_{j \\leq i}L_j$ are marked. Fact 1 可以得知 Cache 中的 Active Item 在 Cache 中的機率 Fact 2 則是說明如果 Layer 中有 Marked Item，則之後的 Layer 都會有 Marked Item Theorem 2. The competitive ratio of the marking algorithm is $2H_k - 1$. 4.1 Lower Bound 透過最糟情況的設計，來證明 Competitive Ratio 的 Lower Bound。 Proof. A cycle of request $k+1$ items, where the optimal cost 1, while the cost on the Marking Algorithm is $2H_k - 1$. $w = (x_1|x_2|…|x_k)$, Active set $X = {x_1, x_2, …, x_k}$, Marked set $M = {x_1}$ $w^y = (y|x_1, x_2| … |x_k)$, Marked set $M = {y, x_1}$ Continuous request $x_2, …, x_{k-1}$ When ends $w^{k-1}=(x_{k-1}|…|x_2|y|x_1,x_k)$ Marked set $M = {x_{k-1},…,x_2,x_1,y}$, size $k-1$ Last Request $x_k$ Marked set is full, so unmark all items. The cost must be 1. \\[1 + \\frac{2}{k} + \\frac{2}{k-1} + \\cdots + \\frac{2}{3} + \\frac{2}{2} = 2H_k - 1\\] 先設計一個 k+1 個 Request，首先 cache 中已經存在 $x_1$。首先 Request $y$ 然後再 Request $x_2, …, x_k$， 因為是從 $k_2, …, x_k$ 不斷去請求，因此分母會不斷減少，最後一次 Request $x_k$ 則必然會產生 Cost = 1。以此得到以上的公式，是一個 Harmonic Number， 所以可以得到 $\\text{Competitive Ratio} = 2H_k - 1 \\approx 2ln(k) - 1$。 4.2 Upper Bound 在 [2] 中更使用 Potential Method 來證明 Competitive Ratio 的 Upper Bound，證明了 $2H_k - 1$ 是 Tight Proof. Prove that Marking Algorithm is $2H_k - 1$ competitive, using Potential Method. 如果當前的 Work Function 有 $s$ 個 Layer，且包含未標記的 Item，則 Potential Function 為: Potential Function $ϕ(w) = s(H_k - H_s + 1)$ $H_k, H_s$ is Harmonic Number of $k, s$ $s$ is the number of layers for work function $w$ 首先把 Request 分成 3 種情況: (a) Request outside the $S(w)$, $t$ is the number of (a) (b) Request in the $S(w)$ but not in the cache, $l$ is the number of (b) (c) Request in the cache 最後是我們證明的目標: \\[Δcost_A + Δϕ \\leq (2H_{k-1}) \\cdot Δopt\\] 完成以上的定義後我們就能來分析 Request 的情況。 Class (a) $r \\notin S(w)$, 這裡給個例子來說明什麼是 Class (a) 的 Request: Example $k = 3, M = \\text{{a, b, c}}, \\text{Request} = \\text{{d}}$ $(a|b|c) \\rightarrow (d|a,b|c)$ 這裡可以看出 (a) 其實就是一定會產生 Cost 的 Request，並且從此我們能看出 $Δopt = t$，實際上 $t$ 就是 OPT 的 Cost。 Class (b) $r \\in S(w), r \\notin M(w)$, 同樣用例子來說明 Class (b) 的 Request: Example $k = 3, M = \\text{{a, b}}, Request = \\text{{c}}$ $(a|b|c) \\rightarrow (c|a,b|)$ 這裡根據 Fact 2. Class (b) 可以使 $s$ 減少 1，所以將會有 $l \\leq s$。 Class (c) $r \\in S(w), r \\in M(w)$, C 是已經被 Marked 的 Item，所以實際上不會有變化。 如果出現一個 $\\text{Class (c) i}$ Request，則最多會有 $l+t+i-1$ 個 Marked Item 因為 (a), (b) 最多貢獻 $l+t$ 個 Marked Item，而 $i$ 則是 Class (c) 的 Request 同時 Active item 的數量會是 $k-i+1$ 根據 Fact 1. 得出這次請求的 Cost = 1 的機率是 $(l+t+i-1)/(k-i+1) = (l+i)/(k-i+1)$ 因為 i 的範圍是從 1 到 $k-l-t$，所以可以得到以下公式: \\[\\Delta \\text{cost} = \\sum_{i=1}^{k-l-t} \\frac{l + t}{k - i + 1} = (l + t)(H_k - H_{l+t})\\] 上面用 Harmonic Number 來表示，這裡的 $H_k - H_{l+t}$ 是因為 $\\sum_{i=1}^{k} \\frac{1}{i} = H_k$，所以這裡是 $H_k - H_{l+t}$ Upper bound 這樣我們就算出了所有的 Request 的 Cost，然後再加上 Potential Function 的變化，就能得到 $(2H_{k-1}) \\cdot Δopt$ 的部分， 跟以下的不等式: \\[\\begin{aligned} \\Delta \\text{cost} + \\Delta \\Phi &amp; \\leq (l + t)(H_k - H_{l+t}) + s'(H_k - H_{s'} + 1) - s(H_k - H_{s} + 1) \\\\ &amp; \\leq (l + t)(H_k - H_{l+t}) + s' H_k - s(H_k - H_s + 1) \\\\ &amp; \\leq (l + t)(H_k - H_{l+t}) + s' H_k - l(H_k - H_{l+1}) \\\\ &amp; = (2H_k - 1)t + 2t - (l + t)H_{l+t} + 1H_{l} - (t - s') H_k \\\\ &amp; \\leq (2H_k - 1)t \\\\ &amp; = (2H_k - 1)\\Delta \\text{opt} \\end{aligned}\\] (1) 是來自於 $\\Delta cost$ 的界線以及 $\\Phi ,s,s’$ 的定義 (2) $l \\leq s$ 並且 $\\Phi$ 隨著 $s$ 增加而增加 (3) 因為 $2t - (l+t)H_{l+t} \\leq -lH_{l+1}$ 跟 $s’ \\leq t$ 以上就是透過分析 Marking Algorithm 的 Upper/Lower Bound，Competitive Ratio = $2H_k - 1$ 並且是 Tight 的證明。 這個 Marking Algorithm 只是一個最簡單的例子來說明 Competitive Paging Algorithm 的分析，實際上在 OS 中 Marking Algorithm 還有更多的變化， 通常更多的透過 Learning strategy 來做決策，例如: Clock-PRO，在分析上大部分是基於實驗的方式。 Last Edit 06-21-2024 09:07"
  },"/jekylls/2024-06-09-arch_user_repository.html": {
    "title": "Note | Arch User Repository",
    "keywords": "Note Jekylls",
    "url": "/jekylls/2024-06-09-arch_user_repository.html",
    "body": "Notes for Arch User Repository. 透過這篇文章來介紹 AUR 跟使用方式。 Introduction 因為是非官方維護的軟體，當然在安全性上的風險要使用者自己負責 Arch User Repository(AUR) 就是 Arch Linux 的第三方軟體庫，這裡的軟體是由社群貢獻者維護的，而不是由 Arch Linux 官方維護的。 一些在其他常用 distribution 上如 Debain 的軟體 Arch 官方不一定有維護，就需要到 AUR 來尋找。 跟 Debian, Ubuntu 不同可以透過 /etc/apt/sources.list 來新增 nonfree, universe 這些 Repository，並透過 apt 來管理， AUR 是 pacman 無法直接管理的，需要以手動的方式來進行安裝，所以會讓一些初學者感到困惑。 從 AUR Packages 的網頁可以搜尋到所有在 AUR 上的套件，但是 AUR 其實並不是收錄任何軟體的 Binary 執行檔， 而是名為 PKGBUILD 的腳本文件，PKGBUILD 本身只有如何安裝該軟體的指令腳本而已。 在 PKGBUILD 中軟體的安裝很隨意，可以用任何方式進行，只要能把軟體安裝到系統，除此之外跟 pacman 幾乎沒有差異， 最終也會透過 pacman 來安裝與管理。 Advantages &amp; Disadvantages Advantages 最大的優點當然是高自由度，從基本的系統程式，到遊戲、Chrome、Vscode 都有人打包 安裝中的許多依賴都能依靠 PKGBUILD 內以腳本完成，並且因為是社群驅動的維護因此可以在上面找到最新的版本 同時 AUR 比官方的 Repository 大很多，所以 pacman -Ss package 找不到時，wiki 上都會註明 you can install it from the AUR. Disadvantages 缺點也顯而易見，你不知道這個維護者會不會在其中塞入惡意程式 Malicious Software Packages Found On Arch Linux User Repository 這些風險都要依靠用戶自己發覺。 為了方便大部分會使用如 yay, paru 這類的 AUR Helper 來管理 AUR 的套件，但是應該要牢記 AUR 的本質，並且官方的安裝方式依然是以手動的方式進行。 Attention 在開始使用 AUR 之前最好先了解一下這個 Package 的相關資訊，這樣可以避免一些不必要的風險。 首先檢查該 Package 是由誰 Maintaniner, Licenses 是開源或者是官方發行，跟底下的用戶留言。 這裡可以看一下軟體的來源是什麼地方 有的 AUR Package 是需要自行編譯的，如果是一些比較大的 Package 可能會需要很多時間 例如上圖是 visual-studio-code-bin，這是一個 -bin 版本的 Package，可以直接安裝不需要編譯 如果是帶有 -git 的 Package，代表他的來源是從最新的 git 上面拉下來的，這樣的 Package 會是最新的版本比較不穩定， Package maintainer 可能也沒時間做完整的測試，這樣的 Package 也要小心使用。 Getting started 首先確定 base-devel 已安裝，他包含了一些基本的編譯工具，如: gcc, make, autoconf… sudo pacman -S base-devel 最佳化 /etc/makepkg.conf 的設定，這裡可以設定一些編譯的參數來加速，詳細可以參考 makepkg#Tips and tricks /etc/makepkg.conf MAKEFLAGS 設定編譯的核心數量，使用全部的核心數量: MAKEFLAGS=\"-j$(nproc)\" PKGEXT 可以設定 Compression 的方式，預設是 .pkg.tar.xz: PKGEXT='.pkg.tar' 這樣就不會壓縮，.tar 是單純的打包 PKGEXT='.pkg.tar.lz4' 這樣就會使用 .lz4 算法來進行壓縮 /etc/pacman.d/mirrorlist 去 Mirrorlist 中找到一個比較快的 Server，這樣可以加速下載的速度 最後記得更新一下 sudo pacman -Syy。 Manual Installation 整個手動安裝過程可以分為 4 個步驟: Clone the AUR Package git repository. Follow the instructions of the PKGBUILD file. Install additional dependencies via pacman. Build and install the package. 這些過程其實都可以透過 AUR Helper 來完成，但是這裡要介紹的是手動的方式，以 visual-studio-code-bin 這個 Package 來做為範例。 進到 visual-studio-code-bin 的 AUR Packages 頁面，點選 Git Clone URL 來 Clone 下來 使用 git clone 到本地 git clone https://aur.archlinux.org/visual-studio-code-bin.git 進到 visual-studio-code-bin 的目錄，查看 PKGBUILD 的內容 cd visual-studio-code-bin vim PKGBUILD 使用 makepkg -si 來安裝，pacman 會自動安裝相依的套件，並打包成 .pkg.tar.xz .pkg.tar.xz 是 Arch Linux 的 Package 標準格式 上面的步驟會在打包完成後自動安裝，手動安裝使用 sudo pacman -U $PACKAGE_NAME 來安裝 如果想查詢哪些 Package 是由 AUR 安裝的，可以使用 pacman -Qm 來查詢 BKGBUILD 在進入 PKGBUILD 的時候可以看到他的安裝寫法，這裡可以檢查一下他的安全性，看看是否有一些奇怪的指令或來源 這裡看到它 x86 的來源是 https://update.code.visualstudio.com/${pkgver}/linux-x64/stable 這是 Visual Studio Code 的官方更新來源， 所以至少它的來源是安全的。 AUR Helper 這裡介紹常見的 AUR Helper: yay，yay 是由 Golang 寫的，把常見的操作都包裝成了一個指令就可以完成。yay 曾經缺乏維護導致被棄用，但是現在已經有人接手維護。 另外 AUR Helper 還有 paru, trizen 等等… AUR Helper 同樣的也不屬於 Arch Linux 的一部分，所以也需要從 AUR 上面安裝。之前的軼事說明了一件事，AUR Helper 也是由社群維護的， 需要依賴於社群的力量來維護並不斷發展，所以使用稍舊的版本可能短期內沒有問題，但隨著版本的更新可能會導致他停止工作。 yay 的官方 Github yay’s Github 官方的安裝步驟如下: pacman -S --needed git base-devel git clone https://aur.archlinux.org/yay.git cd yay makepkg -si yay 的使用上跟 pacman 很類似，例如以下: # Search Package yay $PACKAGE_NAME # Install Package yay -S $PACKAGE_NAME # Remove Package yay -R $PACKAGE_NAME # Update All Package yay -Syy yay -Syu yay 在安裝時對於 dependencies 會自動安裝，並且優先去 Arch Linux 的官方 Repository 找，如果找不到才會去 AUR 上面找， 並且在安裝過程中會詢問使用者是否要查看 PKGBUILD 的內容，這樣可以避免一些不必要的風險。 yay 會把 Cache 存放在 ~/.cache/yay，如果想要清除 Cache 可以使用 yay -Sc 來清除 透過 yay 安裝的 AUR Package 也可以透過 pacman -Qm 來查詢 Last Edit 06-12-2024 09:07"
  },"/jekylls/2024-04-09-hirschbergs_algorithm.html": {
    "title": "Algorithm | Hirschberg’s Algorithm",
    "keywords": "Algorithm Jekylls",
    "url": "/jekylls/2024-04-09-hirschbergs_algorithm.html",
    "body": "Notes: Hirschberg’s Algorithm, A Linear Space Algorithm for Computing Maximal Common Subsequences. Hirschberg’s Algorithm 是一種用來解決 Needleman-Wunsch Algorithm 的空間複雜度的演算法，原本的空間複雜度為 O(m*n)。 在論文的發表時間(1975)，記憶體是一個很昂貴的資源，所以 Hirschberg’s Algorithm 的提出是一個很大的突破。 這個問題如果想要實際寫程式的話，可以參考 LeetCode 72. Edit Distance。 簡單的 Golang 實作，Golang Implementation. Introduction Edit distance Edit distance 是針對兩個 String 之間的差異度的量化測量，可以用來判斷兩個字串之間的相似度，在 DNA 或 Unix 的 diff 等等應用上都有很大的用途。 Levenshtein distance 是指將一個字串變成另一個字串所需要的最少操作次數，操作包括 Insert, Delete, Replace。 \\[lev(a, b) = \\left\\{ \\begin{aligned} &amp; |a|, &amp;&amp; \\text{if } |b| = 0 \\\\ &amp; |b|, &amp;&amp; \\text{if } |a| = 0 \\\\ &amp; lev (tail(a), tail(b)), &amp;&amp; \\text{if } head(a) = head(b) \\\\ &amp; 1 + min \\left\\{ \\begin{aligned} &amp; lev(tail(a), b) \\\\ &amp; lev(a, tail(b)) \\\\ &amp; lev(tail(a), tail(b)) \\end{aligned} \\right. &amp;&amp; \\text{otherwise} \\end{aligned} \\right.\\] “Benson”, “Ben” 這兩個字串的 Edit distance 為 3 要把 “Benson” 變成 “Ben” 需要做 3 次 Delete 操作 要把 “Ben” 變成 “Benson” 需要做 3 次 Insert 操作 這邊會介紹 Needleman-Wunsch Algorithm 和 Hirschberg’s Algorithm 這兩個演算法，用來計算兩個字串之間的 Edit distance。 Needleman-Wunsch Algorithm Needleman-Wunsch Algorithm, A general method applicable to the search for similarities in the amino acid sequence of two proteins. Needleman-Wunsch Algorithm 是生物資訊中用來比對蛋白質或 DNA 序列的演算法，最早於 1970 年提出。是很標準的 Dynamic Programming 演算法，用來計算兩個序列之間的最佳對齊。 Time Complexity O(mn), Space Complexity O(mn) 假如我們有兩個字串 Benson 和 Ben Step 1: Create a DP Table 跟大部分的 DP 演算法一樣，我們需要先建立一個 DP Table，用來存放每個子問題的解，大小為 (m+1) * (n+1)。 Step 2: Choose a scoring system 假如兩個字串要做 Align 的話，每個位置只有三種可能的情況 match, mismatch, gap，在 Needleman-Wunsch Algorithm 中有很多種評分標準， 這裡使用 min 來作為評分方法的話，可以設計以下的 Scoring System: Match: 0 Mismatch: 1 Gap: 1 Step 3: Initialize the DP Table 現在我們依照 Gap 的評分標準，初始化 DP Table 的第一列和第一行。 Step 4: Fill in the DP Table Dynamic programming recursive formula: \\[dp(i, j) = \\left\\{ \\begin{aligned} &amp; dp(i-1, j-1), &amp;&amp; \\text{if } x(i) = y(j) \\\\ &amp; min \\left\\{ \\begin{aligned} dp(i-1, j-1) + 1 \\\\ dp(i-1, j) + 1 \\\\ dp(i, j-1) + 1 \\end{aligned} \\right., &amp;&amp; \\text{otherwise} \\end{aligned} \\right.\\] 依照上面的 Recursive Formula 依序歷遍整個 DP Table，注意是以 Row-Major 的方式填入 DP Table 如果 x(i) = y(j)，則 dp(i, j) 直接取左上角的值 否則取 Left, Top, Left-Top + 1 的最小值 Step 5: Traceback 最後我們可以從右下角的位置開始往左上角回溯，因為之前使用的 Scoring System，這裡我們使用 Min trace back: ⇦ 代表左側的字串要塞入一個 Gap ⇧ 代表上方的字串要塞入一個 Gap ⇖ 代表這兩個字元是 Match 最後的 Alignment 結果如下，Benson 和 Ben 之間的 Edit distance 為 3: Benson Ben--- Prefixes Alignment 我們也可以發現在 DP Table 中，每個格子都代表該左側和上方的 Prefix 之間的 Edit distance，例如以下: ”s”, “Benso”, Edit distance is 4 “so”, “Benso”, Edit distance is 3 “son”, “Ben”, Edit distance is 2 但是在 DNA 比對中往往資料量很大，所以 Needleman-Wunsch Algorithm 的空間複雜度是 O(mn)，這樣的空間複雜度在當時的記憶體是一個很大的負擔。 Hirschberg’s Algorithm Hirschberg’s Algorithm 可以在 Space Complexity O(min(m, n)) 的情況下，計算出 Needleman-Wunsch Algorithm 的結果， 並且保持 Time Complexity O(mn)，並且 Hirschberg’s Algorithm 是一個 Divide and Conquer 的演算法。 Divide and Conquer 首先要能做到 Divide and Conquer，我們需要確定一個 Base Case 被拆分後還是可以滿足原本的問題，所以為什麼可以進行拆分就會是接下來的問題。 假如有兩個字串 L, R 之間的 Edit-Distance 是 4 Len(L) = 8, Len(R) = 8 代表要講 R 修改成 L 需要經歷至少 4 次的操作 假如我們可以做拆分的話，現在變成四段的字串如下 L1 = L[1:6], L2 = L[7:8] R1 = R[1:4], R2 = R[5:8] 這裡必須保證 ED(L1, R1) + ED(L2, R2) = ED(L, R) 是相同的，這樣問題就來到我們怎麼找到一個點劃分 R R 無論如何都會以 Len(R) / 2 作為拆分點 L = “CDEFABGH”, R = “ABCDEFGH”，以此為例會得到以下的結果 注意下圖中 “ABCDEFGH” 無論如何都是以中點拆分 Needleman-Wunsch Algorithm Needleman-Wunsch Algorithm 可以幫助我們找到適合拆分的位置，既然我們知道 R 必須被拆分，那我們就可以用 R1 和 R2 來計算出 L 適合的拆分點。 這裡使用的評分方式跟之前一樣，Match: 0, Mismatch: 1, Gap: 1 將 L 與 R1 進行 Needleman-Wunsch Algorithm，得到 DP Table 將 Rev(L) 與 Rev(R2) 進行 Needleman-Wunsch Algorithm，得到 DP Table 因為我們要計算從後面往前的 Prefix 的 Edit distance，所以這邊要將字串反轉 分別將兩張 DP Table 組合會得到以下的結果 “ABCD” 到 “CDEFABGH” 的最小編輯距離在 “ABCD”, “CD” “HGFE” 到 “HGBAFEDC” 的最小編輯距離在 “HGFE”, “HGBAFE” 會發現其實就是取 R1, L 和 Rev(R2) 與 Rev(L) 的最小編輯距離，以這個點來分割 L 不會造成任何額外的編輯距離 以上的方式就是使用 Needleman-Wunsch Algorithm 來找到適合拆分的點，在這裡我們做些總結 將 R 使用 Len(R)/2 分為 R1 和 R2 使用 Needleman-Wunsch Algorithm 找到 R1 和 L 的最小 Prefix 編輯距離 使用 Needleman-Wunsch Algorithm 找到 Rev(R2) 和 Rev(L) 的最小 Prefix 編輯距離 Rev 的目的是從後面往前找到 Prefix 的編輯距離 將兩個 DP Table 的最後一列相加，找到各自對 L, Rev(L) 的最小 Prefix 編輯距離，即是 L 的拆分點 時間複雜度不會改變是 O(mn)，但是空間複雜度可以降到 O(mn)，因為需要一個剛好 mn 的 DP Table。 Optimize Space Complexity 在這裡我們能發現在 Needleman-Wunsch Algorithm 中，我們其實只需要保留兩列的 DP Table(各自的最後一列)， 因此在 DP Table 的建立過程中並不需要把整個表的空間都保留下來，只需要一個上方列的 1D Array 就可以建立 DP Table。 這樣的空間複雜度就可以降到 O(min(m, n)) 在 Divide and Conquer 的第一步我們可以先選擇較小的字串作為 R，這樣可以減少空間的使用 Last Edit 06-04-2024 00:43"
  },"/jekylls/2024-03-08-ANTLR_guide.html": {
    "title": "Compiler | ANTLR Guide",
    "keywords": "Compiler Jekylls",
    "url": "/jekylls/2024-03-08-ANTLR_guide.html",
    "body": "ANTLR (ANother Tool for Language Recognition) 是一個強大的 Parser Generator，可以用來產生語法分析器，並且可以用來產生語法分析樹， 並且在一些商業或開源程式中被使用，例如: Eclipse Modeling Framework, 就使用 ANTLR 來將程式碼轉換為 OCL (Object Constraint Language) PrestoDB 也使用 ANTLR 來解析 SQL ANTLR in Maven ANTLR v4 Maven 這邊是 ANTLR v4 Maven 的官方文件，可以參考這個文件來使用 Maven 中有幾個 ANTLR 相關的 Plugin 可以使用: ANTLR 4 Maven Plugin: 最完整的 ANTLR Maven Plugin，可以在 Maven 中使用 ANTLR4 ANTLR 4 Runtime Maven: 只包含 ANTLR 4 Runtime 的部分 ANTLR 4 Tool Maven Configuration: 下面的範例使用了 ANTLR 4 Maven Plugin，讓 ANTLR 加入到 Maven 的 Build 週期中 sourceDirectory: ANTLR 原始檔案的位置 outputDirectory: ANTLR 產生的檔案的位置 可以輸入 mvn org.antlr:antlr4-maven-plugin:help -Ddetail=true 來查看 ANTLR 4 Maven Plugin 的詳細資訊 這邊會列出 configuration 的詳細資訊，反而比 antlr4-maven-plugin 官方文件詳細 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.13.1&lt;/version&gt; &lt;configuration&gt; &lt;listener&gt;true&lt;/listener&gt; &lt;visitor&gt;true&lt;/visitor&gt; &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt; &lt;outputDirectory&gt;src/main/generated-sources&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;antlr&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;antlr4&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; ANTLR Command Line Options: 如果不使用 Maven 提供的 configuration，可以直接在 pom.xml 中加入 ANTLR4 的參數 這部分就跟 ANTLR Command Line 的參數輸出一樣，可以直接在這邊輸入 &lt;configuration&gt; &lt;arguments&gt; &lt;argument&gt;-package&lt;/argument&gt; &lt;argument&gt;ocl.runner&lt;/argument&gt; &lt;argument&gt;-o&lt;/argument&gt; &lt;argument&gt;${project.basedir}/src/generated-sources&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; 把上面設定好就可以再 Maven 的建置中使用 ANTLR 產生一個語法分析器了，通常會在 Java 中 import ANTLR 的相關工具， 所以建議是在 dependency 也加入 antlr4-maven-plugin 或 antlr4-runtime。 Parser Generation and Execution 通常不會把 ANTLR 產生的檔案與原始檔案放在一起，這樣在管理上會比較麻煩，所以通常會把 ANTLR 產生的檔案放在另一個 Directory。 這邊選擇把 ANTLR 產生的檔案放在 java 的同層目錄下的 generated-sources 中 因為多了一個地方存放 Java source code，所以可以使用 build-helper-maven-plugin 來幫助 Maven 來找到這些 source code 如果這些設定都完成了，就可以在 antlr4 的 sourceDirectory 中加入 grammar file 了，這邊以下面的語法為例: grammar Expr; prog: expr EOF ; expr: expr ('*'|'/') expr | expr ('+'|'-') expr | INT | '(' expr ')' ; NEWLINE : [\\r\\n]+ -&gt; skip; INT : [0-9]+ ; 加入這個 grammar file 到 sourceDirectory 中，然後執行 mvn clean install 就可以看到 ANTLR 產生的語法分析器了 在 java 中添加一個 main function 來測試 ANTLR 產生的語法分析器，如下: public class Expr { public static void main(String [] args) throws Exception { CharStream charStream = CharStreams.fromString(\"10+20*30\"); // Make lexer ExprLexer lexer = new ExprLexer(charStream); // Get a TokenStream on the lexer CommonTokenStream tokens = new CommonTokenStream( lexer ); // Make a parser attached to the token stream ExprParser parser = new ExprParser( tokens ); // Get the top node (the root) of the parse tree then print it ParseTree tree = parser.prog(); System.out.println(tree.toStringTree(parser)); } } 執行 mvn exec:java -Dexec.mainClass=\"{YourMainClass}\" 就可以看到 ANTLR 產生的語法分析樹了，也可以在 pom.xml 中加入 exec-maven-plugin 來執行這個 main function。 ANTLR Command Line ANTLR 也有提供一系列 Command Line 的指令，可以用來進行產生語法分析器跟 GUI 的 Parser Tree，下面是安裝步驟: wget https://www.antlr.org/download/antlr-4.13.1-complete.jar mv antlr-4.13.1-complete.jar /opt/javalib/ vim ~/.bashrc # Join the following to the .bashrc file # export JAVA_HOME=/opt/jdk-17.0.10 # export PATH=$JAVA_HOME/bin:$PATH # export CLASSPATH=/opt/javalib/antlr-4.13.1-complete.jar # alias antlr4='java -jar /opt/javalib/antlr-4.13.1-complete.jar' # alias grun='java org.antlr.v4.gui.TestRig' source ~/.bashrc antlr4 # [benson@arch ~]$ antlr4 # ANTLR Parser Generator Version 4.13.1 # -o ___ specify output directory where all output is generated # -lib ___ specify location of grammars, tokens files # ... # -Xexact-output-dir all output goes into -o dir regardless of paths/package Last Edit 03-19-2024 19:32"
  },"/jekylls/2024-03-06-arch_linux_installation.html": {
    "title": "Note | Arch Linux Installation",
    "keywords": "Note Jekylls",
    "url": "/jekylls/2024-03-06-arch_linux_installation.html",
    "body": "Notes how to install Arch Linux 紀錄一下怎麼安裝 Arch Linux，非常推薦讀官方的 Installation guide 這邊大概就是一個簡化版的安裝流程，不得不說 Arch 的自由度真的很高， 所以安裝的時候可以根據自己的需求來安裝。 Pre-installation 1.1 Connect to the internet 1.2 Update the system clock 1.3 Partition the disks 1.1 Connect to the internet 這邊會透過 systemd-networkd 來配置網路，如果要設置 Static IP: nano /etc/systemd/network/20-wired.network [Match] Name=en* [Network] DHCP=no Address=10.1.10.9/24 Gateway=10.1.10.1 DNS=10.1.10.1 #DNS=8.8.8.8 配置後 systemctl restart systemd-networkd 重啟網路服務 檢查 ip addr 是否與配置一樣 1.2 Update the system clock 使用 timedatectl 來設置系統時間，首先 timedatectl set-ntp true 啟動 NTP 服務: timedatectl list-timezones | grep Asia/Taipei 查詢時區是否存在 timedatectl set-timezone Asia/Taipei 設置時區 timedatectl status 查看目前時間是否正確 root@archiso ~ # timedatectl status Local time: Sun 2024-03-03 15:21:00 CST Universal time: Sun 2024-03-03 07:21:00 UTC RTC time: Sun 2024-03-03 07:21:00 Time zone: Asia/Taipei (CST, +0800) System clock synchronized: yes NTP service: active RTC in local TZ: no 設置成功應該會看到以上的訊息 1.3 Partition the disks fdisk 是一個常用的分割硬碟的工具，這邊會使用 fdisk 來分割硬碟: 先使用 fdisk -l 查看硬碟分割情況 fdisk /dev/sda 開始分割硬碟 這邊我只分割一個 SWAP 跟一個主分割區，因為剩下的空間會使用 Btrfs 來管理 使用 mkfs 來格式化分割區 mkswap /dev/sda1 swapon /dev/sda1 mkfs.btrfs /dev/sda2 mount /dev/sda2 /mnt mount 之後操作就跟 Debian 一樣，唯一的差別是 arch 沒有 target 這個目錄，所以把操作的目錄都換成 /mnt 就好。 在最後的時候記得用 genfstab -U /mnt &gt;&gt; /mnt/etc/fstab 來生成 fstab 延伸閱讀: 可以參考 Using btrfs on Debian 來設置 Btrfs，或者官方說明 Arch - btrfs Install Linux 2.1 Select the mirrors 2.2 Install the base packages 2.1 Select the mirrors 這邊要選擇最快的鏡像，可以使用 reflector 來自動選擇最快的鏡像，或者手動編輯 /etc/pacman.d/mirrorlist 來選擇鏡像。 reflector --country Taiwan --age 12 --protocol https --sort rate --save /etc/pacman.d/mirrorlist 選擇最近 12 小時內更新過位於台灣的鏡像，並且排序下載速度排序 官方也有一個提供的 Mirrorlist 這步驟也可以在安裝完系統後再執行，最好在進行前備份 /etc/pacman.d/mirrorlist 2.2 Install the base packages 使用 pacstrap 來安裝基本的套件，pacstrap /mnt base linux linux-firmware 如果想要安裝其他套件可以在後面加上套件名稱，例如: pacstrap /mnt base linux linux-firmware vim openssh 先使用 pacman -Sy 更新套件庫，就可以使用 pacman -Ss 搜尋套件名稱 Configure the system 3.2 Chroot 3.5 Network configuration 3.8 Install a bootloader 剩下的部分官方文件都有詳細的說明，這邊就不再贅述，可以參考 Installation guide 來進行安裝。 fstab 跟官方說明設置就好，然後記得查看是否有錯誤 3.2 Chroot chroot 可以讓你進入安裝好的系統，這樣就可以提前進行一些設置 如果在 pasctrap 少安裝了一些套件，可以在這邊安裝，例如: pacman -S openssh 安裝後記得到 systemctl enable sshd 啟動 sshd 服務 Time Zone 用 symbolic link 來設置時區，然後用 hwclock --systohc 來設置硬體時鐘 Localization Hostname # Time Zone ln -sf /usr/share/zoneinfo/Asia/Taipei /etc/localtime hwclock --systohc # Localization vim /etc/locale.gen # Uncomment `en_US.UTF-8 UTF-8` and other needed locales locale-gen echo \"LANG=en_US.UTF-8\" &gt; /etc/locale.conf # Hostname echo ${YOU_HOSTNAME} &gt; /etc/hostname 3.5 Network configuration 這邊可以參考 systemd-networkd 來設置網路，或者使用其他的套件來設置網路。 下面是沿用 systemd-networkd 的設置 vim /etc/systemd/network/20-wired.network # Wired network like following: # [Match] # Name=en* # [Network] # DHCP=no # Address=10.1.10.9/24 # Gateway=10.1.10.1 # DNS=10.1.10.1 systemctl enable systemd-networkd 3.8 Install a bootloader 這一步非常重要，因為如果沒有 bootloader 就無法開機 在安裝 bootloader 前先把 initramfs 跟 passwd 設置好，然後再安裝 bootloader。 選擇 GRUB 作為 bootloader，Arch 這邊有其他 bootloader 的選擇，可以參考官方文件 這裡沒有 UEFI 的安裝方式，如果要安裝 UEFI 可以參考官方文件 Arch - GRUB # Initramfs mkinitcpio -P # Set root password passwd # Install bootloader, os-prober can detect other OS pacman -Sy grub efibootmgr os-prober grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg 這裡可以把剩下的部分都設置好，然後 exit 離開 chroot，官方是建議關機前 umount -R /mnt 卸載所有的分割區，然後再 reboot， 重啟之後如果進入 GRUB 引導到 Arch Linux 就代表安裝成功。 useradd -m -G wheel,users -s /bin/bash ${USER_NAME} 新增使用者 passwd ${USER_NAME} 設置密碼 Other 4.1 Firewall Arch 通常沒有預設安裝防火牆，並且 iptables 是被 systemd 服務管理的，所以最好開啟 systemctl enable iptables 來啟動防火牆服務。 並且要設置好防火牆規則，這裡有一個簡單的腳本 iptables-script.sh 設定完記得保存規則 iptables-save &gt; /etc/iptables/iptables.rules ip6tables-save &gt; /etc/iptables/ip6tables.rules Last Edit 03-06-2024 15:21"
  },"/jekylls/2024-03-01-maven_multi_module_project.html": {
    "title": "Note | Apache Maven Multi-Module Project Guide",
    "keywords": "software Jekylls",
    "url": "/jekylls/2024-03-01-maven_multi_module_project.html",
    "body": "在開發 Java 的時候一定會考量的是如何管理專案的結構，尤其是當專案規模大到一定程度的時候，這裡就是因為遇到了陳年舊 Code 把所有程式塞在同一個專案裡面， 為了對專案做更好的管理架構，使用 Module 來分割專案是一個不錯的選擇，並且也可以增加代碼的可維護性。 Reference: Maven by Example - Chapter 6. A Multi-Module Project, Guide to Working with Multiple Modules 其實概念上就是依靠 POM.xml 來管理多個專案，這樣就會有一個頂層的 POM.xml 在此之下會有多個子專案的 POM.xml，這樣就可以透過頂層的 POM.xml 來管理所有的子專案， Maven 把這樣的機制稱為 Reactor，Maven 會透過 Reactor 來做以下操作: 收集所有可用的 Module 來 Build 依照 Module 之間的依賴關係跟順序來 Build Parent POM 這邊用 Eclipse 來建立一個 Maven Project 把 POM.xml 中的 &lt;packaing&gt; 設定為 pom，這樣就會變成一個 Parent POM，這樣就可以透過這個 POM.xml 來管理所有的子專案。 建立一個 Maven Project 使用 maven-archetype-quickstart 這個 Archetype 把 POM.xml 中的 &lt;packaing&gt; 設定為 pom 設定為 pom 之後就會變成一個 Parent POM 其他的檔案都可以刪掉了 POM.xml 的 artifactId 可以命名為 root Module 的 dependency 可以放在 Parent POM 中，這樣所有的 Module 都可以共用這些 dependency 如果單獨放在 Module 的 POM.xml 中，那麼就代表是只有這個 Module 可以使用這些 dependency version, groupId 可以在 Parent POM 中設定代表所有的 Module 都共用這些設定 Parent POM: &lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example.myapp&lt;/groupId&gt; &lt;artifactId&gt;root&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;root&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; Module POM: &lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.example.my&lt;/groupId&gt; &lt;artifactId&gt;root&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;application&lt;/artifactId&gt; &lt;name&gt;application&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;/project&gt; Module Module 就是一個獨立的專案，跟一般的 Maven Project 一樣，如果 Module 之間有互相 dependency 的關係， 那就在 POM 加入對該 Module 的 dependency，這樣 Maven 就會依照順序來 Build。 如果 application module 依賴於 data-site 就可以在 application 的 POM.xml 中加入以下的設定: &lt;dependency&gt; &lt;groupId&gt;com.example.my&lt;/groupId&gt; &lt;artifactId&gt;data-site&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 全部設定完後使用 mvn verify 就可以驗證整個專案的 Build，Maven 會依照 Module 之間的依賴關係跟順序來 Build jar。 如果想要把所有專案的 jar 都打包成一個大的 jar 可以使用一些 Maven Plugin 來達成，例如 maven-assembly-plugin。 [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary for root 0.0.1-SNAPSHOT: [INFO] [INFO] root ............................................... SUCCESS [ 0.921 s] [INFO] data-site .......................................... SUCCESS [ 2.003 s] [INFO] application ........................................ SUCCESS [ 0.732 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3.793 s [INFO] Finished at: 2024-03-01T20:31:18+08:00 [INFO] ------------------------------------------------------------------------ mvn verify 成功後就會看到類似上面的訊息，代表整個專案都 Build 成功了 Last Edit 03-01-2024 15:32"
  },"/jekyll/2024-02-03-storage_and_file_system.html": {
    "title": "OS | Storage and File System",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2024-02-03-storage_and_file_system.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 參考資料: Jserv Linux 核心設計: 作業系統術語及概念, Linux 核心設計: 檔案系統概念及實作手法 Everything is a file. 是一個 Unix 設計哲學，但是這個設計並沒有完全實現在 UNIX 和後續的 BSD 上，直到 Plan 9 才算是真正的 Everything is a file， 使用了 9P 這個 Protocol，而 Linux 也採用了 9P 應用在虛擬化技術上。 如果從恐龍書來學習作業系統，那麼通常 File System 會被放在比較後面來學習，但是以 UNIX 的發展來看的話， File System 是從第零版的 UNIX 手冊就已經包含了檔案系統，而不見得已經包含了 Scheduler， 這也代表 File System 是作業系統中一個非常關鍵的部分。 Block storage Hard drive Scheduling File system Overview File structure Directory structure Filesytem across hard drives File System Overview 不管是 HDD 或是 SSD 都是 Block device，也就是資料的最小儲存單位是 block，對於 OS 來說硬碟就是一連串的 block。 再給定 Logical block address (LBA) 就可以讀取或寫入資料 LBA 就是一連串的 block 並且有邏輯上的連續性 CS 的先驅們設計了 File 和 Directory 這樣的概念，把 block device 變成邏輯上有意義的資料的集合 在 Linux 中為了讓不同的 File System 可以共存，所以設計了 VFS(Virtual File System) 這樣的機制，VFS 是一組檔案操作的抽象介面， 只要依循 VFS 開發的 File System 就可以在執行時期動態的掛載到 Linux 的 Kernel 上。 9.3.1 Directory and File 實際上 Directory 也是一個 File，在傳統的 UNIX 中 Directory 也是一個 File 其中記錄了其他 File 的名稱和對應的檔案編號 透過檔案編號就可以找到對應的 control block，control block 中記錄了檔案的 block 分布於硬碟上的位置 benson@101-debian:~$ tree -L 2 --inodes [ 12] . ├── [ 16506] .bash_history ├── [ 2545] .cache ├── [ 3631] go │ └── [ 3632] pkg └── [ 1582] workspace ├── [ 1792] Hotshot824.github.io └── [ 17040] jekyll-gitbook 5 directories, 0 files 這裡我們使用 tree -L 2 --inodes 來查看目前目錄下的檔案結構，會發現 Directory 也有對應的 inode，這代表 Directory 也是一個 File。 Block Size 大部分 Block device 所支援的 block 大部分是 4KB，跟 OS 的 page size 相同 這樣可以讓 OS 在管理記憶體與 Block device 的時候更有效率 9.3.2 Universal I/O Model Linux 因為要秉持 UNIX 哲學 Everything is a file，因此不同種類的 I/O 都是透過 File Descriptor 來操作的，這樣就可以在這個一致的介面下操作。 我們可以使用 mount 來查看這些虛擬檔案系統 benson@debian:~$ mount sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime) proc on /proc type proc (rw,nosuid,nodev,noexec,relatime) udev on /dev type devtmpfs (rw,nosuid,relatime,size=4042436k,nr_inodes=1010609,mode=755,inode64) devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000) ... File Structure 這裡會介紹幾種常見的 File Structure，例如: Contiguous Allocation, Linked List, Indexed Allocation，但真正的重點是 i-node， 這是目前 Linux 使用的 File Structure。 9.4.1 Contiguous Allocation Contiguous Allocation(連續配置) 是一種最簡單的檔案配置方式: 基本構造是依照一個識別方式來找到檔案的起始位置，然後再依照檔案的大小來找到檔案的結束位置。 因為檔案是連續配置的，所以不管是循序存取或是隨機存取的效率都很高 例如: 存取 file1 的第 20 個 block 只要讀取起始位置加上 20 就可以了 但是檔案基本上無法繼續增長，適用於靜態的檔案系統，例如: CD-ROM, 磁帶機等等 再 Linux 上例如 root file system 每次開機都會掛載，並不會變化就可以使用 Contiguous allocation Contiguous allocation 可以獲得比較高的效率，但是勢必會有 fragmentation 的問題 Extents Extents 是取得連續配置與 fragmentation 中取得平衡的方案，file system 會盡可能的分配連續的空間給檔案，這樣的連續空間被稱為 Extents。 如果 file1 的空間不夠用，就會分配一個的 Extent 給 file1 延伸 後續的演算法都可以使用 Extents 來擴充，加入 Extents 的概念 Extents 可以視為分配空間的單位，但是保證這個單位空間是連續的 9.4.2 Linked List 想要存取 jeep 就必須從 block 9 -&gt; 16 -&gt; 1 -&gt; 10 -&gt; 25 這樣去存取 Linked List 就跟字面上的意思一樣，每個 block 都會記錄下一個 block 的位置，這樣就可以連結起來， 因此在空間上可以不連續，但是在邏輯上是連續的。 在效能上不會太好，尤其是 Random Access 的效能，如: SSD, HDD 即使是 DRAM 也是 sequential access 的效能比較好 SSD 的效能可能會降低至 1/2，而在 HDD 效能會下降的非常多 File Allocation Table(FAT) FAT 就是把 Linked List 的指標直接拿出來放在一個表格，這樣的好處是可以提前知道那些 block 是連續的，要讀取那些 block，這樣就可以提前 sequential access。 可以做部分的優化，sequential access 的效能比 random access 快很多 假如要讀取 jeep 就可以提前知道 jeep 的那些 block 是連續的，這樣就可以提前讀取 多餘被讀取的 block 就先丟進 DRAM，之後再丟棄就可以 OS 可以提前讀進這些檔案的連結方式，也可以更快速的做 Random Access 如果 Main Memory 夠大還可以把 FAT 放在 Main Memory，這樣就可以更快速的存取 這裡要注意，FAT 所存放的是 block 的下一個 block 的位置，而不是直接指向該 block 的位置，因此跟 Index Allocation 有所不同 9.4.3 Indexed Allocation 系統為每個檔案建立一個 index block，這個 index block 會記錄這個檔案的所有 block 的位置，這樣就可以直接存取這個 index block。 主要的問題是 index block 的大小，如果 index block 是 4K，那最多存儲 65536 個 data block，假如 data block 也是 4K 這個檔案系統最大為 256MB 的檔案 因此這裡可以向 MMU 一樣使用多層的 index block，這樣就可以存儲更大的檔案 缺點是假如我有一個很小的檔案如 4K，那麼這個檔案的 index block 會佔用很多的空間，假如有三層就要 12K 的 Index block 要維護 9.4.4 i-node i-node 是 Unix 和 Unix-like OS 中所使用的一種資料結構，用一個資料結構來記錄一個檔案 metadata，包含了檔案的大小、擁有者、權限、時間戳等等 i-node 的這個設計在第一代的 UNIX(1960) 就已經使用，這個設計已經沿用了 60 年以上，很難想像一個檔案結構的觀念在這麼長的時間內都沒有被超越。 當然 i-node 有一些改善，例如控制權限從 owner, group, others 變成了 ACL(Access control list) 但是 i-node 的基本結構並沒有改變 i-node 的前 12 個 block 是直接指向 data block(Direct data block)，所以不用再去使用第二層的 index-block，這樣如果檔案小於 48KB 就可以直接使用 i-node 來紀錄， 節省了 index-block 的空間。 一個 i-node 本身通常是 128 bytes，所以一個 block 可以存放 32 個 i-node 除了 Direct data block 之外還有 Single indirect block 和 Double indirect block Single indirect: 就是第二層的 index block 可以指向編號 12 ~ 267 的 block，也就是 256 個 block, Double indirect: 就是第三層的 index block 可以指向編號 268 ~ 65535 的 block，也就是 65536 個 block 因此一共可以定址 12 + 256 + 256^2 + 256^3 = 16843020 個 block，也就是 64.25GB 的檔案 在此之上還可以使用 Extents 來擴充，這樣就會讓 Data block 的大小變得更大 因此如果假設 data block 是 4KB，要存取某個 block: 當 File size &lt; 48KB 就可以直接使用 i-node 兩次存取到 block &lt; 1072KB 使用 Single indirect block，最多三次的 Access &lt; 263216 使用 Double indirect block，最多四次的 Access &lt; 64.25GB 最多五次的 Access 要注意這裡所提的都是最糟情況下，實際上已經存取過的 i-node 或 index 會被放在 DRAM 中，所以實際上的 Access 次數會比較少 9.4.5 Hole Hole 是一個很重要的功能，如果一個檔案中並沒有實際的資料，那是否要真的要先分配空間給這個檔案，例如: VMWare 的虛擬機器通常可能設定了 100GB 的硬碟，但是實際上只有 10GB 的資料，剩下的資料還沒有寫入 如果沒有 Hole 的機制，就要把這 90GB 的空間都分配出來，全部以某種方式填滿 再有 Hole 的情況下這些空間都被視為 Logical 0，不占用實際的 Disk 關於 VMWare 的虛擬機器通常會提供 Provisioning Policies(佈建原則) 給使用者選擇 延伸閱讀: Linux 檔案的hole, Sparse file 9.4.6 Free Space Management 空的 block 也需要某種管理機制，讓 OS 能快速的找到空的 block 來分配給檔案，這裡有幾種常見的管理機制: Bit Vector 來管理空間，每個 block 用一個 bit 來表示是否被使用 這樣的好處是可以直接使用 Bitwise Operation 來操作，並快速的找到連續的空間 但是操作 Bit Vector 會有一些 overhead，例如: 把 Bit Vector 放在 DRAM 中才能快速的操作，並且 Time Complexity 是 O(n) Linked List 來管理空間，每個 block 會記錄下一個空的 block 使用 Linked List 來管理的好處是找到空的 block 就放入 Linked List 就好，OS 只要知道 Head 就可以找到所有的空間 缺點是比較難找到連續的空間 實際的例子是 ext2, ext3 都是使用 Linked List 來管理空間 btrfs 在 btrfs 中所有使用中的 block 都由 extent 來管理，因此 btrfs 中的檔案都是由 contiguous block 構成 btrfs 會使用一顆 B-tree 來管理被使用的 block 這樣也代表沒被放入 B-tree 的 block 就是空的 block 例如兩個 node 分別記錄 250~440 與 800~1000，那麼 441~799 就是空的 block Directory Structure 會對 Directory structure 的操作有兩種: 已經給定的 path name，找出相對應的檔案，例如: vim /home/benson/example.txt 就是在 /home/benson 下找出 example.txt 列出 Directory 下所有的檔案 ls /home/benson 就是列出 /home/benson 下所有的檔案 9.5.1 Unix Directory Directory 是一個特別的檔案 這個檔案紀錄了這個 Directory 下所有的的東西 Directory 可以用 opendir(), readdir() 操作 主要包含這些: name, type, i-node number Directory Design Method Linear List 每一個目錄檔案可以視作是一個特殊的文字檔案，裡面紀錄 name, type, i-node number 這種方式很適合列表功能，例如: ls Data Structure for Fast Searching 可以使用檔案路徑名稱來快速的找到對應的 i-node，例如: hash, b-tree 這種方式很適合指定路徑名稱的功能，例如: vim /home/benson/example.txt Mixed 這兩種方式可以混合使用，這樣就可以快速列表跟快速搜尋，例如: btrfs Last Edit 3-6-2024 12:58"
  },"/jekylls/2024-01-05-debian_btrfs.html": {
    "title": "Note | Using btrfs on Debian",
    "keywords": "OS Jekylls",
    "url": "/jekylls/2024-01-05-debian_btrfs.html",
    "body": "Notes debian using btrfs as main file system 想在 debian 上安裝 Btrfs，有很多種方式，這裡紀錄一下怎麼從頭開始安裝 debian，並且使用 Btrfs 作為主要的檔案系統。 Install Debian 在安裝 Debian 的時候要進入 Advanced options，並且選擇 Expert install，這樣才能夠在安裝的時候選擇 Btrfs 作為檔案系統。 接著就跟標準的安裝方式一樣。 在這個過程中如果不確定的選項就直接按 Enter 就好，這樣就會使用預設的選項 Partition disks 直到 Partition disks 的時候，才是這次要做的重點 這邊要選擇 Manual，這樣才能夠自己選擇要安裝的檔案系統，並且自己分割硬碟，分割硬碟的時候只要分割出 SWAP 和 /， 其餘的都不做分割之後用 Btrfs 來管理 subvolume 就好。 Manual gpt: 使用 UEFI 的啟動方式 msdos: 傳統的 BIOS 啟動方式 分割好之後 alt + F2 進入 console，先 df -h 查看一下目前的狀況 df -h # Umount /target umount /target # Mount /mnt mount /dev/{partition} /mnt # Create subvolume and mount subvolume cd /mnt mv @rootfs @ btrfs subvolume create @boot btrfs subvolume create @home btrfs subvolume create @snapshots # Mount rootfs mount -o noatime,compress=zstd:1,space_cache=v2,ssd,discard=async,subvol=@ /dev/{partition} /target cd /target mkdir -p boot mkdir -p home mkdir -p .snapshots mount -o noatime,compress=zstd:1,space_cache=v2,ssd,discard=async,subvol=@boot /dev/{partition} /target/boot mount -o noatime,compress=zstd:1,space_cache=v2,ssd,discard=async,subvol=@home /dev/{partition} /target/home mount -o noatime,compress=zstd:1,space_cache=v2,ssd,discard=async,subvol=@snapshots /dev/{partition} /target/.snapshots # Umount /mnt umount /mnt nano /target/etc/fstab nano 裡面 ctrl + k 剪下整行，ctrl + u 貼上 修改後的 fstab 應該會類似下面這樣，這樣就可以在重開機的時候自動掛載了 # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # systemd generates mount units based on this file, see systemd.mount(5). # Please run 'systemctl daemon-reload' after making changes here. # # &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt; # / was on /dev/sda2 during installation UUID=82197af0-96dc-4967-90f6-05072569ebf6 / btrfs noatime,space_cache=v2,compress=zstd:1,ssd,discard=async,subvol=@ 0 0 UUID=82197af0-96dc-4967-90f6-05072569ebf6 /home btrfs noatime,space_cache=v2,compress=zstd:1,ssd,discard=async,subvol=@home 0 0 UUID=82197af0-96dc-4967-90f6-05072569ebf6 /boot btrfs noatime,space_cache=v2,compress=zstd:1,ssd,discard=async,subvol=@boot 0 0 UUID=82197af0-96dc-4967-90f6-05072569ebf6 /.snapshots btrfs noatime,space_cache=v2,compress=zstd:1,ssd,discard=async,subvol=@snapshots 0 0 # swap was on /dev/sda1 during installation UUID=bf901835-ce39-47cd-a225-d3f9c4b25685 none swap sw 0 0 /dev/sr0 /media/cdrom0 udf,iso9660 user,noauto 0 0 延伸閱讀: ArchWiki btrfs MOUNT OPTIONS, BTRFS documentation 上面的連結說明了 Mount Options 的意義，另外是官方的說明文件 上面操作結束後就 ctrl + alt + F1 回到安裝畫面，把剩下的步驟都完成就可以了，全部完成重開機之後，使用 findmnt | grep btrfs 確認一下有沒有掛載成功。 Sub-Volume &amp; Snapshot Sub-Volume 要在已經存在的 Sub-Volume 下創造 Sub-Volume 的方式就使用 btrfs subvolume create {path} 這邊可以把這個操作當成跟建立目錄一樣去建立 subvolume Snapshot 而建立 snapshot 的方式就是使用 btrfs subvolume snapshot -r {path} {snapshot} path: 目標的 subvolume 路徑 snapshot: snapshot 的存放路徑 r: read-only，避免 snapshot 之後被修改 如果想要還原的話也可以反向使用 snapshot 來還原，也就把上面的 path 和 snapshot 對調並且把 r 拿掉就可以。 延伸閱讀: Btrfs - Arch: Configuring the file system 這邊只講了很基礎的 btrfs 建立步驟，實際上 btrfs 還有很多需要調校的地方，例如: docker container 或是 tmp 這樣會頻繁寫入的地方，是否還需要使用 cow，這些都需要自己去調校，這部分之後要再研究一下。 Last Edit 01-07-2024 14:59"
  },"/jekylls/2023-12-26-code_generation.html": {
    "title": "Compiler | Code Generation (Unfinished)",
    "keywords": "Compiler Jekylls",
    "url": "/jekylls/2023-12-26-code_generation.html",
    "body": "Compilers course notes from CCU, lecturer Nai-Wei Lin. The target machine Instruction selection and register allocation Basic blocks and flow graphs A simple code generator Peephole optimization Instruction selector generator Graph-coloring register allocator The target machine 假設這是一台 Byte-addressable 的機器，每個指令由 4 個 bytes 組成，並且有 n 個 registers，這代表機器的最小尋址單位是 1 byte。 Two address instructions(雙地址指令) opsource, destination Six addressing modes(六種尋址模式) absolute M M 1 register R R 0 indexed c(R) c+content(R) 1 ind register *R content(R) 0 ind indexed *c(R) content(c+content(R)) 1 iteral #c c 1 上面的表格中，第一欄是尋址模式，第二欄是對應的表示方式，第三欄是對應的實際 bit 大小，最後一欄是對應的位址計算方式會需要幾個 word。 M: 使用到記憶體位置需要 1 個 word c: 使用到常數需要 1 個 word Example MOV R0, M /* 2 words */ MOV 4 (R0), M /* 3 words */ MOV *R0, M /* 2 words */ MOV *4 (R0), M /* 3 words */ MOV #1, R0 /* 2 words */ 6.1 Instruction Costs 一個指令的成本等於 1 加上尋址模式的成本 這個成本會跟指令的長度成正比 因此最小化指令長度也可以減少指令的執行時間 這裡很直覺的，如果一個指令的長度越小，那麼執行的時間也會越短，因為每個指令都需要一個 clock cycle 來執行，所以指令長度越小，執行的時間也會越短。 Example 假如有一個 a := b + c 的原始碼，這裡有不同種類的指令可以選擇: 搬移到 Reg 再運算，成本是 2 + 2 + 2 = 6 直接以記憶體地址運算，成本是 3 + 3 = 6 在某些情況下，如果 Reg 中已經存在 a, b, c 那麼成本是 1 + 1 = 2 如果 Reg 中存在 b, c，成本是 1 + 2 = 3 Graph Coloring 我們可以了解到 Register 的分配對於程式的效能有很大影響，但是這個問題是一個 NP-Complete 的問題，這裡有一些 approximation 的方法可以解決這個問題。 7.1 Graph Coloring 首先掃描一次程式碼，並且選擇好要使用的指令，此時可以任意分配 Symbolic register 第二遍掃描時，使用 Graph coloring algorithm 來分配 Physical register 給 Symbolic register 在第二遍掃描分配時有可能會發生衝突，此時就需要進行 Spilling Spilling: 決定那些 Register 需要被 spill，並且將 spill 的資料存到 Memory 中(類似 virtual memory 的 swap) Interference Graph 在每個 Basic block 中，都會去建立一個 interference graph 圖中的每個 Node 代表一個 symbolic register 如果一個 Node 在另一個 Node 被定義的時候仍然活躍，那麼這兩個 Node 之間會連接一條邊 K-colorable Graph 一個圖如果每個 Node 都可以被賦予 K 個顏色，並且相鄰的 Node 顏色都不相同，那麼這個圖就是 K-colorable 這裡的 Color 就是 Physical register 想要確認一個圖是否是 K-colorable，是 NP-Complete 的問題 這裡會用 Greedy algorithm 來解決這個問題 A Graph Coloring Algorithm 如果一個 Node 及其邊的 Neighbor 數量小於 K，那麼就移除這個 Node 重複上面的步驟直到所有的 Node 都被移除或者是留下的 Node 邊的數量大於 K 如果有 Node 無法移除，那麼就需要進行 Spilled 的動作 刪除最多邊的 Node，之後重複上面的步驟，直到最後所有的 Node 都被移除 現在開始依照被移除的逆序來分配 Color 給 Node 每個 Node 都要分配鄰居沒有使用過的 Color 被 Spilled 的 Node 可以使用任何 Color Example: Last Edit 1-4-2023 01:26"
  },"/jekyll/2023-12-23-virtual_memory.html": {
    "title": "OS | Virtual Memory",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-12-23-virtual_memory.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 本章節會主要介紹 Virtual Memory 這是這章節的討論範圍 Virtual Memory Hardware and Software Virtual Memory Example Page Replacement Working Set open, read, writ vs. mmap Differences between these system calls TLB miss MMU Main Function MMU 主要處理記憶體分配的問題 消除了 External fragmentation(外部碎片) 由於 Page size 僅有 4KB，所以 Internal fragmentation(內部碎片)很輕微 如果 TLB miss 假設該 TLB 要去 Mapping 的 Page 在 Memory 中只要將 Virtual memory to Physical memory 的 Mapping 加入 TLB 即可 上述的動作可以使用 Hardware 來完成，或使用 Software 來完成 如果 TLB miss 並且 Hardware 的表格也查不到呢 稱之為 Page fault What is Virtual Memory 8.1 Define the Virtual Memory 嚴格的定義: 由軟體重新定義 Page fault 當 CPU 所要存取的 Memory 根本不存在於 DRAM 中時，就稱作為 Page fault 如果我們啟動 Software 此時該 Software 所要存取的 Memory 根本不存在於 DRAM 中，就稱作為 Page fault，例如 Segmentation fault OS 設計者重新詮釋了 Page fault 的意義，例如: OS 暫時還沒把該 Page 載入到 DRAM 中，重新載入就好 例如: Execl 假如有 500MB 的資料要放入 DRAM 中以供執行，但硬碟的讀取速度有他的極限例如 20MB/s，不可能每次執行 Execl 都要花 25 秒鐘去讀取資料 先把必要的程式碼放入 DRAM 就好，等使用者真的需要使用其他功能時再去載入該功能 OS 讓不同的 Process 隱形共用內容相同的 Page 唯讀，不會有任何問題發生，還可以減少記憶體使用量，例如: fork() 使用 Copy-on-write 的技術來實作 寫入，OS 必須確保每個 Process 在邏輯上都有自己的 Page，例如: 即使 fork() 使用同一份 Memory，但是要確保程式的 Logic address 上是不同的 稍微寬鬆一點的定義: 讓 Mapping 變得更加靈活 透過設定 MMU，使 Memory Mapping 更加靈活的例子 Shard Memory，讓不同的 Process 可以共用同一份 Memory 可以將檔案 Mapping 到 Process 的 Memory Space，讓 Process 可以直接存取檔案 可以將部分 I/O 的記憶體 Mapping 到 Process 的 Memory Space，讓 Process 可以直接控制 I/O Linux 中使用 mmap 存取 /dev/mem 8.2 Virtual Memory Components 下圖是 Virtual Memory 中會需要的組件 Hardware table(Page table) 這部分由 OS 來寫入硬體所設計的表格，由 MMU 來讀取，例如: x86 定義表格，Linux 依照 x86 的定義來實作 每個 Process 都有自己的 Page table 如果 TLB miss 硬體查詢 Page table 還是找不到，此時觸發 Page fault exception Software table(mm_struct, kerne mapping table) 每個 Process 甚至 Linux Kernel 都需要一個查詢表，快速的查詢每個 Memory 是否正確 如果正確還發生 Page fault 如何處理？ 處理 Page fault OS 必須有 Page fault handler(Interrupt service routine) 並且要能紀錄: 錯誤的原因，錯誤的 Address Virtual Memory Example 8.3 Shard Memory 8.4 Demand Paging 8.5 Linux Pure Demand Paging 8.6 Performance of Demand Paging 8.7 Copy-on-Write 8.8 Kernel same-page merging 8.9 Page Table Entry Attributes 8.3 Shard Memory stack, heap 這部分是程式自己獨有的 黑色的部分是共享的: code, libc: 這兩個是同一個程式但是開了兩個 Process 因為在執行時會去 Mapping 硬碟上的執行檔，這樣 OS 就能知道其實這兩個 Process 完全是一樣的 並且這部分只能 Read，這樣就沒有 Locked 所造成的效能瓶頸 mmap: 這部分不是由 Kernel 去分享的，當 Shell 去呼叫 mmap syscall，後得到一個符號這樣就可以將該記憶體映射至自己的 Memory shell-1 將資料傳給 shell-2 要傳的是資料在 mmap 中的 offset 盡量不要去指定 mmap 要被映射的 Address Virtual Memory 允許 Code, Data 共享 多個程式使用相同的 Library，例如: libc 應用程式透過 Shared memory 共享資料，例如: mmap Parent 和 Child 都沒呼叫 execv 的話，記憶體內容幾乎一樣，例如: Copy-on-Write Linux 允許隱含式的共享 Linux kernel 可能會掃描 Memory，將內容一樣的 Page 隱含共享 例如: 電腦的桌布跟登入畫面使用同一張圖片，記憶體內容是一樣的 8.4 Demand Paging Define of Demand paging 作業系統不會立刻將使用者所需要的 Memory 配置給 User 發生 Page fault 時，才會載入該段記憶體的資料 Demand paging advantage 因為只需要載入需要的資料 Less I/O needed Less memory needed Faster response 啟動時間，只需要載入需要的資料就能啟動了 More users/programs 因為每個程式需要的記憶體較少，因此可以載入更多的應用程式 Demand paging shortcoming 對於 Disk system，Demand paging 雖然減少了讀取的量，但是會引發大量的 Random access 例如: 執行兩個程式，這兩個程式一共 100 MB，OS 可以驅動 Disk 使用兩秒鐘的時間來載入這兩個應用程式。 如果使用 Pure demand paging，假設兩個程式輪流執行，一共需要 20 MB 5120 個 pages，並且執行的程式碼「隨機散布」， 目前 Disk 的 IOPS 約為 200，則載入兩個程式的時間變為 25.6 Sec 除了引發大量的 Random access 以外，Demand paging 也可能會造成執行期間有些 Lag 例如: 不希望玩遊戲時會有延遲，就要一次把遊戲所要用的資料全部載入 Random access 對 SSD 的速度大約是 Sequential access 的 1/2 ~ 1/3，HDD 則有可能從 200MB 降低到 1MB IOPS: Input/Output Operations Per Second(每秒輸入/輸出次數) Linux 在這方面的處理方式是假如要 1個 Page，就會一次載入周遭 8 個 Page，因為通常同一個程式會連續使用到相同的 Page，這樣就能減少 Random access 的問題。 上面張圖表示了一個程式配置到 DRAM 的情況 OS 會透過 process 的 kernel mapping table 來看要去哪裡找資料 code, libc: disk stack, mmap, heap: zero-out page pool zero-out page pool: OS 會維護一個 pool 在這裡面都是已經預先初始化(0)的 page，用來分配給 stack, mmap 使用 這是為了避免資訊，例如: 上個程式的資料還留在 DRAM 中，下個程式可以從 DRAM 讀取到上個程式的資料 Valid-invalid 下圖是一個 x86 的 Page table entry，後面的 12 個 bit 就是 attribute Present: 0 的話表示該 Page 不在 DRAM 中，此時可能代表 OS 還沒有將該 page 載入 DRAM 中，或者該 page 本來就不在 DRAM 中(程式寫錯) 不管是在 page table 中的第一層或第二層，只要有一個是 0(invalid)，就會觸發 exception 此時 OS 會去查詢 process 的軟體表格(Linux mm_struct) 查詢成功: 配置 physical page 給該 address，然後修改 page table 重新執行 查詢失敗: 程式發生錯誤，例如: pointer 指向錯誤的 address，此時繼續錯誤處理(終止或呼叫 GDB) 大部分的 valid-invalid bit 都在 PTE 的最右邊，這是 CPU 設計時如果指向不存在的 page，在此時前變得 bit 都可以任意使用， Linux 就使用這些 bit 來協助解決 backing storage 為 swap space 的 page fult 8.5 Linux Pure Demand Paging Pure demand paging 的意思是在執行新的程式時，並不會分配任何 page 給 task，直到該 task 索取 以 Linux 為例子，使用 execve() 將執行檔放入當前 process 的 memory space 先在該 task 的 mm_struct 中設定 section 映射的資訊 code, data 對應到執行檔案 bss section 對應到 zero-out page pool lib 對應到相對的 library 只分配少量的 stack space(8KB) 給該 task 執行程式 Linux 並沒有分配任何 page 先給該 task，因此是 pure demand paging 8.6 Performance of Demand Paging 怎麼估算 Demand Paging 的效能，如下: Page fault rate: 0 &lt;= p &lt;= 1 估算一個 Page fault 發生的機率 Effective Access Time(EAT) $EAT = (1 - p) * memory\\,access + p(page\\,fault\\,overhead + swap\\,page\\,out + swap\\,page\\,in + restart\\,overhead)$ swap page out: Free space 不夠就需要將某些 page 從 DRAM 寫入 disk 優化方式: 無論如何都保持一定的 free space swap page in: 當 page fault 發生時，需要將某些 page 從 disk 讀入 DRAM 常用的 page 就先放入 page cache 中，例如: ls page fault overhead, restart overhead: 這兩個 overhead 通常是硬體相關的問題，或 ISR 的部分 assembly 寫的夠不夠好 OS 上主要能優化的就是 swap page out/in 這兩種方式 Example Memory access time: 200 nanoseconds Average page-fault service time: 8 milliseconds EAT = (1 - p) * 200 + p(8,000,000) = 200 + p * 7,999,800 如果每 1000 次 memory access 有 1 次 page fault，那麼速度將變為原本的 1/40 以上的例子是說明，在可能的情況下盡量少用 swap space，這樣可能會造成 CPU 大量的時間都在 idle 8.7 Copy-on-Write 在 Physical memory 中也會使用 Copy-on-Write 的技術，例如: 在剛 fork() 出來的 child process，如果 child process 沒有修改任何資料， 就不會真的複製一份 parent process 的資料，而是共用同一份資料，這樣就能減少記憶體的使用量。 通常 fork 之後的 parent 與 child 在 register 之外都是一樣的，因此直接 copy parent 的 mm_struct 給 child 將 parent 與 child 的 page table 都設定為 read-only 在 read-only 之後如果其中一個 task 去作出修改，就會觸發 page fault Linux 把造成 page fault 的 page 改成 writeable，並且複製一份 page 給 child 這樣 parent 與 child 就重新各自擁有一份可 write 的 page 這樣的技術也可以用在 File System 上，例如: Linux 的 Btrfs 8.8 Kernel same-page merging Kernel virtual memory(KVM) 中會有許多不同的 VM 都會用到相同的 page 例如: 都是執行 Win10，那麼這些 VM 中的 OS Kernel 應該都是一樣的 在一般的 OS 也可能會有相同的 Page 例如: 桌面與登入畫面使用相同的圖片，這樣就會有相同的 Page 上面的情況都比較難有真正的事件發生，因此需要 OS 去主動掃描才能發現共用記憶體，節省記憶體的使用量 8.9 Page Table Entry Attributes 在 Main Memory 中介紹的是 PTE 的前 20 個 bit，這邊會更詳細的說明後 12 個 bit 的意義 Present: 是否在 DRAM 中 Read/Write: 0 表示 read-only，1 表示 read/write User/Supervisor: 表示是否需要 kernel mode 權限才能存取 Write-through: 決定 page 使用的是 write-back 或 write-through Cache Disable: 如果是 1 則禁止該 page 被 cache Accessed: 表示該 page 是否被存取過 Dirty: 表示該 page 是否被修改過 Page Table Attribute Index: 跟 Write-through/Cache Disable 有關，可以設置更細緻的 Cache 設定 Global: 表示該 page 是否是 global page，例如: 共享的 library Available: 這 3 個 bit 可以讓 OS 自由使用 而一個 4KB 的 Page table 可以放入 1024 個 PTE，然後在 Main memory 提過的 Hierarchical pading 在這裡可以更詳細的說明， 這樣的話架構就會如下(以 4 個 PTE 組成一層 Page table 為例): 這樣我們就能把 Page table, TLB, Virtual memory, Physical memory 組合起來，如下: Virtual address 透過 TLB 轉換成 Physical address TLB 紀錄 virtual address 與 physical address 的 mapping attribute 也會紀錄在 TLB 中 TLB miss 會觸發 page fault，透過 CR3 register 找到 page table Page table 紀錄 virtual address 與 physical address 的 mapping page table 找到 physical address 後去更新 TLB，然後重新查詢 如果 PTE 是 invalid，就會觸發 page fault，再去載入該 page 到 DRAM 中 到這裡為止就是完整的 Virtual memory 到 Physical memory 的運作方式 Page Replacement 在這之前所討論的都是在 DRAM 足夠滿足程式大小的情況，但是實際上 DRAM 並不一定能滿足程式所需要的大小 8.10 Why Need Page Replacement 8.11 Page Replacement Concept 8.12 Page Replacement Algorithm 8.13 Algorithm Evaluation Example 8.14 Linux’s Page Replacement 8.10 Why Need Page Replacement 主要的原因就是 DRAM 有限，例如: 使用者同時執行了 Excel, Word, Chrome，這三個程式的大小加起來超過了 DRAM 的大小， 或者是使用者正在看一部影片，但是這部影片的大小超過了 DRAM 的大小。 例如一個 6.2GB 的影片，使用者只有 6GB 的 DRAM，就先把 200MB 的資料寫入 swap space，需要時再從 swap space 讀取 因為 Virtual memory 的存在，所以 OS 可以只載入目前所需要的資料 隨著程式的執行，過去所需要的資料會不斷累積，但不代表這些資料可以被丟棄 因此當 DRAM 不足時，OS 要考慮是否把這些資料寫出到 secondary storage 或丟棄 例如: 一個 word 的程式用來初始化 word，但是接下來不會再使用，這時就可以把這些資料丟棄 8.11 Page Replacement Concept 在 memory 中我們怎麼找出最近不會使用到的 page，這就是 page replacement algorithm 的目的 這個 algorithm 設計的好可以讓使用者感覺不到 page replacement 的存在 當然也有可能設計的不好，造成使用者感覺到 lag 在不考慮 page-out/page-in overhead 的情況下，user 可以使用的 memory 遠超 DRAM 的大小 假設 user 執行的應用程式共需要 12GB 的記憶體，但 user 只有 4GB 的 DRAM 依照未來會存取的順序來對 12GB 的記憶體做排序，就可以知道哪些 page 優先被放入 DRAM 為什麼只要仔入部分資料就可以 例如: 10秒鐘內，user 能存取的資料不會超過 4GB(locality) 如果超過 4GB，那麼就把部分資料 page-out，然後 page-in 需要的資料 實際在 Linux 執行中可以使用 free -h 去查看使用的 Men, Swap，Linux 並不會真的把 Men 使用光，而是會保留一部分的 Men 做為 page cache， available 則是在不使用 swap 的情況下可以分配多少記憶體 那麼在 512MB 的電腦上執行 6.3 GB 的程式，會發生什麼 理論上系統的所有被使用的記憶體大小不能超過 DRAM + Swap OS kernel 本身是 non-swappable 所以 OS 本身所需要的記憶體都會直接配置在 DRAM 中，這裡並不是 kernel 不能被設計成 swappable，而是這樣的設計不合理 OS 會在必要時 kill 掉一些 task，稱為 OOM killer(out-of-memory killer) 8.12 Page Replacement Algorithm 這裡要討論的就是那些 page 應該被丟棄，這就需要 page replacement algorithm 來決定 Algorithm 的目標: 最小化 page fault 的數量 這裡要看優化的目標是什麼，在這裡討論的是最小化 page fault 的數量 在這裡會討論以下幾種 algorithm FIFO: First-in-first-out Optimal algorithm 概念上的最佳化，必需假設知道未來的 Page access pattern LRU: Least-recently-used 過去不曾用到的 page，未來也不太可能會用到 實現時會增加硬體上的成本，因為要加入一個時間戳記 Second-chance 有效且低成本的方式來實現接近 LRU 的效果 LFU: Least-frequently-used 過去最不常用的 page，未來也不太可能會用到 MFU: Most-frequently-used 過去最常用的 page，可能代表已經使用結束了，所以之後不會用到 Linux’s page replacement Basic assessment method 在等等評估演算法的方式中，會使用以下的方式來評估: 在給定的 Reference string 中，計算 page fault 的數量 Reference string: 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 這個 reference string 代表了一個程式的 page access pattern 在這裡假設系統只有 3 個 frame 8.13 Algorithm Evaluation Example FIFO 替換掉最早進入的 page 所以當 4, 0, 1, 2 時，將 4 Swap out Optimal algorithm 提早替換掉未來最久不會使用到的 page 所以當 4, 0, 1, 2 時，發現未來不會使用到 1，因此將 1 Swap out 接下來到 0, 3 時發現 0 還有三次 Access 才會使用，最久因此將 0 Swap out 實際上這樣的演算法是不可能的，因為我們不可能知道未來的 Access pattern LRU 我們無法得知未來的 Access pattern，但是可以知道過去的 Access pattern 替換掉最久沒有使用的 page 所以當 4, 0, 1, 2 時，將 4 Swap out 接下來到 0, 3 時發現 1 是最後使用的，因此將 1 Swap out Second-chance(clock algorithm) LRU 如果要在硬體上實現，需要一個時間戳記，但是這樣會增加硬體的成本，因此有了 Second-chance Second-chance 可以用 PTE 中的 Accessed bit(也稱為 Reference bit)來實現，當 page 被存取時，OS 會將 Accessed bit 設為 1，OS 會定期去清除 Accessed bit 改為 0。 Dirty bit 則是在 page 被修改時，OS 會將 Dirty bit 設為 1，如果把 Dirty bit 為 1 的 page-out，就代表必須要跟 disk 做同步，因此會增加 page-out 的時間。 如果以上兩個 bit 都是 0，就代表這個 page 是可以被 page-out 並且成本低的 實際上剛載入的 memory 的 page accessed bit 都是 0，因此會先被替換掉 這樣假設 A 剛載入一個 page p, 此時 B ctx 他有可能會把 A 的 page p 替換掉，這樣回到 A 又要重新載入 p 解決方式是在一些 OS 上會有 young bit，或者是在載入時就將 accessed bit 設為 1 紅色的字體就是剛 Access 的 page，在實作上可以有很多方式 這裡使用一個 Pointer 依序指向 page，當 page 被指向第二次時並且 accessed bit 為 0，這個可以 page-out Second-chance 有很多種實作方式，這裡只是其中一種 LFU 上方的橘色區塊代表該 page 被使用的次數 當需要 swap out 時，就會找到使用次數最少的 page page-out 如果使用次數都一樣這裡就假設 FIFO MFU 與 LFU 相反，找到使用次數最多的 page page-out Conclusion Optimal algorithm: 理論上最佳化，但是不可能知道未來的 Access pattern LRU: 基於過去預測未來，實際上的效能不錯 但是要完全實現 LRU 需要增加硬體成本，例如: 紀錄 page 上次被存取的時間 Second-chance: 有效且低成本的方式來實現接近 LRU 的效果 如果經常被存取，那 Access bit 會一直被設為 1 要 replancement 的時候會給有 Access bit 的 page 一個 second chance 很接進 LRU 的方式，相當於使用 1 個 bit 來當作時間戳記 FIFO: 簡單，但是效能不好 因為放在 memory 中多久跟 page 是否會被使用沒有直接關係 LFU: 保留過去常用的 page，需要一些機制來消除累積的 page 次數，不然有可能造成某些 page 一直被留在 DRAM 中 MFU: 提除過去常用的 page，通常是在特定情況下才會有較好的效果 8.14 Linux’s Page Replacement Linux 使用了兩個 LRU list 來實現 page replacement 設計兩個 LRU 的好處是避免一些 page 去汙染 active list，例如: 一個一次性但很大的程式，那有可能就會把 active list 中真正需要的 page 踢出去， 有 inacitve list 就可以讓這些一次性的 page 保留在 inactive list 中，不會影響到 active list。 兩個 LRU list: active, inactive list Active list: 最近比較活耀的 page Inactive list: 最近比較不活耀的 page incative list 會定期的去檢查在一段時間內是否有 page 被存取過兩次，如果有就會被移動到 active list head 從 Active/Inactive list 移除 page 使用的是 Second-chance algorithm New page 通常會被放入 inactive list 的 head Inactive list 被填滿，就會將 tail 的部分 page 移到 swap space Active list 被填滿，就會將最 tail 的 page 移到 inactive list head 這樣的好處是，page 在 inactive list 中一直沒有被存取，就會慢慢移動到 tail，最後就會被移出 DRAM page 離開 inactive list 時，會把該 page 紀錄在 table 中，這個 table 只會紀錄最近丟棄的 page 如果有 new page 進入時，就會檢查 table 中是否有相同的 page，如果有就把該 page 移到 active list head 如果最近讀入的 page 剛好在 swap-out table 中，就表示演算法有失誤，例如: 這個 page 會被常常使用，只是存取的間隔比 inactive list 的時間長。 所以這裡會選擇把這個 page 移到 active list 丟進 active list 可以給他比在 inactive list 更長的時間有機會被存取 這裡可以討論的是是否要記錄這個間隔有多長，要不要以此去調整 inactive list 的時間 延伸閱讀: Linux Page Replacement 關於這個 Swap-out table 的設計，他的靈感或許來自 「Adaptive Replacement Cache」(IBM) 使用 Active + Inactive 融合了 LRU + LFU，只要常常被存取，即使不是最近被存取的也不太容易被移出 DRAM 在此之下可以思考的是，是否可以把 Active + Inactive 變成更多層的結構 Linux Page Replacement Steps 要注意這裡所提到的 Active/Inactive list 是 OS 上的資料結構，所以真正的 Asecced bit 是在 page table 上， 這兩個 list 所儲存的是 DRAM 中的 physical address。 在 OS 上的資料結構如何去知道 Access bit 的變化，實際的 Access bit 是存在 task 的 page table 上: 掃描 Inactive list，要反向的從 physical page 去找出對應的 page tabe 從 page table 中把該 Page 的 Access bit mapping 到 Inactive list 的 page 因為 Linux 中的 Inactive list 是使用 Second-chance algorithm，因此是沿著 circular list 不斷往下掃描每一個 entry， 逐一去更新 entry 中的 Access bit 即可。 這樣移除的時候也要反向的去找出對應的 page table，然後更新 page table 中的 valid bit，然後就將該 page 移到 swap space， mm_struct 中的 page table 會記錄這個 page 的使用情況是在 swap space 還是在 disk 中。 Last Edit 1-31-2023 21:14"
  },"/jekyll/2023-12-20-system_testing.html": {
    "title": "Testing | System Testing",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-12-20-system_testing.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 前面講的部分都是如何去產生測試案例，這裡會介紹 Mutation Testing，這是一種可以評估測試案例的方法 System Testing 通常是在 Integration Testing 之後，並且是內部測試的最後一個階段，因此此時通常會在一個完整的系統(包含軟體與硬體)上進行， 用來評估系統是否符合其需求規格。 系統測試通常是屬於黑箱測試的範疇，因此不需要對程式碼或邏輯的內部設計有所了解。 System Testing 8.2 Performance Testing 8.3 Load Testing 8.4 Stress Testing 8.1 System Testing 系統測試所使用的輸入資料是所有已經成功完成集成測試的整合軟體組件。 這是 User acceptance testing(使用者驗收測試)前的最後一個階段，也就是最後的內部測試階段。 在這個階段測試可以採用一種接近破壞性的測試方式，這代表測試不僅僅侷限於驗證系統是否符合需求規格，也會去驗證系統是否能夠承受極端的使用情境，還是否能符合預期。 因此在這個階段測試軟體/硬體在要求規格中所定義的邊界，甚至超出邊界的情況 Types of System Testing 性能測試(Performance testing) 負載測試(Load testing) 壓力測試(Stress testing) 可用性測試(Usability testing) 安全測試(Security testing) 8.2 Performance Testing 性能測試的目標不是發現錯誤，而是消除性能瓶頸並為未來的回歸測試建立基準 進行性能測試代表需要一個可以謹慎控制的測量與分析的過程 理想情況下是正在測試的軟體已經足夠穩定，使這個過程可以專注於性能問題 以下會介紹性能測試的重要步驟 Set of Expectations 一套清晰定義的預期結果對於有意義的性能測試至關重要，如果測試者根本不知道在性能方面要達到什麼目標， 那麼在測試過程中所採取的任何行動都是毫無意義的 例如: 對於一個網路應用程式，至少要知道兩件事 在一個給定的時間內，系統應該能夠處理多少個同時連線的使用者 系統應該能夠在多少時間內回應使用者的請求 Looking for Bottlenecks 一旦已經了解想到達到的目標，就可以開始不斷增加系統的負載來開始測試，同時尋找系統中的瓶頸 例如: 以之前的網路程式為例 這些瓶頸可以存在於多個層面上，為了找到這些瓶頸，可以使用各種工具來監控系統的狀態 這是一些工具的例子: Application level: Developer 可以使用一些分析工具來找出程式碼中的低效之處 Database level: 使用特定於資料庫的分析器與查詢優化 Operating system level: System Engineer 可以使用如 top, vmstat, iostat(on Unix) 或者 PerfMon(on Windows) 之類的工具來監控硬體資源， CPU, memory, swap space, disk I/O, network I/O 等等… Network level: Network Engineer 可以使用如 tcpdump, Wireshark, iptraf 之類的封包嗅探器，網路協議分析器如 ethereal， 以及各種工具如 netstat, MRTG, ntop, mii-tool 等等… Performance Tuning 當 Load testing 的結果顯示系統效能沒有達到預期目標時，就是進行調整的時候了，假設從 Application/Nework level 開始 這時就要確保程式碼中沒有低效之處 確保程式碼中沒有不必要的 DB Query，並且 DB 在特定的 OS/Hardware 上的配置下是最佳化的 JUnitPerf 實踐 TDD(Test-Driven Development) 的開發者會發現，像是 Mike Clark 所寫的 JUnitPerf 這樣的工具在 JUnit 中是非常有用的， 他可以通過增加 Load testing 和增加計時測試功能來增強現有的單元測試代碼 一旦重構某個特定函數或方法後，就可以使用 JUnitPerf 來確保其性能沒有受到影響，Mike Clark 稱其為 Continuous Performance Testing(持續性能測試) 延伸閱讀: JUnitPerf 當然有可能即使 Application level 的程式碼已經是最佳化的，但是系統的瓶頸還是存在於其他層面，因此系統的性能還是沒有達到預期目標， 那麼在前面討論過的所有 Level 上都還有許多調整工作可以進行 這裡列出一些可能的方法，這些方法都不局限於軟體的 Source code Use Web cache mechanisms, such as the one provided by Squid. Publish highly-requested Web pages statically, so that they don’t hit the database. Scale the Web server farm horizontally via load balancing. Scale the database servers horizontally and split them into read/write servers and read-only servers, then load balance the read-only servers. Scale the Web and database servers vertically, by adding more hardware resources (CPU, RAM, disks). Increase the available network bandwidth. One Variable at a Time 這裡就跟機器學習中的 Hyperparameter tuning 一樣，以列舉的方式來找出最佳的參數，並且一次只會調整一個參數 進行性能測試時，有時更像是一種藝術而不是科學，因為有太多的變數需要考慮，而且這些變數之間的關係也很複雜 每次調整效能時，必須警慎的一次修改一個變數，並再次測量數據 Staging Environment 通常會有一個獨立的環境，用來進行性能測試，這個環境通常稱為 Staging Environment 這個環境或許不能複製 Production Environment 的所有特性與效能，但是至少要能夠模擬 Production Environment 的行為 在這個預期之下系統的預期性能可以相應的進行縮放 現在測試的環境通常都是在雲端上，因此可以很容易的進行縮放 Baseline 在運行時要尊從「Run load test -&gt; Measure performance -&gt; Tune System」 的循環重複進行，直到被測試的系統達到預期的效能為止。 並且要在系統中建立一個 Baseline，這個 Baseline 代表系統在預期的負載下所能達到的效能 這是為了在 Regression Testing(回歸測試)中，可以透過這個 Baseline 來衡量系統的效能表現 Benchmarks 另一種方式是建立一個 Benchmark，這個 Benchmark 代表系統在預期的負載下所能達到的效能，並且這個 Benchmark 會被用來衡量系統的效能表現 這就很像手機每次發表新的型號時，都會使用 AnTuTu Benchmark 來衡量其效能表現一樣 各家廠商可能會去針對這個 Benchmark 進行優化，好拿到更好的成績 8.3 Load Testing 在測試相關的文章中，Load testing 通常被定義: 「通過向被測系統提供他所能夠操作的最大任務來進行系統練習的過程」 Load testing 有時也被稱作 Volume testing(容量測試)或者 Longevity/Endurance testing (耐久性/持久性測試) Example of Volume Testing 通過編輯一個非常大的文件來測試一個文字編輯器 通過向打印機發送一個非常大的文件來測試一個打印機 對於一個 Mail Server，通過向其發送大量的郵件來測試容量 這裡有一個特別的測試是 Zero-volume testing，這是一種測試，用來測試系統被提供空任務時的行為 Example of Longevity/Endurance testing 通過長時間內讓 Clien 對 Server 進行循環發送請求來測試一個 Client-Server 系統 延伸閱讀: Volume testing Goals of Load Testing Load testing 的目標如下 檢查再粗略的測試中不會被發現的錯誤，例如: Memory management bugs, Memory leaks, Buffer overflows, etc. 確保系統能滿足性能測試中建立的 Performance baseline 這些都是通過指定的最大負載下對程式運行回歸測試來完成的 Performance vs. Load Testing Performance testing 通常使用負載測試技術和工具來進行測量與建立 Baseline, 或使用 Benchmark 來進行測量 Load testing 是在一個預定地負載下對系統進行測試，通常是系統在仍然能正常運作下可以承受的最大負載 注意 Load testing 並不是通過壓倒性的負載來破壞系統，而是試圖讓系統能像一台嚴密潤滑的機器一樣運作 Large Datasets 在 Load testing 中「大型的資料集是非常重要的」 因為很多重要的錯誤只有在處理非常大的實體時才會出現: LDAP/NIS/Active Directory 中成千上萬的使用者，成千上萬的 mailboxes，Database 中的 multi-gigabyte tables，File 上的超深層文件/目錄等等… 8.4 Stress Testing Stress testing(壓力測試)是試圖通過壓倒系統資源或從系統中奪走資源來破壞被測系統(也被稱為負面測試 Negative testin)。 這種測試的目的是確保系統能夠在壓力下能夠 Graceful Failure(優雅的失敗)並恢復，這種被稱為 Recoverability(可恢復性)的能力。 在 Performance testing 中，系統的負載是在可控的情況下，但壓力測試則需要引入混亂與不可控性 Example of Stress Testing 將一次可以連線的最大用戶數量翻倍 隨便拔除 Server 中的網路線，例如透過 SNMP 來控制 斷開 Database 的連線，然後重新啟動 系統運行中拔掉硬碟，然後重新插入新的硬碟重組 RAID 在伺服器上運行其他會強佔消耗資源的 Process Goals of Stress Testing Stress testing 不僅僅是為了破壞系統而破壞他，而是讓測試人員觀察系統對失敗的反應 是否有保存狀態，還是直接崩潰 是中斷狀態後恢復，還是優雅的失敗 重啟後，他能從最後一個狀態恢復嗎 是否有向用戶提供有用的 Log 訊息，還是顯示難以理解的 Hex dump 因為意外的失敗，系統的安全性是否受到威脅 Last Edit 1-2-2024 05:23"
  },"/jekyll/2023-12-12-mutation_testing.html": {
    "title": "Testing | Mutation Testing",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-12-12-mutation_testing.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 前面講的部分都是如何去產生測試案例，這裡會介紹 Mutation Testing，這是一種可以評估測試案例的方法 Mutation Testing 7.1 What is Mutation Testing? Mutation testing(突變測試)是一種基於錯誤的測試技術 如果一個測試能檢測出大部分簡單的錯誤，那麼這個程式就已經被充分的測試了 突變測試會去產生一組原程式的缺陷版本，這些缺陷版本被稱為突變體(mutant)，每個突變體都只包含一個簡單的錯誤， 通常是針對程式的一些運算子做修改，讓他與原程式不同造成錯誤。 上圖是一個突變的範例，把 &gt; 改成 &lt;, &gt;=, &lt;=, ==, != 這樣就產生了五個突變體 Mutation Analysis Process 突變分析的流程: 首先以一組測試案例來測試原程式，這裡應該要確保原程式是能通過測試的 同樣的一組測試案例來測試突變體 如果一個突變體產生了與原程式不同的輸出，那麼這個突變體就被殺死了 一個測試案例如果至少殺死了一個突變體，那麼這個測試案例就被認為是足夠有效的 Equivalent Mutants Equivalent Mutants(等價突變體): 如果兩個程式在每個輸入上始終保持相同的輸出，那我們可以說這兩個程式在功能上是等價的 突變體也是一樣，如果突變體與原程式是等價的，那麼這個突變體就是等價突變體 因為是等價的，所以突變體並不會被測試案例集殺死 以之前的例子為例，下面這個原始程式與突變體就是等價的，這邊要注意到即使輸入兩個相同的變數，原始程式返回 b，突變體返回 a，但這個程式在輸出上始終保持相同的輸出，所以這兩個程式是等價的。 int max(int a, int b) { if (a &gt; b) { return a; } else { return b; } } int max(int a, int b) { if (a &gt;= b) { return a; } else { return b; } } 7.2 Mutation Score Mutation Score(突變分數)，就是一組測試案例所可以殺死的非等價突變體的比例 P: Program T: Test Case Set K: Number of killed mutants M: Number of mutants E: Number of equivalent mutants 以下是突變分數的公式，分數越高代表測試案例集越有效，可以殺死更多的突變體 \\begin{flalign} &amp; \\text{MS(P, T)} = \\frac{\\text{K}}{\\text{M - E}} &amp; \\end{flalign} Measuring Mutation Score of a Test Case Set 如果一組測試案例集的突變分數為 100% 就代表是 Mutation adequate(突變充分的)，這組測試案例集可以殺死所有的非等價突變體。 但是在真實的開發中，要擁有一組突變分數到達 95% 以上的測試案例集是非常困難的。 7.3 Mutation Operators Mutation Operators(突變運算子)是對原程式做修改的規則: 突變運算子的集合取決於所使用的程式語言 Mothra 是一個用於 Fortran 77 程式的突變測試工具，他提供了 22 種突變運算子 延伸閱讀: The Mothra tool set (software testing) 這裡列出 Mothra 提供的突變運算子: AAR: array reference for array reference replacement ABS: absolute value insertion ACR: array reference for constant replacement AOR: arithmetic operator replacement ASR: array reference for scalar variable replacement CAR: constant for array reference replacement CNR: comparable array name replacement CRP: constant replacement CSR constant for scalar variable replacement DER: DO statement end replacement DSA: DATA statement alternations GLR: GOTO label replacement LCR: logical connector replacement ROR: relational operator replacement RSR: RETURN statement replacement SAN: statement analysis SAR: scalar variable for array reference replacement SCR: scalar for constant replacement SDL: statement deletion SRC: source constant replacement SVR: scalar variable replacement UOI: unary operator insertion Categories of Mutation Operators 我們可以把突變運算子分成三種類別: Replacement of Operandoperators: 把程式中的每個 Operand 都替換成其他的 Operand AAR, ACR, ASR, CAR, CNR, CSR, SAR, SCR, SRC, SVR Expression Modification Operators: 把程式中的每個 Operator 都替換成其他的 Operator ABS, AOR, ROR, UOI Statement Modification Operators: 把程式中的每個 Statement 都替換成其他的 Statement DER, DSA, GLR, LCR, SDL High Cost of Mutation Testing 突變測試的主要缺點是運行大量突變體對抗測試案例集的成本非常高 為一個程式產生的突變體數量與數據引用數和數據對象數的乘積成正比 即使是一個小程式，這個數字也是相當大的 Reducing Cost of Mutation Testing 這裡提出幾種可能減少突變測試成本的方法: Selective Mutation: 做更少的方法 Weak Mutation: 做的更聰明的方法 Schema-Based Mutation: 做的更快的方法 MuJava 是一個針對 Java 的突變測試系統，實現了這三種方法，同時在 Eclipse 上也有提供插件 MuClipse 7.5 Selective Mutation Selective Mutation(選擇性突變): 是指只針對一些特定的方法做突變，忽略那些會造成大量突變體的 Operator N-Selective Mutation(N-選擇性突變): 忽略最常導致突變體產生的 N 個 Operator 像是 Mothra 來說就省略了 SVR、ASR 兩種 Operator SVR、ASR 兩種 Operator 是會產生最多突變體的前兩名 Operator Compare Selective Mutation with Non-Selective Mutation 假如我們使用了「一組測試案例」去針對 Selective Mutation 來達到 100% 的突變分數，如果這組測試案例在 Non-Selective Mutation(非選擇性突變)的突變分數也能表現良好， 那麼就能以此來證明 Selective Mutation 是有效的。 Non-Selective Mutation: 針對所有的 Operator 做突變 Experiments 來自 An experimental determination of sufficient mutant operators 的實驗結果，它們使用多個程式來進行突變，並在之後比較四種不同的突變方法: 最終的結論是，實際上只要保留五個關鍵的 Mutation Operator 就能提供與 Mothra 中的 22 個 Operator 幾乎相同的覆蓋率， 並且成本在小型程式中減少了四倍，在大型程式中減少了五十倍。 這五個關鍵的 Operator 是: ABS, AOR, LCR, ROR, UOI 7.6 Weak Mutation Weak Mutation(弱突變) 是一種近似的突變測試方法，他在程式的突變位置執行後立刻去比較突變體與原程式的內部狀態， 已經有實驗證明，Weak Mutation 與 Strong Mutation(強突變) 一樣有效，並且成本降低至少 50%。 Four Potential Points for State Comparison 那我們需要考慮的就是在什麼時間點去比較突變體與原程式的內部狀態，這裡提出了四個可能的時間點: 在突變符號的 Innermost Expression 進行第一次評估之後 在突變 Statement 進行第一次評估之後 在包含突變語句的 Basic Block 進行第一次評估之後 在包含突變語句的 Basic Block 全部評估之後 while() { if() { y = x + /* 1 */ 1; /* 2 */ } /* 3 */else { ... } /* 4 */ } Schema-Based Mutation Analysis Schema-Based Mutation Analysis(基於模式的突變分析)是一種弱突變的方法 將所有突變體編碼到一個可以參數化的 Source-level program(源程式)中，稱為 Meta-Mutant(元突變體) 元突變體使用與編譯原始程式相同的編譯器進行編譯 元突變體具有運行時充當任何突變體的能力 這樣成本就能減少到一個數量級 Metaoperators Metaoperators(元運算子)是一種用來產生元突變體的突變運算子，例如: AOR metaoperator: 用來產生 AOR 元突變體 對於 Statement Result = A - B，可以產生四種突變體: Result = A + B Result = A * B Result = A / B Result = A % B 那麼這些突變體可以通用的表示為 Result = A ArithOp B，其中 ArithOp 是 Metaoperator Metaprocedures 把上面的方法重寫為 Result = AOrr(A, B)，其中 AOrr 是 Metaprocedure 在 AOrr 中會執行 Result = A ArithOp B 這五種可能的突變體 這樣就可以把所有的突變體都編碼到一個可以參數化的 Source-level program 中 Class Level Mutation Operators in MuClipse Access control: AMC Inheritance: IHD, IHI, IOD, IOP, IOR, ISK, IPC Polymorphism: PNC, PMD, PPD, PRV Overloading: OMR, OMD, OAO, OAN Java-specific features: JTD, JSC, JID, JDC Common-programming mistakes: EOA, EOC, EAM, EMM 這些是 MuClipse 所提供的 Class-level 突變運算子，這些突變運算子都是針對 Java 語言所設計的 7.7 Equivalent Mutants 這裡做了一個實驗，對於 6 個程式一共 866 個 Class 進行不同層級的突變，來查看等價突變體的比例: 在 Method-level 突變測試中，Equivalents Mutants 的比例約為 5% ~ 15% 在 Class-level 突變測試中，Equivalents Mutants 的比例平均為 75% Last Edit 1-2-2024 02:51"
  },"/jekyll/2023-12-10-main_memory.html": {
    "title": "OS | Main Memory",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-12-10-main_memory.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. Physical configuration of DRAM on PC DRAM + BIOS + MMIO Three main functions of DRAM Segmentation memory management Paging memory management Linux memory layout External fragmentation x86 Start - BIOS Address 在一台 PC 剛開機後唯一能讀取的記憶體就是 BIOS(ROM)，必須透過 BIOS 才能完成後續 OS 的內容載入到 DRAM 中 x86 在啟動的時候，執行的第一個指令是在 0xFFFFFFF0 這個位址 因此主機板上必須使用 Address decoder 將 0xFFFFFFF0 對應到 BIOS 的 Entry point BIOS 設定完畢後，會進入 OS 的 Entry point 例如: Linux 的 start_kernel BIOS in Memory Usage BIOS 雖然不需要電力就能保存，但是通常速度會比 DRAM 慢，所以大部分的 BIOS 會把自己複製到 DRAM 中， 並且配置 data, stack, heap section，然後在 DRAM 中執行。 為什麼不使用 Flash memory，因為 Flash memory 是 Block device 而 CPU 能讀取的必須是 byte addressable Memory Layout 下圖是如果記憶體只有 256MB 或 512MB 的 Memory Layout，在 32-bit 的 CPU 中，最多只能使用 4G 的定址空間 DRAM 之外的空間就是 MMIO，這部分的空間通常只能透過 Kernel 存取 注意到他把 BIOS 映射到最上層的位置，這樣 BIOS 就不會去占用 DRAM 的定址空間 延伸閱讀: System address map initialization in x86/x64 architecture part 1: PCI-based systems 假如一個 4G 定址空間的機器裝上 4G 的 DRAM，那麼扣除 PCI 的空間後，只會剩下 3.25G 的空間可用 最好的解決方式就是邁入 64-bit 的時代，不只解決了 Memory address 的問題，也解決了檔案不能超過 2G 的問題 延伸閱讀: Why does my motherboard see only 3,25 GB out of 4 GB RAM installed in my PC? 7.1 BIOS - Memory Map 使用 dmesg 可以看到 Linux 在啟動時的 BIOS-e820 Physical Memory 配置，可以看到幾個區域 usable: 可以使用的記憶體 reserved: 保留給 BIOS 或是其他的 Device 使用 ACPI data/NVS: 配置給 ACPI 或 NVS (Non-Volatile Storage)使用 [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009f7ff] usable # 0.62 MB [ 0.000000] BIOS-e820: [mem 0x000000000009f800-0x000000000009ffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000000ca000-0x00000000000cbfff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000000dc000-0x00000000000fffff] reserved [ 0.000000] BIOS-e820: [mem 0x0000000000100000-0x00000000bfeeffff] usable # 3069.94 MB [ 0.000000] BIOS-e820: [mem 0x00000000bfef0000-0x00000000bfefefff] ACPI data [ 0.000000] BIOS-e820: [mem 0x00000000bfeff000-0x00000000bfefffff] ACPI NVS [ 0.000000] BIOS-e820: [mem 0x00000000bff00000-0x00000000bfffffff] usable # 1 MB [ 0.000000] BIOS-e820: [mem 0x00000000f0000000-0x00000000f7ffffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000fec00000-0x00000000fec0ffff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000fee00000-0x00000000fee00fff] reserved [ 0.000000] BIOS-e820: [mem 0x00000000fffe0000-0x00000000ffffffff] reserved [ 0.000000] BIOS-e820: [mem 0x0000000100000000-0x000000023fffffff] usable # 5120 MB 實際分配給電腦的記憶體是 8192 MB，而經由 BIOS 分配以後為 8191 MB，可以發現這些 usable 的地址其實是不連續的， 有些地方映射到 DRAM，有些地方映射到 MMIO。CPU 會去透過 Address decoder 將 Physical address 映射到不同裝置的記憶體。 DMA 與 DMA32 是保留給 16/32-bit 的 DMA controller 使用 Normal 就是任何能夠使用 64-bit address 的裝置 Movable zone 是用來處理 Hotplug 的記憶體，例如: USB Keyboard unavailable ranges 代表該區域的記憶體不可使用 可能因為 BIOS 或是其他的裝置使用了這些記憶體 [ 0.014535] Zone ranges: [ 0.014536] DMA [mem 0x0000000000001000-0x0000000000ffffff] [ 0.014539] DMA32 [mem 0x0000000001000000-0x00000000ffffffff] [ 0.014541] Normal [mem 0x0000000100000000-0x000000023fffffff] [ 0.014543] Device empty [ 0.014545] Movable zone start for each node [ 0.014548] Early memory node ranges [ 0.014548] node 0: [mem 0x0000000000001000-0x000000000009efff] [ 0.014550] node 0: [mem 0x0000000000100000-0x00000000bfeeffff] [ 0.014553] node 0: [mem 0x00000000bff00000-0x00000000bfffffff] [ 0.014554] node 0: [mem 0x0000000100000000-0x000000023fffffff] [ 0.014558] Initmem setup node 0 [mem 0x0000000000001000-0x000000023fffffff] [ 0.014577] On node 0, zone DMA: 1 pages in unavailable ranges [ 0.014636] On node 0, zone DMA: 97 pages in unavailable ranges [ 0.027855] On node 0, zone DMA32: 16 pages in unavailable ranges ... [ 0.102320] Kernel/User page tables isolation: enabled Kernel/User page tables isolation 是用來避免 SPECTRE 之類的攻擊，讓 Kernel 有自己的 Page table，缺點是會降低 System call 的效能 延伸閱讀: 实模式启动阶段：从bios获取内存信息 cat /proc/iomem 可以看到 Physical address 的配置 與之前的 dmesg 顯示的內容相同 並且 kernel 會配置自己的 code, rodata, data, bss section 00001000-0009f7ff : System RAM # 0.62 MB 00100000-bfeeffff : System RAM # 3069.94 MB bff00000-bfffffff : System RAM # 1 MB 100000000-23fffffff : System RAM # 5120 MB 8f400000-90201b41 : Kernel code 90400000-90cd5fff : Kernel rodata 90e00000-91046d3f : Kernel data 916fe000-927fffff : Kernel bss DRAM Main Functions 7.2 Buffer/Cache 7.3 Program Layout 7.4 Memory Segmentation 7.5 Memory Paging 7.6 Fragmentation Problem 這裡先介紹 DRAM 的特性: DRAM 的特性是只要 CPU 開始工作，DRAM 就會全速運轉 即使 DRAM 有多個插槽，OS 會把他們視為一體 DIMM (Dual In-line Memory Module) 通常插滿記憶體代表容量變大，頻寬變大，但 OS 無法獨立控制每個記憶體 NUMA (Non-Uniform Memory Access) 有多個 CPU 的情況下，每個 CPU 都有自己的 DRAM，但是 CPU 之間可以透過 QPI 互相存取 這種情況下 OS 可以獨立控制每個 CPU 的 DRAM，例如: 雙路的 Server 的架構 記憶體無論如何都要耗電 DRAM 不管是讀取或寫入或儲存都需要耗電，所以 OS 會把 DRAM 全部用完 如果某一個區段沒有真正的用途，那還是要耗電保存這部分 garbage 記憶體的主要用途 執行程式的時候使用 充當 Cache Memory 的角色，主要是當作 HDD, SSD 的 Cache 充當 Buffer 的角色，再與周邊 Device 溝通時，需要保留一塊 Memory 讓裝置可以存取，例如: DMA 7.2 Buffer/Cache 這裡使用 top 查看記憶體使用情況 OS 會盡力把記憶體用完，例如: 拿來當作 Cache, Buffer 讓 I/O 更快 類似 memory cleaner 這樣的軟體，去釋放記憶體反而可能導致 I/O 變慢 MiB Mem : 7939.9 total, 7495.9 free, 357.6 used, 314.9 buff/cache MiB Swap: 953.0 total, 953.0 free, 0.0 used. 7582.3 avail Mem 7.3 Program Layout 這裡說明 Program 在記憶體中的配置，以及他們的用途這部分就不多做介紹了，可以參考下圖。OS 在這裡的任務就是將執行檔從儲存裝置中載入到 Memory 中，配置好執行的環境。 如果想觀察 Kernel 的 code, data, bss section 可以使用 cat /proc/kallsyms，但是需要 root 權限 7.4 Memory Segmentation 記憶體的區段機制(Memory Segmentation)是很直覺以程式的區段作為管理的單度，每個 seg. 都對應到程式的一個邏輯上的 seg.(text, data, bss, heap, stack)， 通常 seg. 是不可以被拆分，要拆分的話需要硬體支援，有固定的拆分大小。 例如: x86 的 ds. fs. gs. 三個 data 的 segmentation，其中 fs. gs. 是額外的 data seg. 同一個程式的 seg. 可以在 Physical memory 中不連續，這樣可以更有效率的使用記憶體，但是這樣就有可能產生 Fragmentation Problem(碎片化問題)。 Management of Segmentation 既然是基於 Segmentation 的管理機制，那麼他所使用的定址方式就會是基於 Base + Offset 的方式(相對定址)。 例如這樣的指令: regtarget = regbase + offset 這個 regbase 是 OS 能夠管理的 例如設計三個 Register 可以分別指向 code, data, stack 的開始位置，這樣就能分開存放這三個 seg. 在這種機制下 Context switch 的時候除了切換 general register 以外，還需要切換 base reg. file 這種 base reg. file 的設計同時還要引入 limit(長度)用來限制存取的範圍，如果存取錯誤的記憶體就會觸發 Segmentation Fault。 以 ARM Cortex-M 為例，MPU_RBAR 是用來設定 base address，MPU_RASR 是用來設定 limit。 MPU: Memory Protection Unit Memory Segmentation 的機制並不太適合在動態的環境下，要新增程式或減少程式，容易造成碎片化問題，但在靜態的環境下，例如: 嵌入式系統，就很適合使用這種機制 7.5 Memory Paging Memory Paging 是目前主流的記憶體管理機制，相較於 Memory Segmentation，他的管理單位是 Page，而不是 Seg. Page 是固定大小的記憶體區塊，通常是 4KB OS 指定每個 Page 的用途，例如: code, data, stack, heap 不一定是連續分配的 Page Page Table 為了確保程式在執行時不會忽鄉干擾，OS 要能很輕鬆的切換正在執行的程式，所以硬體上會支援 Mapping Table 的機制 假如系統最多有 48 個 Page，那麼這個 Table 就起碼要有 48 個 entry 要能針對每個 Page 設定不同的權限，例如: stack(rw-), code(r–), data(rw-) 即使 stack, data 是相同權限，但是 OS 會給他們不同的功能，例如: stack 可以隨程式增加大小 下圖是一個程式從 CPU 使用 Virtual address 到 Physical address 的過程，其中 OS 會透過 Mapping Table 來做轉換， 實際上這樣的轉換是在 CPU 中完成的，是已經設計好的硬體線路 可以看到表格中會有 Vir. address, Phy. address 的對應，與屬性權限等等，如果 Context switch 的時候，OS 只需要切換這個 Page Table 就可以了。 同時 OS 會知道這個 Page 是什麼類型，例如: stack 假如碰觸到邊界，OS 就會去增加 stack 的大小。 在虛擬記憶體上，OS 會把這些資訊記錄在 Task Control Block(TCB) 中，使用 RBtree 來管理 Linux Kernel 中使用 mm_struct 來描述 Task 的 Virtual Memory Layout 相較於 Segmentation，Paging 的優點是: 在記憶體分配上 Paging 比較有彈性，只要有足夠的 Page 就可以 在 Hardware 上並沒有 Segmentation 的概念，是由 OS 來實現的 每一個 Page 的大小是固定的，分配演算法會比較好實現 缺點: Page 的數量遠超過 Segmentation，所以 Page Table 會很大，可能會造成硬體 Mapping 上的效能負擔 如果使用 Assembly 寫程式，Paging 的程式碼會比較難寫，難懂 7.6 Fragmentation Problem 只要會造成 Memory 動態分配的情況，就必然會有 Fragmentation Problem，這是一個無法避免的問題 External Fragmentation: 如果一塊記憶體中的 Free space 足夠，但無法滿足程式的需求，這些 Free space 就是 External Fragmentation Internal Fragmentation: 因為配置演算法，本來只需要 X size 的記憶體，但是分配了 X + Y size 的記憶體，其中 Y size 就是 Internal Fragmentation 例如: OS 分配空間時覺得剩下的空間太小了，乾脆把整個空間都分配給程式，那就是 Internal Fragmentation 這裡有一些解決 External Fragmentation 的方法，例如: Memory Compaction(記憶體壓縮): 搬移記憶體使 Free space 變成連續的，但是成本很高 更好的分配演算法: Best Fit, Worst Fit, First Fit Fragmentation in Paging/Segmentation 在 Pagging 的情況下，由於每個 Page 的大小一致，並且透過 Mapping 的方式使用 Page，所以不會有 External Fragmentation 但因為程式所需要的記憶體大小不會剛好是 Page 的大小，所以會有 Internal Fragmentation Memory Paging Hardware 7.7 MMU and MPU 7.8 TLB 7.9 TLB Miss 7.10 Translation table structure 7.11 Hardware Handling of TLB Miss 7.12 Software Handling of TLB Miss 這裡會介紹 Mapping Table 的基本形式，即是 TLB(Translation Lookaside Buffer)，如果 TLB 不夠大要怎麼處理 7.7 MMU and MPU MPU(Memory protection unit): 可以設定某段記憶體的權限，例如: code(r–), data(rw-) MMU(Memory management unit): 除了 MPU 的功能以外，還可以透過 MMU 使 OS 更有效的分配記憶體的使用 MMU management unit 以 Segment 為最小管理單位 較早期的電腦會以 Segment 為最小管理單位 在 Protect 上會比較直觀 因為 Segment 的特性，所以對於記憶體管理的幫助不大 以 Page 為最小管理單位 在 Protect 上會比較複雜，因為每個程式都會有很多 Page 但在管理上因為 Page 固定大小，因此可以更有效率的管理記憶體 因為 Page 很多，所以需要很多的 Mapping entries，例如: 4GB 的記憶體就要 1M 個 entries(4GB / 4KB)，這會成為硬體的負擔 MMU Architecture 實際上 MMU 會建立在 CPU 內部，下面是一個簡單的架構表示 CPU 中 MMU, Address decoder, DRAM 之間的關係: L1 Cache 與 MMU 誰在前誰在後，取決於 CPU 的設計 要注意的是 L1 Cache 如果在 MMU 前面 Context switch 的時候，就要把 L1 Cache 清空 MMU Function 這裡介紹下 MMU 應該會是什麼樣子，這下面舉的是抽象簡化的例子 假如有以下編碼 regtarget = regbase + offset Segment 機制下: regbase 可能是 basecode 這樣的管理方式，代表是 code seg. + offset Paging 機制下: regbase 代表的是 Page Number，所以是 Page Number + offset 例如: 一個 32 bit 的系統，Page number 會是 20 bit，offset 會是 12 bit，因此 offset 最多只能是 4KB，因此不需要 limit 的設計 7.8 TLB 其實 TLB(Translation Lookaside Buffer) 就是一個在 CPU 中的高速 Buffer(SRAM)，把 MMU 的 Page Table 存放在 TLB 中，利用這種方式可以加速地址轉換的速度。 p(page number): 程式在 Logical address 中的 Page number f(frame number): 實際在 Physical address 中的 Page number d(displacement, offset): 在 Page 中的 offset 這裡來看一個實際的例子 Intel Sandy Bridge 的 TLB 規格: Level 1 TLB i-TLB: 72 entries, i means instruction d-TLB: 100 entries, d means data Level 2 TLB (如果在 L1 TLB 中沒有找到，就會到 L2 TLB 中查找) 1024 entries TLB Size 那麼一個 Page 的大小是 4KB，4KB * 1024 = 4MB，但是一個程式不可能只有 4MB 大小，這裡再看另一個例子，MIPS R8000-style TLB，一共有 384 entries， 這樣也才 384 * 4KB = 1536KB，都是遠遠小於實際程式需求的。 解決方法: Hardware: 增加 TLB 的數量，但是這樣會增加硬體成本(CPU 變大，成本變高，搜尋時間變長) Dynamic: 增加動態載入 TLB 的方式 要動態載入 TLB 就要思考這是怎樣的資料結構，同時要硬體支援 7.9 TLB Miss TLB 有大小的限制，因此會發生 TLB Miss，當 Miss 發生時就會需要動態載入 TLB 增加 TLB 的大小: TLB 基本上是一個平行搜尋的硬體 Page number 會同時與 TLB 中的數百到數千個 entries 做比對 擴充 TLB 會增加硬體成本，例如: 電晶體數量，時脈降低，耗電量增加 既然這樣的話增加 Page size 也是一種方法，但是就會降低軟體在管理上的彈性，例如: huge page Dynamic Loading TLB entries 當 MMU 發現某一個 Page number 在 TLB 找不到對應的 entries 時，觸發 TLB Miss 將當下要使用的 TLB 載入， Main Memory 足夠放入所有的 TLB entries，但是這部分應該要由 Hardware 來實現還是 Software 來實現? 7.10 Translation table structure 這裡介紹如何把 TLB 的 entries 放到 Main Memory 中，使用什麼樣的 Data structure 來管理 假如我們要把 TLB 的 entries 放到 Main Memory 中，那假設以下情況: 500MB 的程式，需要 500KB 的記憶體空間放 Page Table 700MB 的程式，需要 700KB 的記憶體空間放 Page Table 1500MB 的程式，需要 1500KB 的記憶體空間放 Page Table 以上這些程式一樣會動態載入記憶體，這樣的話久而久之也會造成 External Fragmentation，這樣就反其道而行了，Paging 本來是用來解決 Fragmentation 問題的， 但是為了這些大小不一的 Page Table，反而造成了 Fragmentation 問題。 Hierarchical Paging 這裡就使用一種 Hierarchical Paging(階層式分頁)的方式來解決這個問題，一個 4KB 大小的 Page 可以管理 1024 entries，第二層也是以 4KB 還分層管理， 這樣兩層就能放入 1024 * 1024 = 1M 個 entries，這樣就能解決上面的問題。 這樣的話定址的格式就會改成以下的格式: LV1 PTE(Level 1 Page Table Entry): 第一層的 Page Table Index LV2 PTE(Level 2 Page Table Entry): 第二層的 Page Table Index d(displacement, offset): 在 Page 中的 offset 注意到這樣的指令格式剛好是 210 x 210 x 212 = 232，也就是 32-bit 的 CPU 可以使用這樣的指令格式。 在這裡 LV1, LV2 在沒有映射出去的時候，會設定為 Invalid bit 延伸閱讀: Andrew S. Tanenbaum, Modern Operating Systems, Chapter 3, 3.3.2 Page Tables 有更多種類的 Page Table 結構可以參, Marvin Solomon CS 537 7.11 Hardware Handling of TLB Miss 目前大部分的 CPU 都是用硬體來處理 TLB Miss，硬體會透過之前提過的 Hierarchical Paging 的指令方式來查找 Page Table， 在 Logical address 中會以逗號隔開的方式記錄: 0xpte1, 0xpte2, 0xoffset 下圖是一個簡單的架構表示如何處理 TLB Miss: 首先 CPU 會先去 TLB Search，TLB 中沒有的話就會觸發 TLB Miss PTBR 會記錄 LV1 Page Table 的 Physical address，以供 CPU 找到 LV1 Page Table PTBR(Page Table Base Register): 在 Intel x86 中是 CR3 找到第二層的 LV2 Page Table，然後在 LV2 Page Table 中找到對應的 Frame number 將一個 TLB Entry 置換成剛剛找到的 Frame number 重新啟動 TLB Search，這次就會找到對應的 TLB Entry 7.12 Software Handling of TLB Miss MIPS 處理器使用了 software-managed TLB，這種方式會比較慢，當 TLB miss 時將會觸發 exception，然後 OS Kernel 會去進行後續處理。 這裡列出三類型的 TLB exception: TLB Refill: TLB 中沒有相對應的 entry 時就會發生 TLB Invalid: Virtual address 使用被設定為 Invalid 的 entry 時就會發生 TLB Modified: TLB 有對應的 entry，但是該 entry 的權限不符合(No dirty bit)時就會發生 MIPS 會告知 OS TLB Miss 的原因與位置，並根據 CPU 正在執行哪一個 task 來使用適當的演算法找出對應的 entry 要注意軟體處理 TLB Miss 有兩種可能: Real Error: 也就是程式碼出錯，例如: 存取了不該存取的記憶體 TLB Miss: 也就是程式碼沒有出錯，只是 TLB 中沒有對應的 entry OS Kernel 在這裡可以像硬體一樣去使用 Paged Page Table(分頁分頁表)，或使用 Hash Table 來管理 TLB entries。 並且在軟體上會清楚正在執行哪些 Task，所以可以提前把共用的 TLB entries 載入到 TLB 中，這樣就可以減少 TLB Miss 的發生。 Advantages: 透過軟體的話，可以更有效的搜尋，甚至修改搜尋演算法 可以提前載入共用的 TLB entries，減少 TLB Miss 的發生 Disadvantages: 要去執行 Exception 會牽涉到 Mode change，會有額外的成本 部分 MIPS 也有硬體機制來走訪 Page Table，部分 SPARC、POWER 處理器允許軟體來處理 TLB Miss 填寫 TLB 的功能必須由 Kernel 來做，由 User space 來做的話會有安全性的問題 Fragmentation 7.13 OS memory allocation architecture 7.14 Kernel Management of Frames 7.15 Slab Allocation 7.16 Malloc Linux Kernel 幾乎不會修改與 Kernel space 相關的 Page Table，因此 Kernel 會盡可能地去使用大的 Page，例如: 1GB， 來降低對於 TLB entries 的使用，因為在 User space 中 Kernel 的部分是共用的，修改 Kernel 的 Page Table 可能會造成額外的 TLB Miss。 User space 的部分就會使用 4KB 的 Page，這裡要討論的是 malloc 怎麼分配記憶體給 Task 使用，malloc 不會每次都剛好等於 4KB 的倍數， 硬體上可以解決的是 4KB Page 為單位的 Fragmentation 問題，但是程式自由配置的大小問題還是需要由軟體來解決。 7.13 OS memory allocation architecture 下圖展示一個作業系統的記憶體分配架構，怎麼初始化到分配記憶體給 Task 使用 在這個架構之下這裡要討論的是: Kernel 如何管理分配 Frame Kernel lib 會提供 kmalloc() 來提供給 Kernel 的 Task 使用 libc 如何實現 malloc() 來提供給 User space 的 Task 使用 這裡雖然記憶體最小的管理單位是 4KB，但是 libc 透過 syscall 來向 Kernel 一次要求多個 page 例如: sbrk(), brk(), mmap() 而 malloc() 會在這些 page 中分配記憶體給 Task 使用，以此做到更細微的記憶體分配 7.14 Kernel Management of Frames Kernel 透過 MMU 的機制，可以把記憶體都視為 4KB 大小的 Page(Frame) 來管理 Kernel 將所有的 DRAM 都 Mapping 到 Kernel space 這代表 Kernel 可以直接存取所有的 DRAM，即使已經分配給 User Task 使用 因為記憶體幾乎是完全交由 Kernel 來管理，所以 Kernel 內部必須小心地處理記憶體 因此開機系統啟動時會去探測記憶體大小和狀況，這裡會使用 bootmem bitmap 只會在最初使用，因為在怎麼優化效率都不高 延伸閱讀: Linux 核心設計: 記憶體管理 Buddy system buddy system 的目的是更快的搜尋到可以分配的記憶體，並且盡量保持記憶體的連續性 假設在分配時，kernel 已經知道有一塊 2M 的連續記憶體可以分配，並且 task 也可以用掉這 2M 的記憶體就可以直接分配這 2M 給 task 使用， 同時保持記憶體的連續性，可以使 MMU 做更有效率的優化。 在上面這個例子中，我們假設 kernel 已經知道有 32 個 frame 可以分配，藍色的部分代表已經被使用的 frame: 合併的條件是來自相同的 parent 橘色與黃色的 frame 來自同一個 parent A，所以可以合併 黃色的 parent 是 A，棕色的 parent 是 B，所以不能合併 只有橘色與黃色合併後，parent 變為 B，才能與棕色合併 如果某 Kernel task 需要 2 個 page，那麼就優先把 2 個 page 的 leaf 分配給他 同理如果需要 4 個 page，就優先把 4 個 page 的 leaf 分配給他 在 Linux kernel 中實現 buddy system 的方式是透過 bitmap 來實現樹，而不是真的使用一個樹的資料結構 7.15 Slab Allocation Linux kernel 中有許多頻繁使用的資料結構，例如: task_struct, file, inode 等等，如果直接使用 buddy system 來分配記憶體， 會很容易造成 Fragmentation，所以使用 slab 來解決這個問題 雖然 buddy system 可以每次以 page(4KB) 為單位分配記憶體了，但是系統執行時絕大部分的資料結構都是小的，因此 Linux 使用 slab 來解決小物件的分配。 slab 會去向 buddy system 要數個 page，在這裡會被稱為 cache(kmem_cache, 並不是指 CPU cache) slab 描述 kmem_cache 的定義在 mm/slab.h cat /proc/slabinfo 可以看到目前系統中有哪些 cache slab allocator 是由很多個不同大小的 cache 組成，這些 cache 通常是為了分配特定 object 的大小 例如: 在 Linux kernel 中 task_struct 大約只需要 1.7KB 就可以使用一個 cache 來服務這個 object cache 中使用三種 list 來管理: full, partial, free 因此 slab 會達到以下的目的: 可以比 buddy system 分配更小的記憶體 cache 常用的 object 可以被重複使用，不需要重新分配記憶體(allocating, initialising, destroying) 把這些常用 object 與 L1 或 L2 cache 對齊，可以更好的利用 hardware cache 這裡可以想像 slab 就是把常用的 object 當成牛奶瓶，cache 就是牛奶箱，牛奶瓶可以重複去牛奶箱中拿取放回。 延伸閱讀: Chapter 8 Slab Allocator, 内存管理 slab 分配器 7.16 Malloc 通常程式都會有 heap，而 malloc 幾乎都是從 heap 中分配閒置的記憶體給程式使用。 heap 是連續配置的記憶體空間，大小為 page 的倍數 malloc 這裡會使用一些演算法來從 heap 分配記憶體給程式使用 例如: ptmalloc 延伸閱讀: Glibc 内存管理, Overview of Malloc malloc 實現這邊打算之後另外寫一篇文章，這邊只要知道 task 使用 malloc 也不是直接向 kernel 索要 page 就可以了 Last Edit 1-26-2024 19:15"
  },"/jekyll/2023-12-09-class_level_testing.html": {
    "title": "Testing | Class-Level Unit Testing",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-12-09-class_level_testing.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 這章節主要開始介紹從 Class 層級的 Unit testing。 Class level 開始不再把 Method 看作單一的個體，而是以狀態的角度來查看一個 Class 在不同狀態下是否能符合規格。 6.1 UML State Machine Diagram 在這個章節就是介紹什麼是 State Machine Diagram，以及相關的名詞與概念 這裡可以參照 UML 中的 State diagram, UML 2 Tutorial - State Machine Diagram Finite State Machines(FSM, 有限狀態機)用來描述 Object 隨時間變化的動態行為模型 FSM 視為對 Object 的一種局部視圖 每個 Object 都作為一個獨立的實體，透過 Event 與 Action 來做出反應與外部通訊 6.1.1 Events 事件代表 Object 可以偵測到的改變，例如: 一個 Object 對另一個 Object 的呼叫或 Signal 的接收 特定的 Variable 的變化或時間的流逝 任何可以影響一個 Object 的事物都可以被視為 Event Event 發生在某一時刻，他並不具有持續性 Event Type Call event: 一個 Object 接受明確的同步呼叫請求，op(a : T) Change event: Boolean expression 的值發生變化，when(exp) Single event: Object 收到一個明確，並被命名的異步通訊(Async)，sname(a : T) Time event: 絕對時間或相對時間的流逝，after(time) 以之前的例子來說，receive order, rejected, approved 都是 Event 6.1.2 State Finit state machine 定義了數個 State，一個 State 可以用三種互補的方式來描述: 一組性質相似的 Object values Object 等待某個事件或多個事件發生的時間段 Object 執行某個持續進行的活動得時間段 6.1.3 Transitions 離開一個 State 的轉換定義了 Object 在該 State 中對於 Event 發生的回應 通常情況下一個轉換 e(a : T)[guard]/activity 包含: Event trigger(事件觸發器): e(a : T) Guard condition(守衛條件): [guard] Effect(效果): activity Target state(目標狀態) Event Triggers Event triggers(事件觸發器)指定了能夠觸發轉換的事件 如果該 Event 包含參數，這些參數在轉換的過程中可能會產生某些效果 Guard condition 一個轉換可能有 Guard condition(守衛條件)，這是一個 Boolean expression 他可能引用被轉換的 Object 的 attributes, parameters 當觸發事件發生時就評估 Guard condition 如果為 true 則轉換發生; false 則不發生 同個事件可以觸發單一 State 中的多個轉換 但是使用相同事件的轉換之間必須有不同的 Guard condition 而一個 Guard condition set 將涵蓋所有可能性，確保 event 會觸發某個轉換 一個事件中只有一個轉換可能被觸發 Effects 當一個轉換觸發時，他的 Effects(效果)將被執行 效果可以是一個 Action 或 Activity Action: 可以是一個基本的計算，例如: assignment statement simple arithmetic computation sending a signal calling an operation createing/destorying an object getter/setter Activity: 一連串的 Actions Change of State 當一個 Effect 執行完畢，則 State 就將改變到 Target state 6.1.4 Activites in State 一個 State 中也可以包含一系列的 Activity: Entry activity: that is executed when a state is entered –entry/activity. Exit activity: that is executed when a state is exited –exit/activity. Internal activity: that is executed after the entry activity and before the exit activity –e(a:T)[guard]/activity. 6.4 State Type FSM 中的 State 可以分為以下幾類 Initial state/Final state: 一個 Psudo state(偽狀態)，用來表示 State machine 的開始與結束 Terminate: 表示 State machine 的銷毀或結束，包含釋放部分資源，清除狀態等 Simple State: 沒有子結構的 State Nonorthogonal State: 包含一個或多個子狀態的複合狀態，狀態活動時代表其中至少有一個子狀態處於活動狀態 Orthogonalstate: 包含兩個或更多子狀態的複合狀態，狀態活動時代表其中所有子狀態都處於活動狀態 上圖左邊是 Nonorthogonal state，右邊是 Orthogonal state 6.2 Test Coverage Criteria 關於測試覆蓋標準的詳細說明可以參考 Test Coverage Criteria 這裡主要以 All-definition-use coverage 來說明 Control flow All-state coverage: 所有的 State 都被執行到的路徑 All-transition coverage: 所有的 Transition(邊界)都被執行到的路徑 Data flow All-definition coverage: 所有的 Definition 都被執行到的路徑 All-use coverage: 所有的 Use 都被執行到的路徑 All-definition-use coverage: 所有的 Definition 與 Use 都被執行到的路徑 Both All-path coverage: 所有的路徑都被執行到，但如果有無限迴圈的話就無法完成 6.2.1 Testing Paths Definition-Use Pairs The value of a definition of a variable may be used by several different uses of the variable. A use of a variable may use the value defined by several different definitions of the variable. Each definition and each of its uses compose a definition-use pair. The set of definition-use pairs includes all the data flow relations. All-definition-use coverage 的測試流程如下: 找出所有的 Associations: 每個 Definition 與 Use 之間都會有一個 Association 使用圖形演算法找出路徑，這裡使用 BFS 找到一條路徑後就標記該條路徑覆蓋到的 Association 直到所有的 Association 都被標記為止 不是所有 Association 都是可行的，這裡可以使用手動判斷或是使用演算法來判斷 要注意這裡所說的都是 Nonorthogonal state 6.2.1 Orthogonal States 因為 Orthogonal State 中的 concurrency(並行)特性會使得測試變得複雜，所以需要將 Orthogonal State 轉換成 Nonorthogonal State 來進行測試。 如果一條路徑包含了 Orthogonal State 可以將 State 中的每個完整子路徑都視為一條路徑來擴展成一組完整路徑集合 因為 Orthogonal State 中的 Concurrency 特性，我們需要將 Orthogonal State 轉換成 Nonorthogonal State Orthogonal States to Nonorthogonal States 假設有一個 Orthogonal State 有 n 個區域，並且 ni 中有 mi 個狀態 每個新狀態都是一個 n-tuple( x1, …, xi, xn ), xi 是第 i 個區域中的舊狀態 如果在第 i 個區域中存在從 xi1 到 xi2 的轉換，那就存在一個 ( x1, …, xi1, xn ) 到 ( x1, …, xi2, xn ) 的轉換 上圖將上方的 Orthogonal State 中的所有狀態可能同時發生的情況都列出來，然後組成下方的 Nonorthogonal State 6.3 Constriant Logic Graph 如果直接從 State Diagram 來進行走訪與測試的話會需要很多而外的處理，所以更簡單的方式是將 State Diagram 轉換成 Constriant Logic Graph 來進行走訪。 假如有一個 stack 的 State machine 如圖左，將其轉換為右圖的 CLG 後會更好的找出 Associations 與路徑 這樣做的好處是可以專注在 Graph traversal algorithm，將所路徑上的 Expression 收集後再處理成 CLP 就可以找出 test case， 不用再像 State Diagram 需要處理複雜的 Node 結構。 Last Edit 12-12-2023 17:37"
  },"/jekylls/2023-11-29-semantic_analysis.html": {
    "title": "Compiler | Semantic Analysis Notes",
    "keywords": "Compiler Jekylls",
    "url": "/jekylls/2023-11-29-semantic_analysis.html",
    "body": "Compilers course notes from CCU, lecturer Nai-Wei Lin. Semantic Analysis 這個階段主要用來檢查程式碼的語意是否正確，如果語意有錯誤則會產生錯誤訊息。 Outline Semantic Analyzer Attribute Grammars Syntax Directed Translation Type Checking Syntax Tree Construction Bottom Up Translators Bison A Bottom Up Translator Generator Semantic Analyzer 狹義上的 Semantic Analyzer(語意分析)只負責 Type checking， 但廣義上的語意分析則可以同時在 Type checking 時進行程式碼的 Interpretation 或 Translation。 Type checking of each construct Interpreation of each construct Translation of each construct 5.1 Attribute Grammars Attribute Grammar(屬性語法) 也可以稱為 Syntax Directed Definition(SDD, 語法導向定義): Attribute Grammars 是帶有 Associated semantic attributes(關聯語意屬性)與 Semantic rules(語意規則)的 Context-free grammar 每個 Grammar symbol 都關聯一組 Semantic attributes 每個 Production 與關聯一組計算屬性的 Semantic rules Example 5.1 An Attribute Grammar L -&gt; E '\\n' {print(E.val)} E -&gt; E '+' T {E.val = E.val + T.val} E -&gt; T {E.val = T.val} T -&gt; T '*' F {T.val = T.val * F.val} T -&gt; F {T.val = F.val} F -&gt; '(' E ')' {F.val = E.val} F -&gt; digit {F.val = digit.val} # val represents the value of an expression 例如 E -&gt; E '+' T，他的 Semantic rules 為 E.val = E.val + T.val，其中 E.val 與 T.val 為 Semantic attributes。 把 3 * 5 + 4 當作範例化成 Syntax Tree 將會如上圖所示 Synthesized attributes(合成屬性): 如果一個 Node(Grammar symbol) 在解析樹中的屬性值是由其子節點的屬性值計算出來的，則稱該屬性為合成屬性 Inherited attributes(繼承屬性): 如果一個 Node(Grammar symbol) 在解析樹中的屬性值是由其父節點的屬性值計算出來的，則稱該屬性為繼承屬性 5.1 的例子中可以看出全部都是 Synthesized attributes，下面是 Inherited attributes 的例子 D -&gt; T { L.in := T.type } L L -&gt; int { L.type := integer } L -&gt; float { L.type := float } L -&gt; { L1.in := L.in} L1 ',' id { addtype(id.entry, L1.in) } L -&gt; { addtype(id.entry, L.in) } S-Attributed Attribute Grammar 如果一個 Attribute Grammar 的每個 Attribute 都是 Synthesized attributes，則稱為 S-Attributed Attribute Grammar L-Attributed Attribute Grammar An attribute grammar is L attributed if each attribute in each semantic rule for each production A -&gt; X1 X2 … Xn is a synthesized attribute, or an inherited attribute of Xj , 1 &lt;= j &lt;= n, depending only on the attributes of X1 , X2 , …, Xj-1 the inherited attributes of A 簡單來說就是如果一個 attribute 所使用的「資訊來自上層或左邊」，則稱為 L-Attributed Attribute Grammar A Counter Example 這裡簡單舉一個錯誤的例子: A -&gt; { L.in := l(A.in) } L { M.in := m(L.s) } M { A.s := f(M.s) } A -&gt; { Q.in := q(R.s) } Q { R.in := r(A.in) } R { A.s := f(Q.s) } 在這裡使用到了 R.s 因此導致錯誤，要注意 R.s 應該是來自於 R，但是在這裡 R 還沒有被計算出來。 Attribute Parse Tree 這裡表示一種名為 Attribute Parse Tree(屬性解析樹)的資料結構，虛線的位置表示語意規則，實線的位置解析的語法規則。 這樣整棵樹就能以一種 Preorder 的方式來計算出所有的 Semantic attributes。 Example E -&gt; T { R.i := T.s } R { E.s := R.s } T -&gt; num { T.s := num.val } R -&gt; addop T { R1.i := R.i addop.lexeme T.s } R1 { R.s := R1.s } R -&gt; ε { R.s := R.i } 上面是一個直譯的例子，最後在 Root 能求出 E.s 的值 Type Checking 在這裡所談的是 Static Type Checking(靜態型別檢查)，也就是在編譯時期就能檢查出來的型別錯誤。 這邊可以把 Type 的應用分為兩種: Type Checking(型別檢查) 型別檢查確保運算子(Operator)與運算元(Operand)的型別是相容的，例如: Java 規定 &amp;&amp; 的兩邊必須是 boolean Translation application 根據型別的名稱，Compiler 可以決定需要多大的記憶體空間，這些資訊還會被很多後續的階段使用，例如: Array 中的起始位置，進行運算時的型別轉換等等… 5.2 Type Systems Type system(型別系統)是一種用來描述程式語言中的型別的規則，而 Type checker(型別檢查器)實現了這些規則 Type expression(型別表示式)是一種用來描述型別的方式，例如: int x，宣告了一個名為 x 的變數，型別為 int Type Expression 基本型別 boolean, int, float, char, real, void, type_error 類型構造函數也是一種 Type expression array: array(S, T), means an array of T type with size S product: T1 x T2 x … x Tn, means a product of T1, T2, …, Tn pointer: pointer(T), means a pointer to T type function: int function(int, float), means a function that takes an int and a float and returns an int. Type Checker 通過宣告的時候把型別資訊與對應的 Identifier 放入符號表(Symbol Table) 這樣在表達式中就能透過符號表來檢查 Identifier 的型別是否正確 Type Checking of Expressions 這裡展示一個可能的型別檢查的規則: E -&gt; literal { E.type := char } E -&gt; num { E.type := int } E -&gt; id { E.type := lookup(id.name).type } E -&gt; E1 mod E2 { if E1.type == int and E2.type == int then E.type := int else E.type := type_error } 在 mod 的語意規則中，加入了型別檢查的規則，如果 E1 與 E2 的型別都是 int，則 E 的型別也是 int，否則 E 的型別為 type_error， 這樣在後續的處理就能夠判斷出來是否有錯誤。 Postfix Translation Scheme Postfix Translation Scheme(後序翻譯方案)是一種用來描述語法規則的方式，假如一個文法是 S-Attributed Attribute Grammar， 那在 LR 中就能夠使用 Postfix Translation Scheme 來描述語法規則。 5.3 Bottom-Up Translators 因為在 Bottom-Up 的過程中我們會有一個 Stack 紀錄已經解析過的 Token，因此我們也可以透過這個 Stack 來傳遞 Attribute。 假如有一個語法規則 A -&gt; XYZ, { A.a := f(X.x, Y.y, Z.z) } 如果要對這個語法規則做 Reduce，那麼 Stack 中必然已經存在依序的 XYZ 此時就可以透過偏移量來取得 XYZ 的 Attribute，例如: val[Top-2] is X.x 如果要做 Reduce，最後 A 會存在於消除 XYZ 後的 Stack 最上面，因此會與 X 的位置相同，也就是 val[Top-2] Example 這裡給出一個例子，實際上在 Bison 中使用時會有一些差異，但是大致上是相同的。 下圖是以這個例子來建立的例子 實際上這樣的方法並不好用，因為在 Bison 中如果想要手動處理 Stack 的偏移，尤其是 -1 這種偏移量，會非常的麻煩 延伸閱讀: Bison 3.4.6 Actions, 這裡會提到如何使用 $0, $-1 等方式來取得 Stack 中的值 參考範例: negative_stack.y 這裡是可以參考的範例 Syntax Tree Construction 5.3 Syntax Tree Construction 在這裡我們能發現在 Syntax Analysis 的時候，Bottom-up 的 Parser 是比較強大的方法，因為能透過 lookahead 等方法提前知道後續的 Token， 但是在 Semantic Analysis 的時候，Top-down 的 Parser 是比較強大的方法，因為在傳遞 Attribute 的時候，Top-down 的 Parser 能夠很容易的傳遞 Attribute。 而這裡會使用一種 Abstract Syntax Tree(抽象語法樹)來表示語法結構，這是一種 IR(Intermediate Representation, 中間表示法)， 透過這個資料結構我們就能排除掉 Bottom-up 或 Top-down 的 Parser 的難點。 Syntax Tree for Expression Interior nodes(內部節點)表示運算子，例如: +, -, *, /, mod Leaf nodes(葉節點)表示運算元，例如: literal, identifier, num 假設建構節點的函數如下: mknode(op, left, right) mkleaf(id, entry) mkleaf(num, value) 上圖展示了一個 a - 4 + b 的 AST Last Edit 11-29-2023 18:10"
  },"/jekyll/2023-11-28-test_coverage_criteria.html": {
    "title": "Note | Test Coverage Criteria",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-11-28-test_coverage_criteria.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 本關介紹關於 Structural Testing 的 Test Coverage Criteria，分為兩個部分 Control Flow 與 Data Flow 1.1 Control Flow 在 Control Flow 中，Coverage Criteria 通常注意的是 Edge, Node, Condition，透過這些是否被執行到來判斷是否有達到覆蓋的標準 Statement coverage (SC) Decision coverage (DC) Condition coverage (CC) Decision/condition coverage (D/CC) Multiple condition coverage (MCC) Path coverage (PC) 上面的這幾個例子都會用以下的圖來講解: Statement coverage (SC) Every statementin the program has been executed at least once. 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 2 -&gt; 6 Statement Coverage 又可以被稱為 Node Coverage，基本上就是要求每個 Statement 都要被執行到， 但是 Statement Coverage 並不會要求每個 Decision 都要被執行到，所以有可能會有一些 Decision 沒有被執行到。 Decision Coverage (DC) Every statementin the program has been executed at least once, and every decisionin the program has taken all possible outcomes at least once. 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 2 -&gt; 6 1 -&gt; 2 -&gt; 3 -&gt; 5 -&gt; 2 -&gt; 6 Decision Coverage 又可以被稱為 Edge Coverage，基本上就是要求每個 Edge 都要被執行到，所以 Decision Node 就會有分支產生， 就算 Decision Node 中有多個 Condition，但也只要求 Decision 的 True/False 都要被執行到。 Condition Coverage (CC) Every statementin the program has been executed at least once, and every conditionin each decision has taken all possible outcomes at least once. 程式中的每個 Condition 都要是少執行到一次 True/False，但不一定要求每個 Decision 都要被執行到。 Decision/condition coverage (D/CC) Every statementin the program has been executed at least once, every decisionin the program has taken all possible outcomes at least once, and every conditionin each decision has taken all possible outcomes at least once. 程式中的每個 Condition 與 Decision 都要是少執行到一次 True/False。 Multiple Condition Coverage (MCC) Every statement in the program has been executed at least once, all possible combination of condition outcomes in each decision has been invoked at least once. There are 2n combinations of condition outcomes in a decision with n conditions. 程式中的每個 Condition 的可能的組合都要被執行到，所以有 n 個 Condition 就會有 2n 種可能的組合。 所以以上面的突來說，如果 Condiction 是 CA, CB, CC: DC: 一個 1 與 2 的組合都能滿足標準 CC: 下標相同數字的組合都能滿足標準，因為 Condiction 都有被執行，例如: 11, 21 DCC: Combination 2 不可能滿足 DCC，其他下標相同的組合都能滿足 DCC，例如: 11, 21 因為 2, 7 的組合 Decision 都是 True，沒有滿足 Decision 的需求 MCC: Combination 1 ~ 8 都要被執行，3 個 Condiction 有 23 種可能的組合 Path Coverage (PC) Every complete pathin the program has been executed at least once. A loop usually has an infinitenumber of complete paths. 通常不會用到 Path Coverage，並且在 PC 的標準下如果有迴圈將會產生無限條路徑 Test Coverage Criteria Hierarchy 而覆蓋的範圍從大到小依序為: PC &gt; MCC &gt; D/CC &gt; CC = DC &gt; SC 使用 D/CC 就能有不錯的覆蓋率，並且不會有太多的測試成本，所以 D/CC 是一個不錯的選擇 1.2 Data Flow Data Flow 中則有 Definition, Use, DU-Path 的概念，以這些是否被執行來判斷是否有達到覆蓋的標準 All-defs coverage All-c-uses coverage All-c-uses/some-p-uses coverage All-p-uses coverage All-p-uses/some-c-uses coverage All-uses coverage All-du-paths coverage 下圖是目前拿來做例子的 Control Flow Graph: 上面兩條路徑是剛好覆蓋了全部 Node 的路徑，但是還要檢查是否有覆蓋到全部的 Associations All-Defs Coverage Test cases include a definition-clear path from every definition to some corresponding use (c-use or p-use). All-Defs 是相對寬鬆的標準，只要所有的 Definition 都有被執行到就能滿足標準 All-C-uses Coverage / All-P-Uses Coverage All-C-uses: Test cases include a definition-clear path from every definition to all of its corresponding c-uses. All-P-uses: Test cases include a definition-clear path from every definition to all of its corresponding p-uses. 都是要求全部的 C-use/P-use 被執行到 All-C-Uses/Some-P-Uses Coverage / All-P-Uses/Some-C-Uses Coverage All-C-Uses/Some-P-Uses: Test cases include a definition-clear path from every definition to all of its corresponding c-uses. In addition, if a definition has no c-use, then test cases include a definition-clear path to some p-use. All-P-Uses/Some-C-Uses: Test cases include a definition-clear path from every definition to all of its corresponding p-uses. In addition, if a definition has no p-use, then test cases include a definition-clear path to some c-use. 都是要求全部的 C-use/P-use 被執行到，但是如果該變數沒有 C-use/P-use，則要求被執行另外一種 Use 至少一次 All-Uses Coverage / All-DU-Paths Coverage All-Uses: Test cases include a definition-clear path from every definition to each of its uses including both c-uses and p-uses. All-DU-Paths: Test cases include all du-paths for each definition. Therefore, if there are multiple pathsbetween a given definition and a use, they must all be included. All-Uses 與 All-DU-Paths 都是要求全部的 C-use/P-use 都被執行到，但是 All-DU-Paths 還要求全部的 DU-Paths 都被執行到， 通常只需要 All-Uses 就能滿足需求，All-DU-Paths 相對的測試成本會大很多 Test Coverage Criteria Hierarchy 而覆蓋的範圍從大到小依序為: All-paths &gt; All-Du-Paths &gt; All-Uses &gt; All-C-Uses/Some-P-Uses = All-P-Uses/Some-C-Uses &gt; All-C-Uses = All-Defs = All-P-Use 使用 All-Uses 就能有不錯的覆蓋率，並且不會有太多的測試成本，所以 All-Uses 是一個不錯的選擇 Last Edit 12-01-2023 16:02"
  },"/jekyll/2023-11-25-method_level_structural_unit_testing.html": {
    "title": "Testing | Method-Level Structural Unit Testing",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-11-25-method_level_structural_unit_testing.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 這章節主要開始介紹從 Method 為單位的 Unit testing，以 White-box 的角度來切入 白箱測試 (White-box testing) 是透過實作的程式碼來進行測試，所以這章節會介紹如何從程式碼的角度來進行測試，而不是從 Specification 的角度來進行測試 Structural Testing(White-Box Testing) In structural testing, the software is viewed as a white box. Test inputs are derived from the implementation of the software. Expected outputs are still derived from the specification of the software. Structural testing techniques include control flow testing and data flow testing. Control flow: 透過 flowchart 來確認路徑，並且進行測試 Data flow: 透過 data 的細微變化，來進行測試 通常 Data flow 能比 Control flow 更細緻的進行測試，但是也更難進行測試 5.1 Control Flow Testing Control flow testing(控制流程測試)使用程式的控制結構來制定程式的 Test cases Test cases 的設計是基於在充分覆蓋程式的整個控制結構上 Control flow graph(CFG, 控制流程圖)可以用來描述程式的控制結構 Control flow graph 一個程式的 Control flow graph, G = (N, E)，包含一組節點 N 和一組邊 E 每個 Node 都代表一組程式的 Statement，有五種不同的 Node Entry: 程式的進入點 Exit: 程式的結束點 Statement: 程式的一般陳述句，不會創建新的分支，如: letExpr Decision: 包含一個條件陳述，由該條件創建 2 個或以上的分支，如: ifExpr, switchExpr Merge: 分支的合併點 如果 Control 可以從 N1 到 N2，則存在一條從 N1 到 N2 的 Edge Test Input 測試輸入可以由一條 CFG 的完整路徑中得到 完整路徑: 一條從 Entry 到 Exit 的路徑 測試覆蓋標準(Test coverage criterion) 衡量一組測試輸入對程式的覆蓋程度 假如有一個完整的 CFG 使用 BFS 會找出右邊的路徑，但要注意路徑並不代表一定能找出符合的測試輸入 Path Predicate Expression Input vector: 是輸入變數的 Vector 相對應的 Tuple of values Complete path: 包含一系列的 Decision Node Path predicate expression: 一個 Boolean expression，用來描述一個 Complete path 的 Input vector 簡單來說 Path predicate expression 就是找出能達成這條路徑的一系列 expression: Path Sensitization Path sensitization 是尋找 Paht predicate expression 的解集的過程 可以使用 Constraint Logic Programming(CLP) predicates 來實現 Path sensitization 如果一個 Path sensitization 有解就代表這條路徑是可行的 Test Coverage Criteria Statement coverage (SC) Decision coverage (DC) Condition coverage (CC) Decision/condition coverage (D/CC) Multiple condition coverage (MCC) Path coverage (PC) 這幾種 Control flow 的 Test coverage criteria 會在另外一篇做介紹，請參考: Note - Test Coverage Criteria 5.2 CFG to CLG 這裡介紹如何將 Control Flow Graph(控制流程圖)傳換成 Constriant Logic Graph(限制邏輯圖)， 如果能將 CFG 轉換成 CLG 就能更容易地透過 Graph algorithm 來找出 Path predicate expression 5.3 Data Flow Testing Data flow testing(資料流程控制)也要使用 Control flow graph(CFG) 來找出 data flow anomalies(資料流異常)， 資料流異常是基於 value 和 Variable 之間的關聯性來檢測的，例如: Variables are used without being initialized. Initialized variables are not used once. Definitions and Uses of Variables Definition(定義): An occurrence of a variable in the program is a definition of the variable if a value is bound to the variable at that occurrence. Use(使用): An occurrence of a variable in the program is a use of the variable if the value of the variable is referred at that occurrence. Predicate Uses and Computation Uses Predicate Uses(謂詞使用): A use of a variable is a predicate use (p-use) if the variable is in a predicate and its value is used to decide an execution path. Computation Uses(計算使用): A use of a variable is a computation use (c-use) if the value of the variable is used to compute a value for defining another variable or as an output value. 如果變數 x 被使用在決策就是 p-use，如果變數 x 被使用在計算就是 c-use Definition Clear Paths A path (i, n1, n2, …, nm, j) is a definition-clear path for a variable x from i to j if n1 through nm do not contain a definition of x. 如果路徑中的 n1 到 nm 都沒有定義過 x 變數，則這條路徑是一條 definition-clear path Definition-C-Use Associations 如果有一個 x 變數的 nd 存在，並且也有一個 x 變數的 nc-use， 並且從 nd 到 nc-use 存在一條 definition-clear path， 則會產生一個 definition-c-use association(定義-計算使用關聯) (n&lt;sub&gt;d&lt;/sub&gt;, n&lt;sub&gt;c-use&lt;/sub&gt;, x) Definition-P-Use Associations 如果有一個 x 變數的 nd 存在，並且也有一個 x 變數的 np-use， 並且從 nd 到 np-use 存在一條 definition-clear path， 則會產生兩個 definition-p-use association(定義-謂詞使用關聯): True: (n&lt;sub&gt;d&lt;/sub&gt;, (n&lt;sub&gt;p-use&lt;/sub&gt;, t), x) Flase: (n&lt;sub&gt;d&lt;/sub&gt;, (n&lt;sub&gt;p-use&lt;/sub&gt;, f), x) Definition-Use paths Definition-Use paths(DU-Path, 定義-使用路徑): 如果一條 path (n1, n2, …, nm) 滿足以下條件則他是變數 x 的 DU-Path: n1 是 x 的 definition n1 到 nm 是一條 definition-clear simple path(DCSP, 定義-清晰簡單路徑) n1 到 nm 是一條 definition-clear loop-free path(DCLFP, 定義-清晰無迴圈路徑) Definition-clear simple path: 除了 n1 和 nm 之外的所有 Node 都是不同的，並且 nm 是 x 的 definition nm 必需是一個 c-use Definition-clear loop-free path: 所有 Node 都是不同的 nm 必需是一個 p-use 上圖中的 (1, 2, 4) 是一條 DCSP，而 (1, 2, 3, 5) 是一條 DCLFP (1, 2, 3, 5, 1) 也是一條 DU-Path，因為這是一條 Definition-clear simple path，1 跟 5 都是 x 的 definition Test Coverage Criteria All-defs coverage All-c-uses coverage All-c-uses/some-p-uses coverage All-p-uses coverage All-p-uses/some-c-uses coverage All-uses coverage All-du-paths coverage 這些是關於 Data-flow 的覆蓋流程標準同樣在另外一篇做介紹，請參考: Note - Test Coverage Criteria Example 下面的例子會說明如何進行 Data-flow testing，假如有一個 Control flow graph 如下: 先找出所有 Variable 的 definition, use: x: definition: 1, 6, c-use: 3, 6, 7, p-use: 2, 4, 5 a: definition: 3, 7, c-use: 8, p-use: none 找出所有的 Associations，一定是從 definition 到 use: x1: c-use: (1, 3, x), (1, 6, x), (1, 7, x) p-use: (1, (2, t), x), (1, (2, f), x), (1, (4, t), x), (1, (4, f), x), (1, (5, t), x), (1, (5, f), x) x6: c-use: (6, 6, x), (6, 7, x) p-use: (6, (5, t), x), (6, (5, f), x) a3: c-use: (3, 8, a) a7: c-use: (7, 8, a) 找出可以符合測試覆蓋標準的 DU-Path: 這裡以 All-Uses Coverage 為例，所以要找出可以覆蓋所有 Use 的 DU-Path Data flow testing for CLG 可以發現如果從 Control flow grapg 進行 Data flow testing 在找出 DU-Path 會比較麻煩，因為路徑上還要處理 Decsion Node， 所以可以先將 Control flow graph 轉換成 Constriant Logic Graph 再進行 Data flow testing，這樣在找尋路徑的時候就只需要處理是否有走過這個 Node 就好。 x: definition: 1, 9, c-use: 4, 9, 10, p-use: 2, 3, 5, 6, 7, 8 a: definition: 4, 10, c-use: 11, p-use: none 這時候如果是 All-Uses Coverage 的話就只需要找出一條有走過所有 Node 的路徑就可以了。 Last Edit 11-25-2023 18:18"
  },"/jekyll/2023-11-24-deadlock.html": {
    "title": "OS | Deadlock",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-11-24-deadlock.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. Liveness 指的是 Task 在執行期間必須滿足的一組屬性，以確保 Task 在執行週期中能不斷進行下去，這裡舉了三種例子導致 Liveness failure 的情況: Deadlock, Livelock, Priority inversion，並且會在之後介紹一些預防 Deadlock 的方式。 What is deadlock? How to prevent deadlock? What is livelock? What is priority inversion? Priority inheritance protocol, priority ceiling protocol Deadlock 6.1 What is deadlock? 6.2 Deadlock Prevention 6.3 Deadlock avoidance 6.1 What is deadlock? Deadlock 就是指一群 task 互相等待對方釋放資源，造成所有 task 都無法繼續執行的狀況，例如: TaskA TaskB 1. lock(y) 2. lock(x) 3. lock(x) 4. lock(y) 在這種情況下 A 等 B 的 unlock(y)，B 等 A 的 unlock(x)，造成 A 跟 B 都無法繼續執行 要注意並不是每次都會發生 deadlock，只有在上面這樣交錯 lock 的情況才會發生 deadlock 簡單判別 Dealock 的方式是如上圖，假如一個圖中出現 Cycle，那麼就代表有可能會發生 deadlock。 一種解決方式是索取資源時都按照順序，兩個 Task 都必須先去 lock(x) -&gt; lock(y)，這樣就不會發生 deadlock 6.2 Deadlock Prevention 注意要發生 deadlock 必須滿足以下四個條件，所以只要破壞其中一個條件就可以預防 deadlock 的發生 要發生 deadlock 必須滿足以下四個條件: Mutual exclusion: 資源不能被同時使用 Hold and wait or resource holding: Task 持有一個資源並且等待另一個資源 No preemption: OS 無法把分配出去的資源重新分配 Circular wait: Resuource allocation graph 中至少有一個 cycle Prevention mutual exclusion Deadlock 是因為我們想要使用 Critical section，而 Mutual exclusion 是 CS 的主要特性，所以這是很難避免的條件 Mutual exclusion 是 Critical section 的主要功能 通常也是 Resource 的本身特性，例如: 印表機不可能讓兩個 Task 同時使用 在一些情況下可以改寫演算法，讓 Resource 可以 Lock-free，例如: Lock-free concurrent queue 或讓每個 Task 都有自己的 Resource，不需要共用 例如: ptmalloc，每個 task 都有自己的 pool 去分配記憶體，如果不夠再向其他 task 借 Hold and wait 這個就是可以去避免的條件，這裡講一些避免的方法 所有的 Task 在一開始就把所有的資源都 lock 住 造成資源的使用率很低，因為有些資源可能不會被使用到，例如: 要打開 Powerpoint 但是因為 PPT 有可能會使用到印表機，所以要把印表機 lock 住? lock 的時間拉長，從「需求開始 -&gt; 資源使用結束」變成「程式開始 -&gt; 資源使用結束」 如果要使用新的資源，就必須先把手上的資源都釋放掉 看 6.1 的範例，B 要 lock(x)，就必須 unlock(y)，這樣就不會發生 deadlock 這樣的缺點是程式碼很難寫，B 會 lock(y) 就是想要使用 y，應該不太可能隨便的去 unlock(y) 並且有可能會造成 starvation，想要使用更多資源的 task 會更容易 starvaion No preemption No preemption 也是 Resource 的本身特性，因為如果讓不能 preempt 的資源，變成 preemptable 代價通常很大 印表機原則上是不能被 Preempt 的，例如我要緊急印資料給客戶，此時直接把印表機關機，重開這樣就是一種使印表機 Preemptable 的方法 有些時候可以使用 rollback 的方式解決 例如: 兩個 task 都去執行，到最後要去 commit 的時候去決定誰要 rollback 雖然 rollback 在 worst case 下是很爛的方法，但在一些情況下是可以接受的 也可以用 multi-version 去解決 例如: RCU(Read-Copy-Update)，在更新的時候不會去修改原本的資料，而是複製一份新的資料，並在更新完後再去切換 或者是如果沒辦法立刻 lock 某個資源的話，該 task 就釋放所有的資源(不是去搶別人，就是讓別人搶) Circular wait 這是最容易避免的條件，只要讓所有的 Task 都按照順序去 lock 資源就可以避免 依照特定順序去 lock 資源，例如: 先 lock(x) 再 lock(y) 這樣的方是與資源的特性等等無關，主要與程式碼的撰寫方式有關 例如: lock 的時候依照 memory address 的順序去 lock 但是這樣的缺點是有可能造成資源的使用率偏低 本來是先 lock(b) -&gt; lock(a)，需要的時候才去 lock 變成 lock(a), lock(b)，但是在執行 b 的處理的時候可能不會用到 a，造成 a 的使用率偏低 6.3 Deadlock avoidance 之前講的是在設計時預防 deadlock，這裡講的是在使用時避免 deadlock Deadlock Prevention(預防): 在設計時確保系統不會進入 Deadlock，也就是確保至少一個 Deadlock 的條件不會發生 有可能限制了系統資源的有效利用導致資源利用率低下 例如: Non-blocking, Serializing token, Dijkstra’s algorithm, etc. Deadlock Avoidance(避免): 在分配資源的過程時做出決策，因此要能使系統確定下一個狀態是否 Safe，需要知道所有 Resource 的狀態 有可能導致系統被 Block 降低效能 例如: Banker’s algorithm, Wait/Die, Wound/Wait, etc. 因此這裡要先定義 Safe state，也就是表示絕對不會進入 deadlock 的狀態，反之就是 Unsafe state。 這裡先介紹兩個著名的 Deadlock avoidance 演算法 Priority ceiling protocol(PCP): 假如有兩個 Task A, B，B 優先權較高，但 A 持有 Lock，此時提高 A 的優先權與 B 同等，直到 A 釋放再回到原本的優先權，這樣就可以避免 B 被 A block 可以防止系統進入 deadlock 高優先權的 task 等待低優先權的 task 釋放資源，最多等一次 是非常有用且知名的 Real-time OS 常用演算法 需要搭配 Rate-monotonics scheduling(RMS) 使用 RMS 是 Real-time OS 常用的排程演算法 Stack resource policy(SRP) 特性上與 PCP 非常相似 可以搭配 Earliest deadline first(EDF) 排程演算法使用 If System into Deadlock 假如系統進入 deadlock，有兩種處理方式: Roll-back: 假如系統支援 rollback，就可以把某些 task rollback 並釋放該 task 所持有的資源 Kill: 直接 kill 掉某些 task，例如: 持有資源最多的 task 通常如果系統進入 Deadlock 的話，很難知道內部發生的原因 Linux 支援 Kdump，可以在 kernel panic 的時候 dump 出 kernel 的記憶體，這樣就可以知道 kernel panic 的原因，這要在 kernel compile 時加入相關的設定 Last Edit 1-17-2024 15:24"
  },"/jekyll/2023-11-23-based_on_clp_testcases.html": {
    "title": "Testing | Based on CLP Testcases",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-11-23-based_on_clp_testcases.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 簡單記錄下 OCL 與 CLP 的對應關係，以及如何使用 CLP 產生測試案例。 1. Constraint Logic Programming Specification Constraint Logic Programming Predicates 以下是一個產生測試案例的 CLP Predicate 規範，分別應對 Constructor 與 Method 的測試案例: testConstructor(ArgPre, Obj, Arg, Exc):- ArgPre: the arguments before the constructor call Obj: the object created after the constructor call Arg: the arguments after the constructor call Exc: the exception thrown after the constructor call testMethod(ObjPre, ArgPre, ObjPost, ArgPost, Ret, Exc):- ObjPre: object before invocation ArgPre: arguments before invocation ObjPost: object after invocation ArgPre: arguments after invocation Ret: value returned after successful invocation Exc: exception thrown after failed invocation Example: 以下是一個 Triangle Constructor 的 OCL: contextTriangle :: Triangle(int sa, int sb, int sc) pre IllegealArgException: sa + sb &gt; sc and sa + sc &gt; sb and sb + sc &gt; sa post: a = sa and b = sb and c = sc 以此建立 OCL 後將會得到以下四條路徑，並收集路徑上的限制式(Constraint)來編寫 CLP: Valid 的 CLP 將會是以下的形式: % Include constraint solving library :- lib(ic). testTriangle1([Sa,Sb,Sc], [A,B,C], [Sa,Sb,Sc], []):- % Domains of variables [Sa, Sb, Sc, A, B, C] :: 1 .. 32767, % Constraints on variables Sa + Sb #&gt; Sc, Sa + Sc #&gt; Sb, Sb + Sc #&gt; Sa, A #= Sa, B #= Sb, C #= Sc, % Solving constraints labeling([Sa, Sb, Sc, A, B, C]). Invalid 的 CLP 將會是以下的形式，將 Precondition 的限制式改為不符合的形式: % Include constraint solving library :- lib(ic). testTriangle2([Sa,Sb,Sc], [], [Sa,Sb,Sc], [exception]):- % Domains of variables [Sa, Sb, Sc] :: 1 .. 32767, % Constraints on variables Sa + Sb #&gt; Sc, Sa + Sc #&gt; Sb, Sb + Sc #=&lt; Sa, % Solving constraints labeling([Sa, Sb, Sc]). Last Edit 11-23-2023 15:56"
  },"/jekyll/2023-11-09-synchronization.html": {
    "title": "OS | Synchronization (Unfinished)",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-11-09-synchronization.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 這個章節主要講的是 Linux 中如果有多個 Task 同時存取記憶體，要怎麼去處理 Synchronization 的問題。 多個 Task 如果同時存取 Memory 會發生彼此複寫的問題 同時 Peripheral devices 在 MMIO 中也是一塊 Memory 要保護 Memory 最直覺的想法是不要讓會存取同一份 Memory 的 Task 同時執行 設計 Critical Section，避免 Task 同時執行 Critical Section 還必須是有效率，公平的 具有啟發性的方法 Peterson’s solution 假設 read, write 是 atomic operation 證明 Peterson’s solution 滿足 Critical Section 三個條件 Critical Section 三個條件: Mutual Exclusion, Progress, Bounded Waiting C11 實現 Peterson’s solution 挑選合適的保護機制 Mutex( /semaphore) = spinlock + sleep + wakeup 預期等待的時間很短，使用 spinlock，例如: (Peterson’s solution) 需要等待一段時間，使用 mutex 或 semaphore 上述的 sleep 與 wakeup 在 Linux 中由 system call futex 實現 spinlock 與 peterson’s solution 都類似於使用 while loop 來做 busy waiting 常見的問題形式 Producer-Consumer Problem(生產者與消費者問題)，例如: 驅動程式與周邊設備的溝通 Dining Philosophers Problem(哲學家就餐問題)，例如: 多個 Task 之間的資源交換 Reader-Writer Problem(讀者與寫者問題)，例如: 只有閱讀檔案而不修改檔案的 Task Multi-Process 同時存取時，要依照哪個 Process 的時間 或者如何保證分散式系統之間也能保證存取的正確性(版本概念) 最後討論一些深入的問題 從硬體的角度來看 atomic operation 的實現 了解軟體使用的 atomic operation 的代價 以 Linux kernel 為例說明應用技巧 多種的 spinlock(C11 實現)，semaphore(以驅動程式為例) 更深入的討論 memory order Atomic 在多處理器上要做到多少的保證 5.1 Multi-Process processing one data at the same time 這裡用一個簡單的例子來說明同時處理一筆資料會產生的問題，可以看到輸出結果並不是我們預期的，這是因為兩個 thread 同時存取 global 變數，造成彼此複寫的問題。 #include &lt;pthread.h&gt; int global=0; void thread(void) { for (int i=0; i&lt;1000000000; i++) global+=1; } int main(void) { pthread_t id1, id2; pthread_create(&amp;id1,NULL,(void *) thread,NULL); pthread_create(&amp;id2,NULL,(void *) thread,NULL); pthread_join(id1,NULL);pthread_join(id2,NULL); printf(\"1000000000+1000000000 = %d\\n\", global); } // 1000000000+1000000000 = 1037054916 使用 gcc -o exam1.exe exam1.c -g - 之後使用 gdb 來查看程式碼，使用 disassemble \\m thread 來反組譯觀察問題在哪， \\m 代表把 c 跟 assembly code 一起顯示。 gdb ./exam1.exe (gdb) disassemble /m thread Dump of assembler code for function thread: 4 void thread(void) { 0x0000000000001159 &lt;+0&gt;: push %rbp 0x000000000000115a &lt;+1&gt;: mov %rsp,%rbp 5 for (int i=0; i&lt;1000000000; i++) 0x000000000000115d &lt;+4&gt;: movl $0x0,-0x4(%rbp) 0x0000000000001164 &lt;+11&gt;: jmp 0x1179 &lt;thread+32&gt; 0x0000000000001175 &lt;+28&gt;: addl $0x1,-0x4(%rbp) 0x0000000000001179 &lt;+32&gt;: cmpl $0x3b9ac9ff,-0x4(%rbp) 0x0000000000001180 &lt;+39&gt;: jle 0x1166 &lt;thread+13&gt; 6 global+=1; 0x0000000000001166 &lt;+13&gt;: mov 0x2ec0(%rip),%eax # 0x402c &lt;global&gt; 0x000000000000116c &lt;+19&gt;: add $0x1,%eax 0x000000000000116f &lt;+22&gt;: mov %eax,0x2eb7(%rip) # 0x402c &lt;global&gt; 7 } 0x0000000000001182 &lt;+41&gt;: nop 0x0000000000001183 &lt;+42&gt;: nop 0x0000000000001184 &lt;+43&gt;: pop %rbp 0x0000000000001185 &lt;+44&gt;: ret End of assembler dump. 假設 global 的初始值為 0，在這個程式中有可能會依照這樣執行，CPU1 做完加法後寫入 %rip，CPU2 也做完加法後寫入 %rip，這樣就會造成彼此複寫的問題。 CPU1 CPU2 1. mov 0x2ec0(%rip),%eax 1. 2. 2. mov 0x2ec0(%rip),%eax 3. add $0x1,%eax 3. 4. mov %eax,0x2eb7(%rip) 4. 5. 5. add $0x1,%eax 6. 6. mov %eax,0x2eb7(%rip) Atomicity 的意思就是當我們在對一個 Data struct 做操作時，要保證整個 struct 是一次性的更新，這裡的 global variable 是一個非常簡單的例子，但是在實際的程式中可能會有很複雜的 Data struct， 因此要保證整個 struct 是一次性的更新是非常困難的。 Definition correct solution 一個正確的 Multi-thread program 應該要跟其對應的 Single-thread 的 program 有相同的行為 Critical Section 5.2 Race condition problem 5.3 Critical section three conditions 5.4 Peterson’s solution 5.5 Proof peterson’s solution 5.6 C11 implementation Critical section 其實就是一套協定，這套協定使多個 Task 之間可以互相合作 5.2 Race condition problem Race condition(競爭條件)是指軟體系統的行為，當操作是基於無法控制順序的事件或時間，當這些事件沒依照 Programer 意圖的順序發生時，就會出現 bug。 Race condition 的其中一種解決方式就是 引入 Critical section，讓 Task 之間互相合作 注意 Critical section 所保護的是程式碼 5.3 Critical section three conditions Mutual Exclusion(互斥, 基本條件) 如果有一個 Task 在執行 Critical section，那麼其他 Task 就不能執行 Critical section Progress(進展, 有效率) 如果沒有 Task 在執行 Critical section，只有不在 Remainder section 的 Task 才能決定誰可以執行 Critical section，並且不能無限期的等待 Bounded Waiting(有界等待, 公平性) 如果有一個 Task 想要執行 Critical section，那麼就不能讓這個 Task 被無期限的等待 例如: 有 A, B 兩個 Task，但故意永遠只讓 A 執行 Critical section，這樣 B 就會無限期的等待 libc 其實並沒有滿足 Bounded Waiting 的條件，在某些情況下會造成 starvation 5.4 Peterson’s solution Peterson’s solution 是一個完全解決 Race condition 的純軟體演算法 假設只有 P0, P1 兩個 Task 對於硬體有一些基本假設 read, write is atomic operation shard memory system 因為假設是 shard memory system，所以無法用在 distributed system Peterson’s solution 可以擴展到 N 個 Task 假設 P0, P1 共享兩個變數 boolean flag[2] = {false, false}; /* Represents who wants to enter the Critical section */ int turn; /* 0 means P0, 1 means P1 has priority when entering the critical area */ P0 source code while (1) { flag[0] = true; /* P0 wants to enter the critical section */ turn = 1; /* IF P1 wants to enter the critical section, P1 has priority */ while (flag[1] &amp;&amp; turn == 1) ; /* busy waiting */ /* critical section */ flag[0] = false; /* exit section */ /*remainder section */ } 注意 P0 會在 turn 優先讓 P1 進入 Critical section，同樣的 P1 也會在 turn 優先讓 P0 進入 Critical section 對方想進入 Critical section，那麼自己就讓對方進入 Critical section 沒有人想進入 Critical section，那麼就自己進入 Critical section 對方離開 Critical section 後，自己就可以進入 Critical section 5.5 Proof peterson’s sol to satisfy critical section three conditions Proof: Mutual Exclusion Shard memory 中，Task 都能修改 turn, 所以 turn 只會有兩個值，0 或 1 write 必須是 atomic operation，否則 P0, P1 有可能讀到不同的值 Progress 1. flag[0] = true; 1. flag[1] = true; 2. turn = 1; 2. turn = 0; 3. while (flag[1] &amp;&amp; turn == 1) 3. while (flag[0] &amp;&amp; turn == 0) 假設 P0 想進入 CS，P1 沒有想進入 CS，flag[1] = false，這樣可以直接進入 CS 假設 P0 想進入 CS，P1 只執行到 flag[1] = true，這樣 P0 會進入 busy waiting，但是 P1 會在 turn = 0 再次禮讓 P0 進入 CS 假設 P0, P1 都同時執行到 while loop，但是 turn 會確保至少有一個 Task 會進入 CS Bound Waiting 1. flag[0] = true; 1. flag[1] = true; 2. turn = 1; 2. turn = 0; 3. while (flag[1] &amp;&amp; turn == 1) 3. while (flag[0] &amp;&amp; turn == 0) 4. flag[0] = false; 4. while (flag[0] &amp;&amp; turn == 0) 5. flag[0] = true; 5. while (flag[0] &amp;&amp; turn == 0) 6. turn = 1; 6. while (flag[0] &amp;&amp; turn == 0) 7 while (flag[1] &amp;&amp; turn == 1) 7. while (flag[0] &amp;&amp; turn == 0) 這裡的重點其實在於 turn，不是 flag，因為 flag 只是代表誰想進入 CS，但是 turn 代表誰有權利進入 CS 假如 P0 把 flag 設為 false 的時候，剛好 P1 被 ctx 這樣也並不會錯過進入 CS 的機會 所以在 P0 禮讓的情況下，P1 一定會在下次進入 CS 在這裡會覺得好像 flag, turn 的順序如果調換也會有一樣的效果，但是記住 flag 只是代表誰想進入 CS，但是 turn 代表誰有權利進入 CS，如果調換會有以下情況 1. while(1) { 1. while(1) { 2. turn = 1; 2. 3. 3. turn = 0; 4. 4. flag[1] = true; 5. 5. while (flag[0] &amp;&amp; turn == 0) 6. flag[0] = true; 6. /* Critical Section */ 7. while (flag[1] &amp;&amp; turn == 1) 7. 8. /* Critical Section */ 8. 9. 9. flag[1] = false; 10. flag[0] = false; 10. /* Remainder Section */ 11. /* Remainder Section */ 11. } 12. } 12. P1 進入 CS 但是是因為 flag[0] = false，而實際上 P0 也想進入 CS 只是還沒有執行到 P0 進入 CS 但是是因為 turn = 0，對方禮讓但其實對方還在 CS 中只是還沒將 flag 設置為 false 這樣就會造成 P0, P1 同時進入 CS，違反 Mutual Exclusion Proof flag must before turn: P0 和 P1 同時執行，都有可能去執行 turn，因此 turn 不是 0 就是 1 這裡的前提是一定會有 flag[0] == flag[1] == 1 代表兩者都想進入 CS 這個步驟是必須的，必須先確認要進入 CS，之後才能去禮讓對方 理解這部分是為了如果之後要設計 spinlock，即使有很多已經寫好的演算法，但是依然有可能在特別的情況下需要自己設計，此時理解這部分的證明就非常重要 5.6 C11 implementation 在 C11 實作的時候要注意的是使用 這個標頭檔，並且要注意 Compiler 在做 Optimization，有可能把重要的部份給省略掉， 因為 Optimization 保證的是 Single-thread 的行為，所以在 Multi-thread 的行為上就不一定會正確。 peterson’s-sol.c， P0 的程式碼如下，注意到要使用 atomic_store 進行操作，並且在 atomic_thread_fence() 來保證編譯器最佳化不會修改程式碼的順序。 atomic_int turn=0; atomic_int flag[2] = {0, 0} void p0(void) { printf(\"start p0\\n\"); while (1) { atomic_store(&amp;flag[0], 1); atomic_thread_fence(memory_order_seq_cst); atomic_store(&amp;turn, 1); while (atomic_load(&amp;flag[1]) &amp;&amp; atomic_load(&amp;turn)==1) ; /* Critical Section */ in_cs++; nanosleep(&amp;ts, NULL); if (in_cs == 2) fprintf(stderr, \"p0及p1都在critical section\\n\"); p0_in_cs++; nanosleep(&amp;ts, NULL); in_cs--; /* Remainder Section */ atomic_store(&amp;flag[0], 0); } } 這個程式的執行將會如下，可以看到無論多少次執行這兩個 Task 進入 CS 的次數都在 1 的範圍內。 ./peterson start p0 start p1 p0: 3333, p1: 3332 p0: 6684, p1: 6684 p0: 10046, p1: 10046 p0: 13401, p1: 13400 p0: 16768, p1: 16768 atomic_sotre() 隱含需要使用 atomic_thread_fence() 如果想要更高效率的 code 應該使用: atomic_store_explicit(address, memory_order) atomic_load_explicit(address, memory_order) memory_order 指定使用哪一種記憶體模型 另外要注意在 signal handler 中不應該使用 printf() 這部粉可以參考系統程式設計 Peterson’s solution conclusion Peterson’s solution 提供一個滿足 Critical section 三個條件的純軟體演算法 如果 Critical section 很長，CPU 會浪費大量時間在 Busy waiting 例如: P0 進入 CS 後要執行約 1 分鐘，P1 會在這段時間一直在 Busy waiting，如果是這樣 P1 應該要釋放 CPU 給其他 Task 先使用， 等到時間到了再去檢查是否可以進入 CS，例如: mutex 使用 adaptive 的方式 mutex 後面參數的 PTHREAD_MUTEX_ADAPTIVE_NP 代表使用 adaptive 的方式，這樣就可以避免 busy waiting Semaphore &amp; Mutex 5.7 Definition of Semaphore 5.8 Mutex 5.9 Mutex in libc 5.10 Futex in Linux 這裡來介紹與 Peterson’s solution 不太一樣的機制 Mutex 與 Semaphore，最主要的差異是有可能產生 context switch，並且跟 spinlock 不同的應用場景有哪些。 實際上 Mutex 與 Semaphore 都有 spinlock, sleep, wakeup 這三個機制所實作。 The difference between Mutex and Spinlock Mutex lock 不成功時，幾乎都會去做 context switch context switch 需要去耗費一些 CPU time，所以除非要等很久否則使用 semaphore 會比較好 Spinlock lock 不成功時，會一直做 busy waiting(Loop) loop 會讓 CPU 不斷嘗試進入 CS，但如果等待太久會造成 CPU 資源的浪費 等待時間長應該使用 Mutex，等待時間短應該使用 Spinlock Spinlock 通常效能比 Mutex 好，所以 Database 等大型軟體會使用 Spinlock 但如果 task 持有 lock 但被 scheduleout，會造成其他 task 一直在 busy waiting Semaphore, Mutex 中等待的 task 都被 scheduleout，所以不會造成 busy waiting 5.7 Definition of Semaphore Semaphore 的定義: 假如現在有 P1 - P4 要進入 CS 都執行到 while(S &lt;= 0) 此時持有 lock 的 task 發出 signal，S++ P1 - P4 一定會有一個人離開 while loop，執行 S–; 在這裡必須假設這個步驟是一次執行完畢，所以不會有其他 task 同時離開 while loop 要注意這裡只是一個定義，而不是實現的方式 Semphore 的實作樣貌: wait: 想要進入 CS 的 task 呼叫 wait() value–, 如果 value &lt; 0 就進入 list 等待 sleep(), 呼叫 scheduler context switch signal: 離開 CS 的 task 呼叫 signal() value++, 如果 value &lt;= 0 代表有 task 在等待 wakeup(), 從 list 中取出一個 task 並且喚醒從 sleep() 往下執行 Semphore 的使用方式: Semphore 的 value 可以是: 0&lt;: 代表有多個 task 在等待 =0: 沒有 task 在等待，通常是初始化的狀態 &gt;0: 代表一次最多有 X 個 task 可以進入 CS 例如: 一次最多 3 個 task 可以進入 CS，value 的初始值就是 3，進入 3 台後 value = 0，此時下一個 task 執行 value– 就會進入 list 等待 通常會有一個 Struct 來管理 Semphore，value 是無法直接修改的，必須使用他設計的函數 5.8 Mutex 相較於 Semphore，Mutex 可以有更多特色 可以判斷是誰 Lock 住 Mutex (owner) 可以支援 Priority inheritance 可以支援或不支援 Nested lock 可以支援 Adaptive lock 假如我不知道應該使用 spinlock 還是 mutex，可以使用 adaptive lock，這樣就可以自動選擇 Adaptive Mutex 如果 p 和 q 競爭 Mutex(lock)，這裡討論 p 的情況 Mutex 沒上鎖，p 獲得 lock Mutex 已上鎖，q 持有 lock q 在另一顆 CPU，並且 q 在 OS 的 waiting queue 中，例如: q 在等待 I/O p 會進入 sleep 等待 mutex(context switch) q 既然已經進入了 waiting queue 那代表可能在這一個 epoch 內都不會再執行，那麼 p 也就不需要再去痴痴等待了 q 在另一顆 CPU，但是 q 不在 OS 的 waiting queue 中，代表 q 在運算 p 會進入 busy waiting，直到 q 釋放 lock q 因為在執行，所以代表 q 會在短時間內釋放 lock，因此 p 去 spinlock 等待或許比 ctx 更有效率 q 和 p 在同一顆 CPU，則 p 進入 sleep 等待 mutex(context switch) 既然在同一顆 CPU，那麼 p 跑去做 busy waiting 也沒有意義，反而去搶奪 CPU 資源造成 q 也變慢， q 變慢代表 q 釋放 lock 的時間也會變慢，因此 p 進入 sleep 等待 mutex 會比較好 這裡可以藉由硬體的支援，來使用例如 pause(), mwait() 這樣的指令來讓 p 的 vcore 變慢，讓 q 有機會釋放 lock 以上的這些情境都代表 p 必須要知道 mutex 持有者的狀態，這樣才能決定自己要進入 sleep 還是 busy waiting 這邊介紹的 Adaptive Mutex 出自於 Sun Solaris，Adaptive 還有很多實作方式 5.9 Mutex in libc glibc/nptl/pthread_mutex_lock.c glibc 中的 Adaptive Mutex 實作則是去依照過去等待這個 lock 所釋放的時間來設定 spinlock 的 loop 次數， 如果超過次數還無法成功，那就釋放 CPU 資源，並且進入 sleep。 LLL_MUTEX_LOCK(mutex) 如果需要使用 Lock 的時候，如果對系統不熟悉，盡量避免隨便去使用 spinlock，可以使用 Adaptive Mutex signal-wait-adptive-mutex.c 我們來執行這個範例程式: 在這個程式中使用了 semaphore 來確保 p 會比 q 先執行 去觀察 p 是否執行 usleep() 有，context switch 的次數變多，因為 q 等不到 p 釋放 lock，所以 q 會去做 context switch 沒有，context switch 的次數變少，因為程式很短，所以 q 會去做 busy waiting 等 p 釋放 lock Performance counter stats for './exam3.exe': 6.45 msec task-clock # 0.598 CPUs utilized 200 context-switches # 31.027 K/sec 0 cpu-migrations # 0.000 /sec 57 page-faults # 8.843 K/sec &lt;not supported&gt; cycles 0 stalled-cycles-frontend 0 stalled-cycles-backend # 0.00% backend cycles idle &lt;not supported&gt; instructions &lt;not supported&gt; branches &lt;not supported&gt; branch-misses 0.010775604 seconds time elapsed 0.000000000 seconds user 0.010204000 seconds sys Performance counter stats for './exam3.exe -s': 11.75 msec task-clock # 0.529 CPUs utilized 400 context-switches # 34.055 K/sec 0 cpu-migrations # 0.000 /sec 60 page-faults # 5.108 K/sec &lt;not supported&gt; cycles 0 stalled-cycles-frontend 0 stalled-cycles-backend # 0.00% backend cycles idle &lt;not supported&gt; instructions &lt;not supported&gt; branches &lt;not supported&gt; branch-misses 0.022206888 seconds time elapsed 0.000000000 seconds user 0.018481000 seconds sys 上面是去執行 perf 的結果，與預期的結果相符 Pthread mutex function pthread_mutex_init(): Initialize mutex pthread_mutex_destroy(): Destroy mutex pthread_mutex_lock(): Lock mutex(blocking) pthread_mutex_trylock(): Lock mutex(non-blocking), if mutex is unlocked, lock it and return 0, else return EBUSY. pthread_mutex_unlock(): Unlock mutex pthread_mutexattr_(): Mutex attribute 5.10 Futex in Linux Spinlock 可以直接在 userspace 實作，與 Kernel 無關，但想去實作 Semaphore, Mutex 就需要 Kernel 的支援，因為這兩個機制都需要去做 context switch， futex 就是 Linux kernel 提供的一個機制，可以在 userspace 實現 Semaphore, Mutex。 futex 會透過 futex_op 來決定 mutex 的行為 long syscall(SYS_futex, uint32_t *uaddr, int futex_op, uint32_t val, const struct timespec *timeout, /* or: uint32_t val2 */ uint32_t *uaddr2, uint32_t val3); // Note: glibc provides no wrapper for futex(), necessitating the use of syscall(2). 注意 glibc 中並沒有去實作 futex()，因為這是一個僅限於 Linux 的系統呼叫，所以要使用 syscall() 來呼叫 syscall(SYS_futex, uaddr, FUTEX_WAIT, val, timeout, uaddr2, val3); futex - fast user-space locking 以下的 Function 都只是把 int futex_op 來當作 Pseudo code 來看 Main fuctions futex_wait(&amp;expected, desired, timeout) 等待 expected == desired，timeout 代表等待的時間，如果 timeout == NULL 代表無限期等待 futex_wake(&amp;val, newVal, maxWakeup) 把 val 設為 newVal，並且喚醒 maxWakeup 個等待該 val 變為 newVal 的 task Priority Inheritance(優先權繼承): 這是為了避免 Priority Inversion(優先權反轉)所設計的 futex_wait_pi(&amp;expected, desired, timeout) futex_wakeup_pi(&amp;val, newVal, maxWakeup) 如果正在等待的 task 中有比自己優先權高的 task，持有 lock 的 task 在持有 lock 的期間會把自己的優先權提升到跟等待的 task 同等 Unlock 後優先權最高的 task 會優先取得 lock Priority Inversion 是指低優先權的 task 持有 lock，但是高優先權的 task 此時也想持有 lock，導致高優先權的 task 等待低優先權的 task 釋放 lock Use Semaphore to Solve Common Problems 5.11 Producer-Consumer Problem 5.12 Readers-Writers Problem 5.13 Dining Philosophers Problem 5.14 What is the correct 5.11 Producer-Consumer Problem 比較簡單的情況是「一個 Producer」、「一個 Consumer」 這種情況下使用 atomic_write, atomic_read 來創造一個 out, in 的 Circular queue 來解決 「多個 Producer」、「多個 Consumer」，例如: 多個 Produer 同時要修改 in 如果有 read + modify(compare) + write 可以用一個硬體指令來解決，是否是比較有效率的方式? 上圖是 Circular queue 的概念，這裡的 in, out 代表的是 index，而不是實際的資料 以上是一個範例，假設 Producer 是 NIC，Consumer 是 OS Queue 儲存的可能是一個 pointer，指向儲存資料的記憶體 NIC 透過 DMA 把資料寫入記憶體，並且修改 in OS 取完資料後，修改 out 假如此時是一對一的情況，解決方法如下，注意這裡其實應該要使用 atomic 來實作: Producer 會執行 insert() 寫入資料 insert() { while (true) { item = input(); /* Produce an item */ while (((in + 1) % BUFFER_SIZE) == out) /* Only read in and out */ ; /* Busy waiting, because no free buffer */ buffer[in] = item; /* Insert item */ in = (in + 1) % BUFFER_SIZE; /* Update in */ } } Consumer 會執行 remove() 讀取資料 remove() { while (true) { while (in == out) /* Only read in and out */ ; /* Busy waiting, because no free buffer */ item = buffer[out]; /* Remove item */ out = (out + 1) % BUFFER_SIZE; /* Update out */ output(item); /* Consume the item */ } } 在這個例子裡面 in, out 都只有一個 task 會去修改，所以只要使用 atomic_write, atomic_read 就可以解決 要注意上面的例子都使用了 Busy waiting，如果程式並沒有高速的讀寫，那麼這樣的做法會造成 CPU 資源的浪費 5.12 Readers-Writers Problem Reader-Writer Problem 的定義如下: Reader 只可以讀，Writer 可以讀也可以寫 Reader 可以同時多個 Reader 一起讀同一個資料結構 Writer 同一時間內只能有一個去存取資料結構 實作上我們假設有一系列的 Writer, Reader 進入排序準備進入 CS: rrrwrr 把 rrrwrr 轉換成 rrrrrw 優先處理 Reader 但這樣的問題是，假如同一時間內不斷有 r 進入，那麼 w 就會一直等待，必須設計一個機制處理 w rrrwrr，r 跟 w 之間必須 FIFO，但連續的 r 可以同時進入 CS 所以執行順序會變成 rrr -&gt; w -&gt; rr 這裡的目標是盡量提高平行度，讓多個 Reader 同時進入 CS 假如有以上範例程式: writer 只需要去 wait(rw_mutex) 也就是 lock，跟 signal(rw_mutex) 也就是 unlock 就可以 reader: 最開頭的 wait(mutex) 到 signal(mutex) 是為了 lock CS 讓此時只有 reader 可以進入 readcount == 1 代表這是第一個 reader，所以要去 lock CS 不讓 writer 進入 離開要檢查 readcount == 0 代表最後一個離開的 reader，unlock 讓 writer 此時可以進入 假設現在有一個 writer 正在 CS 中，此時有 r0, r1, r2 要進入 CS r0 會先進入 wait(rw_mutex)，此時 writer 正在 CS 中，所以 r0 會進入 sleep r1, r2 會進入 wait(mutex)，lock 被 r0 拿走，所以 r1, r2 會進入 sleep writer 離開 CS 並且 signal(rw_mutex)，此時 r0 會 singal(mutex)，讓 r1, r2 可以依序進入 Reading 在實際使用時，可以使用 pthread 內建的 rwlock 5.13 Dining Philosophers Problem 如下圖所示，一群哲學家坐在圓桌上，每個哲學家面前都有一個盤子，而盤子之間交錯著刀叉 哲學家吃飯時必須拿起左右兩邊的刀叉才能吃飯 有什麼方法讓所有的哲學家都能吃到飯? 可能的解決方法: 所有人都先拿左邊的餐具，再拿右邊的餐具 可能所有人都拿到左邊的餐具，都等不到右邊的餐具，造成死結 對所有人編號，奇數先拿左邊的餐具，偶數先拿右邊的餐具 有可能有人運氣很差，一直拿不到餐具，不符合 Bound waiting 輪流獲得高優先權，拿到餐具的人可以吃飯然後放下餐具，直到所有人都吃完 輪流獲得高優先權，可能會造成效能瓶頸 The Dining Philosophers Problem in Linux Kernel 每顆 CPU 上都有一個 migration thread，當 CPU 有空閒時，migration thread 會去檢查有沒有 task 想要執行 pull: migration/0 發現我的工作量太少，所以去搬移 migration/3 的工作 push: migration/3 發現我的工作量太多，所以把工作搬移給 migration/0 此時就要避免 migration/0, migration/3 重複了搬移工作，例如: 依照 CPU 編號，優先鎖編號小的 CPU，這樣就只有一個 migration thread 會先執行 搬移結束後此時工作量會平均，所以不會再有 migration thread 去搬移工作 在這個例子 Linux 並沒有去考慮 bound waiting，因為 0 成功了的話 1 也就不用執行了 5.14 What is the correct 當多個 Process 同時在執行，在修改的時候要怎麼去確保資料的正確性，什麼是正確的? 假如有 Task1, Task2 同時要修改一個資料 A Task2 先讀取 A，然後以 A 為基礎做一些運算 Task1 在 Task 讀取後，才讀取 A 並修改 A，然後 Task2 才結束運算 這樣的情況下 Task2 最後運算的結果是錯誤的，因為 Task2 以 A 為基礎做運算，但是 A 已經被 Task1 修改了 同樣兩個 Task，但是這次 Task2 在 Task1 讀取前就做完運算 但是實際上這兩次 Task2 最後的結果都是一樣的 因此如果以 Task2 的結果來判斷正確性，那麼可以說這兩次情景是等價的 是否可以把平行化以後的正確性定義為「其結果等價於某個依序執行的狀況」 Atomic Operation 前面會先介紹 Computer Orangization 的一些基礎，後面介紹使用 Atomic Operation 所產生的成本 5.15 Mesh-Architecture 5.16 DMA with Cache Coherence 5.17 Cache coherence vs Atomic operation 5.18 Atomic operation 5.19 Atomic operation in c11 使用 Atomic operation 所產生的成本跟一般的指令不太一樣，並且會依照受影響的 CPU 的個數而有所不同。並且在 SMP 上如果所有 CPU 每次存取資料都要到 DRAM， 那 DRAM 就會是一個 Bottleneck，所以會有 Cache 的機制，但是 Cache 就要去處理 Cache coherence 的問題。 在同步機制上，不同的指令會影響到的 CPU 數量不同，所以成本也不同 上圖中 L2 cache 之間會有一些同步機制，而 L3 chache 也會有一些同步機制，例如: SNOOP + dictionary Bus Bus 也有各種各樣的類型 支援 Broadcast 的 Bus (Core 數量少) Ring Bus (Core 數量多) Mesh Bus (Core 數量極多) 不規則的連接圖 (AMD 階層式架構) 5.15 Mesh-Architecture 這裡以 Intel® Xeon® Processor Scalable Family Technical Overview 為例，來了解 Coherence 的代價 Core 傳遞資料的方式是透過 Mesh，先走 X 再走 Y CHA(Cache Home Agent): 類似 directory，記錄資料在那些 Core SF(Snoop Filter): 監聽 Bus 上的廣播，是否與自己有關 LLC(Last Level Cache): 最後一層的 Cache 最上層就是對外的通訊介面，例如: PCIe, UPI 左右各有一個通道，用來連接 DDR4，所以左右的 Core 會優先使用自己側的 DDR4 Ultra Path Interconnect(UPI) UPI 是一種擴展系統的一致性協定，讓多個處理器可以共享資料，並且在同一個 Memory space 支援 UPI 的 Intel Xeon 處理器會提供 2~3 個 UPI 通道，來連接到其他 Xeon 處理器 UPI 之間使用 Directory-based home snoop coherency protocol 來維持一致性 Cache Coherence CPU 必須用一些方法保證所有的 Core 看到的資料都是一致的，否則 Shared Memory 就沒有意義 Cache coherence problem: 不同 CPU 有不同的 Cache，所以可能會有新舊資料的問題 Cache coherence protocol: 一種機制，用來確保所有 CPU 看到的資料都是一致的 Snooping: 以廣播的方式來維持一致性，修改資料時會通知其他 CPU Directiory: 在每個 Cache line 上都有一個 Directory，紀錄最新的資料在那些 Cache，這樣就只需要通知有關的 CPU 但是如果核心數多起來，Directory 會變得很大，所以會有一些機制來減少 Directory 的大小，例如: Group，把一些 Core 分組， 這樣就不用紀錄所有的 Core，通知 Group 就可以了 目前通常會使用 Snooping + Directory 的做法 對 OS 來說這些方法不重要，重點是 CPU 保證所有的 Core 最終看到的資料都是一致的。 如果要保證所有的 Core 在 LLC 看到的資料都是一致的，他的成本會很高，因此現在許多的 CPU 僅僅保證「部分指令」對於記憶體系統的存取是 Atomic operation 5.16 DMA with Cache Coherence 能夠去修改記憶體的裝置有 CPU 還有周邊的 Device，例如: NIC, GPU 之前談的 Cache coherence 都是針對 CPU，但是實際上有去修改 Memory 的裝置都應該要去考慮同步的問題，因為 DMA 通常比較慢的原因， 因此要確保 DMA 搬移到 Memory 後，CPU 也能看到最新的資料，有軟硬體的方式可以來做到。因為是 OS 的課程，所以這裡要討論的是對於我們寫程式會有什麼影響。 以軟體的方式來做的話 Linux 中 CPU 把資料寫入到 DMA 會搬移的資料時，會讓 Cache 的資料 flush 到 DRAM，確保 Device 拿到的資料是最新的 DMA 寫入 DRAM 後，CPU 要取這筆資料時會去把 Cache 的資料清除，並且重新從 DRAM 讀取，確保 CPU 拿到的資料是最新的 以硬體來說，要寫入時可以去找尋最新資料的位置在哪 在 DRAM: 就直接寫入 DRAM 更新資料 在 Cache: 硬體去主動把 Cache 的資料取消 直接寫入 Cache 延伸閱讀: Dynamic DMA mapping Guide 5.17 Cache coherence vs Atomic operation 從 Linux Kernel 5.4 之後支援 C11 的 atomic，在 5.4 之前的版本，Linux Kernel 會使用自己的 atomic 這裡要提出一個疑問，如果已經有 cache coherence 的機制，那為什麼還需要 atomic operation? 假如有以下的情況可能發生: 如果某個瞬間，兩個 core 同時對一個變數做了修改，其他 core 是否會看到同步中間的狀態，例如: alignment alignment: 可能一次修改要更新兩條 cache line，此時有可能看到只更新一半的狀態 非常多核心的處理器中，不能保證一個 core 做出修改後會不會有傳遞的延遲 core1 修改 A，但還沒傳遞到 core2 的時候 core2 又修改了 A core 數量越多，資料交換的頻寬就越是效能瓶頸 write buffer, read buffer: 在進行操作時有沒有可能造成 write, read 沒有按照順序執行，例如: 有 cache miss 所以先執行的 write task 被 context switch 這裡要牽涉到的是 Memory order 的問題，因為在 write buffer, read buffer 中是沒有 cache coherence 演算法的，必須要確定順序才能保證正確性 如果要嚴格執行順序的話，要在這些操作之間插入 mfence 可能的解決方法: 讓所有對記憶體的操作都是 atomic 的，此時去 lock bus，讓其他 core 無法存取記憶體，直到這個 core 完成操作 在一個實際的運算中可能每 3-4 個指令就會有 load, store，這樣的話等於要頻繁的去 lock bus 不太現實 部分指令對於一個記憶體區間內是 atomically，這樣會比較好設計 軟體與硬體工程師設計好哪些指令去解決問題 Kaby Lake - Microarchitecture Intel Load buffer &amp; Store buffer in x86 5.18 Atomic operation 因為想要使所有的 load, store 都是 atomic 實在太難設計，因此只保證部分指令是 atomic 傳統上會有 test_n_set, swap 這樣的 Assembly test_n_set: 回傳舊的值，並且把新的值寫入，swap: 交換兩個值 這兩個程式相當於以下 c 程式，register 相當於 CPU 內部真實的暫存器 int test_n_set(int *value) { register tmp = value; *value = 1; /* Update memory */ return tmp; } void swap (int *a, int *b) { register tmp; tmp = *a; *a = *b; /* Update memory */ *b = tmp; /* Update memory */ } while(test_n_set(&amp;lock)) 一個簡單的 spinlock 這樣實作，但是他會不斷去更新 value，觸發 cache coherence 另外這兩個指令都是 read-modify-write，會讓 cache coherence 變得沒有效率 bool atomic_compare_exchange_strong (volatile atomic_int* obj, int* expected, int desired ) 是改進的方式 如果 obj 等於 expected，那麼就會更新 obj 並且回傳 true 如果 obj 不等於 expected，那麼就會把 obj 的值更新到 expected，並且回傳 false bool atomic_compare_exchange_strong (volatile atomic_int* obj, int* expected, int desired );{ if (obj == expected) { /* Only obj equals expected to lock and write obj */ obj = desired; return true; } else { expected = obj; return false; /* Only read obj */ } } 這樣的寫法不會去不斷更新 value，因此也只會觸發一次 write 造成 cache coherence 但是這裡並沒有保證誰會先發現 obj == expected，這部分要軟體去設計 5.19 Atomic operation in c11 c11 提供了一個類似 test_n_set 的支援，把 obj 與 desired swap，並且回傳舊的值 int atomic_exchange(atomict_int *obj, int desired) { register tmp = *obj; *obj = desired; return tmp; } 同樣也有 atomic_compare_exchange_n，的支援，對於 strong, weak 的差別: 在 x86 平台上的時候，strong, weak 沒有差別，因為 x86 會保證誰先發現 obj == expected strong: 保證一定會更新 obj，只有在 obj != expected 時才會回傳 false weak: 不保證一定會更新 obj，有可能因為平台造成 obj == expected 時回傳 false 此時可以使用 while loop 來包裝使用 weak，直到成功為止 應用上如果只需要執行一次，那就使用 strong，如果需要重複執行，那就使用 weak 例如說一個 Initialize 的操作，只需要執行一次並且不會重置 initialized 為 0，所以只有一個 process 會成功 void initialize_once() { if (atomic_compare_exchange_strong(&amp;initialized, &amp;expected, 1)) { /* If this line is executed, the process is the first to successfully initialize */ resource = 100; } /* Resource was already initialized by another process */ } 一個簡單的範例，在於如何實現多個 thread 對於同一個變數做 +1 的操作，compare-exchange-spinlock.c void* Counter() { int expected; for (int i = 0; i &lt; 10000000; ++i) { do { expected = counter; } while (!atomic_compare_exchange_weak(&amp;counter, &amp;expected, expected + 1)); } } 上面會在每次 while 之前都去先把 expected 更新成 counter，然後去比較 counter == expected，如果是的話就更新 counter，並且回傳 true，否則回傳 false Spinlock 5.20 Spinlock Concept and Advanced 5.20 Spinlock Concept and Advanced Spinlock 的設技巧: 「檢查 -&gt; 鎖住」這樣的方法是不對的，在檢查和進入之間有其他 task 做「檢查」，導致多個 task 同時進入 「鎖住 -&gt; 檢查 -&gt; 進入」: 「鎖住」的目的是先讓別人進不去，再去檢查能否進入 如果改變完發現不能進入，那就把「鎖住」的狀態改回來 「檢查和鎖住」用同一個 atomic operation，例如: swap, test_n_set (效率最差) compare_exchange_weak, compare_exchange_strong (比較好，但效率還是不好) 5.21 lockfreeQueue.c 如果使用 Lock-free 的方式會比使用 Semaphore 快上數倍，但程式的複雜度也會提高 volatile int in = 0, out = 0; void put() { static int item=1; /* Using to debug, Ensure that the numbers are an increasing sequence */ while ((in+1)%bufsize == out) ; /* busy waiting */ buffer[in]=item++; /* put item into buffer */ /* A memory fence should be added here to ensure that get() reads the data after item++ */ in = (in + 1)%bufsize; /* The next position to put */ } void get() { int tmpItem; /* temporary variable to store the item */ while (in == out) ; /* busy waiting */ tmpItem=buffer[out]; /* get item from buffer */ out = (out + 1)%bufsize; /* The next position to get */ } Last Edit 12-08-2023 16:03"
  },"/jekyll/2023-10-28-method_level_function_unit_testing.html": {
    "title": "Testing | Method-Level Functional Unit Testing",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-10-28-method_level_function_unit_testing.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 這章節主要開始介紹從 Method 為單位的 Unit testing，以 Black-box 的角度來切入 本章先介紹如何使用組合邏輯來尋找限制式的組合，並使用 Constraint Logic Programming 來產生測試資料， 接著介紹 UML/OCL 系統，使用 OCL 來描述一個 Class/Methods 的限制式，並且透過 Constraint Logic Graph 與 Constraint Logic Programming 來產生測試資料。 Combinational logic Decision tables Constraint logic programming UML/OCL Constraint logic graphs Constraint logic programming 4.1 Combinational Models Many applications must select output actions by evaluating combinationsof input conditions (constraints on input variables). Input variables can also be either parametersof the method, static variables of the class, or instance variables of the object. Combinational logic provides an effective language for these kinds of condition-action relationships. 例如一個 Method 有多個參數，而這些參數可能會有不同的組合，這些組合可能會對應到不同的輸出，這時候我們可以使用 Combinational logic 來描述這些組合，並且對應到不同的輸出 4.1.1 Equivalence Class Partitioning Decision Tables A combinational model uses a decision tableto represent the condition-action relationships and partition equivalence classes. A decision table has a condition sectionand an action section. The condition section lists constraints on inputvariables. The action section lists outputto be produced when corresponding constraints are true. 以 Decision table 來決定條件的組合，並且對應到輸出 Example: Class Triangle 假如有一個 Triangle Java class 如下，Constructor 只要滿足 {sa + sb &gt; sc, sa + sc &gt; sb, sb + sc &gt; sa} 就視為合法的 Tringle，否則拋出 Exception。 // Java class Triangle :: Constructor classTriangle { inta; intb; intc; public Triangle(intsa, intsb, intsc); }; 以這三個條件進行 Combianational 能獲得以下的 Decision table，將這些限制式透過 CLP 求解就能得到右圖的測試資料: Example: Method category The method category() returns the category of a Triangle object based on the lengths of its three sides: “Equilateral”, “Isosceles”, or “Scalene”. A Triangle object is an “Equilateral” triangle if it satisfies the following threeconstraints: {a = b, a = c, b = c}. A Triangle object is an “Isosceles” triangle if it satisfies one and only one of the following three constraints: {a = b, a = c, b = c}. Otherwise, It is a “Scalene” triangle. 關於 category 我們也可以透過 Decision table 來產生測試資料，以下是 category 的 Decision table // Java class Triangle :: category() public String category( ) { if (a == b &amp;&amp; b == c) return \"Equilateral\"; else if (a == b || a == c || b == c) return \"Isosceles\"; else return \"Scalene\"; } 拿上面的程式碼來做比對，實際上這個 Decision table 就是去走過所有在 Category 中可能發生的路徑。 4.2 Unified Modeling Language Unified Modeling Language(UML) 是一種用於可視化、規範、建構和文件化軟體系統工程的圖形化語言 UML 提供一種標準化的方式來編寫系統的藍圖，包括概念性的事務，如業務流程和系統功能，以及具體的事務，如程式描述、Database 架構和可重用的軟體元件 4.2.1 UML Diagrams 在 Software testing 這門課中主要會用到的是 Class diagram、Sequence diagram、State machine diagram UML 2 defines thirteenbasic diagram types, divided into two general sets: Structural Modeling Diagrams: Structure diagrams define the staticarchitecture of a model. They are used to model the ‘things’ that make up a model. Behavioral Modeling Diagrams: Behavior diagrams capture the varieties of dynamicinteraction and instantaneous state within a model as it executes over time. 在之前我有更詳細的關於 UML 的介紹，詳細可見 Unified Modeling Language Concepts 4.2.2 Class Diagrams Class Diagram 描述了一個 Class 有哪些 Attributes 和 Methods，而不是詳細的實作細節 Class Diagram 在說明 Class 或 Interface 之間的關係時最為有用 Association(關聯)和 Generalization(泛化)分別表示連接和繼承 關於類圖可以看之前的筆記，有更詳細的描述 UML Structure Diagrams Introduction 1.1 Class diagram Association 表示兩個 Class 之間有所關聯，例如: Course 和 Student 之間有一個關聯，關聯可以是多對多的關係 Generalization 表示的是兩個 Class 之間有繼承關係，例如: Animal 和 Dog 之間有一個繼承關係 4.3 Basic Object Constraint Language 關於 OCL 的基礎語法可以參考之前的筆記 Object Constraint Language Concepts，這裡就不再贅述。 4.3.5 OCL to CLG Example 這裡介紹如何將 OCL 抽象化成 CLG，之後就能透過 CLG 的路徑來產生測試資料 Constructor Valid: Class invariant before invocation Pre-Conditions Post-Conditions Default Post-Conditions Invalid: Class invariant before invocation Negation of Pre-Conditions 因此一個 Function 的 Valid 輸入將會是符合 Pre/Post-Conditions 的 CLG 路徑，經由 CLP 產生的測試資料， 反過來說當一個 Function 的 Pre-Conditions 不合法時將會產生 Invalid 的測試資料。 Negation 使用 De Morgan’s law 來轉換規則後得到 Pre-Conditions 的 Negation，這樣就有四條不同的路徑與下表相同: Varients sa+sb&gt;sc sa+sc&gt;sb sb+sc&gt;sa (sa, sb, sc) 1 T T T (1, 1, 1) 2 T T F (2, 1, 1) 3 T F T (1, 2, 1) 4 F T T (1, 1, 2) 這裡去除了 Can’t Happen 的條件，在這個例子中 Desicion table 應該會有 23 種可能 4.3.5 OCL Examples with CLG 以之前的 Triangle 為例來撰寫 OCL 將會有: Class invariant(Triangle Objects): 一個 Triangle 應該永遠滿足三邊長的條件，所以使用 Class invariant 來限制所有 Triangle 的三邊長 Constructor: OCL 表示 Triangle 這個 Constructor 的 Precondition 和 Postcondition Pre: 一個 Triangle constructor 的輸入必須滿足 sa + sb &gt; sc and sa + sc &gt; sb and sb + sc &gt; sa Post: Triangle constructor 執行後應該滿足 a = sa and b = sb and c = sc Method category: Post: 檢查 Triangle 的三邊長，並且回傳 Triangle 的類型 context Triangle inv: a + b &gt; c and a + c &gt; b and b + c &gt; a context Triangle::Triangle(intsa, intsb, intsc) pre IllegealArgException: sa + sb &gt; sc and sa + sc &gt; sb and sb + sc &gt; sa post: a = sa and b = sb and c = sc context Triangle::category(): String post: result= if a@pre = b@pre then ifa@pre = c@pre then 'Equilateral' else 'Isosceles' endif else if a@pre = c@pre then 'Isosceles' else if b@pre = c@pre then'Isosceles' else 'Scalene' endif endif endif Constraint Logic Graph 我們依照上面的 Constructor 和 Category 來建立 CLG，得到以下兩張圖，就可以以此來生成測試資料 例如我們想測試 category，能發現在 CLG 上一共有 5 條不同的路徑，我們把第一條路徑的條件放入 CLP 中求解，就能得到測試第一條路徑的測試資料。 Constraint, Pre a@+b@&gt;c@, a@+c@&gt;b@, b@+c@&gt;a@ Input Output a@=b@, a@=c@, result=’Equilateral’ 1, 1, 1 Equilateral a@=b@, a@!=c@, result=’Isosceles’ 2, 2, 1 Isosceles a@!=b@, a@=c@, result=’Isosceles’ 2, 1, 2 Isosceles a@!=b@, a@!=c@, b@=c@, result=’Isosceles’ 1, 2, 2 Isosceles a@!=b@, a@!=c@, b@!=c@, result=’Scalene’ 2, 3, 4 Scalene Post a = a@, b = b@, c = c@     依照這個 Table 去跑限制式這樣就能夠產生五條全部路徑的測試案例，例如第一條路徑的 CLP 如下: 4.4 Significance of Collections in OCL OCL 支援 Collection，詳細的語法可以參考 Object Constraint Language Concepts，這裡介紹 Collection 如何進行測試。 4.4.1 Testing of Collections For boundedcollections, test collections with bound 0, 1, n (where n 4). For a boundedcollection with bound n, test instances of size 0, 1, m (where 1 &lt; m &lt; n –1), n –1, and n. For unboundedcollections, test instances of size 0, 1, and m (where m &gt; 1). You can write a CLP predicate to generate these collection instances. 4.5 Inheritance of Constraints Liskov’s Substitution Principle(LSP, 約化的替代原則): “Whenever an instance of a class is expected, one can always substitute an instance of any of its subclasses.” 當一個程式希望使用某個 Class 時，該 Class 可以被任何繼承該 Class 的子類別所替代 即使 Subclass 進行 override，也只能產生不同的行為，但不違反本來的條件 Example: 有一個 Supercalss Report 其中有一個 Method toString，假設 Report.toString 將會返回一個空字串， 並且有以下的 contract: toString() 應該提供一個具有某種格式的字串 toString() 不能改變 Report 的對象 toString() 永遠不會 throw Exception 在這些規則下，chirdren class HTMLReport 也必須遵守這些規則，但是可以改變 toString() 的行為 toString() 返回一個 HTML 格式的字串 但是如果你的 contract 是: toString() 無論如何都返回空字串，那麼任何 override 這個 method 的子類別都會違反 LSP。 延伸閱讀: Is method overriding always a violation of Liskov Substitution Principle? Inheritance of Constraints 對於 invariants LSP 的影響: 每個 Subclass 的 invariants 都繼承自 superclass Subclass 可以加強自己的 invariants 對於 Pre/Post-condition 的影響: Pre-condition 可能會被減弱(contravariance) Post-condition 可能會被加強(covariance) Last Edit 11-26-2023 15:56"
  },"/jekylls/2023-10-26-syntax_analysis.html": {
    "title": "Compiler | Syntax Analysis Notes",
    "keywords": "Compiler Jekylls",
    "url": "/jekylls/2023-10-26-syntax_analysis.html",
    "body": "Compilers course notes from CCU, lecturer Nai-Wei Lin. Syntax Analysis(語法分析) 在這個階段會檢查 Lexical Analysis 返回的 Token 是否符合語法規則，並且建立語法樹 以下是這個章節的主要大綱，Bison 不會在這篇介紹如何使用，主要是介紹 Syntax analysis 的概念 Introduction to parsers Context-free grammars Push-down automata Top-down parsing Buttom-up parsing Bison a parser generator 4.1 Introduction to parsers 本章會先介紹 Parser 在 Compiler 中的作用，然後介紹 Context free grammar。 4.1.1 The Role of the Parser 在編譯器模型中 Systax analysis 從 Lexical analysis 獲取由 Token 所組成的字串，概念上語法分析需要建構一個 Parse tree 傳遞給 Compiler 的其餘部分進行進一步處理， 但實際上不一定要真的用一個 Data structure 來建構 Parse tree，而是在 Parsing 的過程中進行 Semantic analysis，並將資訊傳遞給 Compiler 的其餘部分。 不真正建構一個 Parse tree 通常是為了節省記憶體，但缺點就是他使 Debug 變得困難，因為無法查看 Parse tree 4.2 Context-free grammars Context-free grammars 可以系統的描述程式語言構造，例如使用 stmt 描述 statements，expr 描述 expressions，那麼: production: stmt -&gt; if (expr) stmt else stmt 我們就能透過其他 production 來描述 stmt, expr 會是什麼，還可以是什麼? 4.2.1 The Formal Definition of a Context-free Grammar A set of terminals: basic symbols from which sentences are formed 例如: if, else, (, ), id A set of nonterminals: syntactic categories denoting sets of sentences 任何非 terminal 都可以是一個 nonterminal，例如: stmt, expr A set of productions: rules specifying how the terminals and nonterminals can be combined to form sentences 例如: stmt -&gt; if (expr) stmt else stmt The start symbol: a distinguished nonterminal denoting the language 通常是最上層的 Production Example: Terminals: id + - * / ( ) Nonterminals: expr op Productions: expr -&gt; expr op expr expr -&gt; '(' expr ')' expr -&gt; '-' expr expr -&gt; id op -&gt; '+' | '-' | '*' | '/' The start symbol: expr 4.2.2 Notation Conventions 通常為了避免陳述 these are the terminals, these are thenonterminals 會使用一些約定來規範符號: Terminals: 小寫字母，例如: a b c 運算符號，標點符號，例如: + - , ( ) 數字，例如: 0 1 2 粗體字符串，例如: if else then 小寫希臘字母，例如: α β γ Non-Terminals: 大寫字母，例如: A B C 在討論構造時，例如 expression、terms、factors，使用: E T F 通常使用 S 來表示 Start symbol 斜體字符串，例如: expr stmt 具有相同標題的 Production 可以使用 | 來分隔，例如: A -&gt; a、A -&gt; b、A -&gt; c 可以寫成 A -&gt; a | b | c 除非特殊說明，第一個 Production 會是 Start symbol 4.2.3 Parse Trees and Derivations 推導(derivation) 步驟是將一個 Production 的替換過程寫出，例如 E =&gt; - E 一系列的推導步驟可以將 E =&gt; -E =&gt; -(E) =&gt; -(id) 如果使用 =&gt;* 表示在零步或多步中推導，=&gt;+ 表示在一步或多步中推導 例如上面的步驟可以簡化為 E =&gt;* -(id) Context free grammar Context free grammar(CFG) 定義的語言 L(G)，是由 CFG G 所定義的語言 一個 Terminal 字串 ω 在 L(G) 中，並且當 S =&gt;+ ω，那我們稱 ω 是 G 的一個句子(sentence) 如果 S =&gt;* α，而 α 可以包含 Non-terminal，那麼 α 是 G 的一個句型(sentence form) 如果 L(G1) = L(G2)，那麼 G1 和 G2 是等價的(equivalent) Left &amp; Right-most Derivations 每個 Derivation step 都需要兩個步驟: 選擇替換哪個 Terminal 替換後選擇一個以此 Terninal 作為開頭的 Production 這樣就可以有 Left &amp; Right 兩種推導方式: Left-most derivation: 每次都選擇最左邊的 Terminal 來替換 例如 E =&gt;lm -E =&gt;lm -(E) =&gt;lm -(E+E) =&gt;lm -(id+E) =&gt;lm -(id+id) Right-most derivation: 每次都選擇最右邊的 Terminal 來替換 例如 E =&gt;rm -E =&gt;rm -(E) =&gt;lm -(E+E) =&gt;lm -(E+id) =&gt;lm -(id+id) Exercise 4.2.1: Consider the following grammar: S -&gt; SS+ | SS* | a and the string aa+a* (a), Giver a leftmost derivation for the string. (b), Giver a rightmost derivation for the string. (a) S =&gt;lm SS* =&gt; SS+S* =&gt; aS+S* =&gt; aa+S* =&gt; aa+a* (b) S =&gt;rm SS* =&gt; Sa* =&gt; SS+a* =&gt; Sa+a* =&gt; aa+a* 4.2.4 Parse Trees and Derivations Parse Tree 是推導的圖形表示，顯示了從 Start symbol 到衍生 Sentence 的過程，這種方式過濾了選擇 Terminal 進行重寫的順序 因此不管是 Left/Right-most 都應該推導出相同的 Parse tree 4.2.5 Ambiguous Grammar 如果一個 Grammar 可以對同一個 Sentence 產生不同的 Parse tree 那就是 Ambiguous Resolving Ambiguity 大部分的 Syntax analysis 都希望 Grammar 不是 Ambiguous，可以透過以下來消除 Use disambiguiting rules to throw away undesirable parse trees Rewrite grammarsby incorporating disambiguiting rules into grammars Example The dangling-else grammar stmt -&gt; if expr then stmt | if expr then stmt else stmt | other 如果用以上的 Grammar 來分析: if E1 then if E2 then S1 else S2 我們無法確定 else 是對應哪個 then，因此會產生兩個 Parse tree 這樣就產生了兩個 Parse tree，因為在語法規則中沒有說明清楚 else 要對應哪個 if，所以可以透過以下方式來消除 Disambiguiting rules: Rule: match each else with the closest previous unmatched then Remove undesired state transitions in the pushdown automaton stmt -&gt; m_stmt | unm_stmt m_stmt -&gt; if expr then m_stmt else m_stmt | other unm_stmt -&gt; if expr then stmt | if expr then m_stmt else unm_stmt 透過這種方式，我們強制一個 then 和 else 之間只能是一個 m_stmt，這樣就可以消除 Ambiguous 4.3 Writing a Grammar 4.3.1 Lexical Versus Syntactic Analysis 由 RE 描述的每種語言也可以由 CFG 描述，例如 (a|b)*abb 也可以用 CFG 來描述: A0 -&gt; aA0 | bA0 | aA1 A1 -&gt; bA2 A2 -&gt; bA3 A3 -&gt; ε 那為什麼不在 Lexical analysis 使用 CFG? 首先 Lexical analysis 不需要與 CFG 一樣強大的表示法 RE 比 CFG 更簡潔更容易理解 RE 建構的 Lexical analysis 比 CFG 建構的 Lexical analysis 更有效率 這樣提供了將前端模塊化為兩個易於管理的部分的方法 Nonregular Constructs REs can denote only a fixed number of repetitions or an unspecified number of repetitions of onegiven construct: an, a* A nonregular construct: L = {anbn| n ≥ 0} 這個語言包含相同數量的 a 和 b，RE 沒辦法描述固定數量的 a 和 b Non-Context-Free Constructs CFGs can denote only a fixed number of repetitions or an unspecified number of repetitions of oneor twogiven constructs Some non-context-free constructs: L1 = {wcw | w is in (a|b)*} L2 = {anbmcndm | n ≥ 1 and m ≥ 1} L3 = {anbncn | n ≥ 0} CFG 只能處理一個重複的結構，這也涉及到 CFG 的 Automata，但是可以描述以下語言: L1 = {ncn | n ≥ 0} L2 = {anbmcmdn | n ≥ 0, m ≥ 0} 4.4 Top-down Parsing 這裡不會詳細介紹 Top-down Parsing，因為 Top-down 要處理的問題比較多 Top-down Parsing 是從上層的 Root 開始，使用 Leftmost derivation 建構一顆到 Leaf 的 Parse tree Predictive Parsing A top-down parsing without backtracking – there is only one alternative production to choose at each derivation step stmt -&gt; if expr then stmt else stmt | while expr do stmt | begin stmt_list end 4.4.2 FIRST and FOLLOW Sets First set The first setof a string α is the set of terminals that begin the strings derived from α. If α =&gt;* ε, then ε is also in the first set of ε. 如果 X 是一個 Terminal，那麼 FIRST(X) = {X} 如果 X 是一個 Nonterminal，並且存在 X -&gt; ε，那麼 FIRST(X) = {ε} 如果 X 是一個 Nonterminal，並且存在 X -&gt; Y1 Y2 … Yn 首先加入 FIRST(Yi) 如果 Yi 存在 ε，那就加入 FIRST(Yi) - {ε} 然後 i + 1 重複以上步驟直到 Yi 不存在 ε 簡單來說 FIRST 就是找出一個 Nonterminal 所有可能的開頭 Terminal Example: E -&gt; TE' E' -&gt; +TE' | ε T -&gt; FT' T' -&gt; *FT' | ε F -&gt; (E) | id FIRST(E) = { ( , id } FIRST(E’) = { +, ε } FIRST(T) = { ( , id } FIRST(T’) = { *, ε } FIRST(F) = { ( , id } 這裡舉 FIRST(E) 為例子，從 E 開始不斷往下找直到找到 Terminal，E -&gt; T -&gt; F -&gt; { (, id } Follow set The follow setof a nonterminal A is the set of terminals that can appear immediately to the right of Ain some sentential form, namely, S =&gt;* αAaβ, a is in the follow set of A. 如果對 Start symbol 尋找 FOLLOW(S) 要先加入 { $ } 如果存在 Production A -&gt; αB FOLLOW(B) 包含 FOLLOW(A) 如果存在 Production A -&gt; αBβ FOLLOW(B) 包含 FIRST(β) - {ε} 如果存在 Production A -&gt; αBβ，並且 FIRST(β) 包含 ε FOLLOW(B) 包含 { FIRST(β) - {ε} } U FOLLOW(A) 簡單來說 FOLLOW 就是找出一個 Nonterminal 所有可能的結尾 Terminal Example, Using previous grammar: FOLLOW(E) = { $ } U FIRST( ')' ) = { $, ) } FOLLOW(E') = FOLLOW(E) = { $, ) } FOLLOW(T) = { FIRST(E') – ε } U FOLLOW(E') U FOLLOW(E) = { +, $, ) } FOLLOW(T') = FOLLOW(T) = { +, $ , ) } FOLLOW(F) = { FIRST(T') – ε } U FOLLOW(T') = { *, +, $, ) } 這裡以 FOLLOW(F) 為例: T' -&gt; *FT' 跟 T -&gt; FT' 這兩處出現 F，但後面跟著的都是 T’，先做 T' -&gt; *FT' 因此 FIRST(β) = FIRST(T’) = { *, ε } 此處出現 ε，因此要依照規則 4 把 FOLLOW(T’) 加入 FOLLOW(F) 因此變成 { FIRST(T') - ε } U FOLLOW(T') = { *, +, $, ) } 4.5 Bottom-up Parsing Bottom Up Parsing 是從底層的 Leaf 開始，使用 Rightmost derivation 建構一顆到 Root 的 Parse tree Handles A handle β of a right-sentential form γ consists of a production A -&gt; β a position of γ where β can be replaced by A to produce the previous right-sentential form in a rightmost derivation of γ 非正式的講 handle 就是和某個 Production 能匹配的 Substring，對他化簡就代表反向的 Rightmost derivation The string ω to the right of the handle contains only terminals Ais the bottommost leftmostinterior node with all its children in the tree 如果有 S =&gt;* αAω =&gt; aβω，那麼緊跟在 a 之後的 Production A -&gt; β 就是 aβω 的一個 Handle，要注意 ω 一定只包含 Terminals， 如果 grammmr 是 Non-amibiguous，那麼 aβω 只會有一個 rightmost derivation，否則可能會有多個。 Handle pruning: 就是一個 Parse tree 識別 Handles 並將他們替換為 Nonterminal，到最後的過程 4.5.3 Shift-Reduce Parsing 因此 Bottom-Up Parsing 又稱為 Shift-Reduce Parsing，因為他們的過程就是不斷的 Shift 和 Reduce Shift: shift the next input symbolonto the top of the stack Reduce: replace the handle at the top of the stack with the corresponding nonterminal Accept: announce successful completion of the parsing Error: call an error recovery routine 例如之前的例子，其實就是一個不斷 Shift 和 Reduce 的過程: 4.6 LR(k) Parsing 4.6.2 Items and the LR(0) Automaton 4.6.4 SLR Parsing 目前流行的 Bottom-UP Parsing 都基於 LR(k) Parsers 的概念，幾乎可以支援所有 CFG，但是建立 Parser 很麻煩，因此通常會使用 Parser generator 來建立 Parser LR(k) Parsing: L 代表從左往右掃描輸入 R 代表以 rightmost derivation 進行推導 k 代表作出語法分析決策時的 Lookahead 輸入字元數 這裡會介紹三種 LR Parsing: SLR(1) Parsing LR(1) Parsing LALR(1) Parsing 從狀態數量來說 SLR &gt; LALR &gt; LR，但是能處理的 Grammar 來說 LR &gt; LALR &gt; SLR 4.6.2 Items and the LR(0) Automaton An LR(0) itemof a grammar in G is a production of G with a dotat some position of the right-hand side, A -&gt; α⋅β An LR(0) item represents a statein an NPDA indicating how much of a production we have seen at a given point in the parsing process NPDA means Non-deterministic Pushdown Automaton DPDA means Deterministic Pushdown Automaton 如果有一個 Production A -&gt; XYZ，那他將會有四個 LR(0) item: A -&gt; ⋅XYZ, A -&gt; X⋅YZ, A -&gt; XY⋅Z, A -&gt; XYZ⋅ 這個點代表 Parse 的進度，藉由這些 item 我們可以建立一個 NPDA，再透過演算法來轉換成 DPDA，這個 DPDA 就是 LR(0) Automaton 上圖左為 NPDA，右為 DPDA，透過以下演算法將 NPDA 轉換成 DPDA: functionitems(G'); begin C:= {closure({S' -&gt; •S})} repeat for each set of items I in C and each symbol X do J := goto(I, X) if J is not empty and not in C then C= C ∪ {J} until no more sets of items can be added to C return C end 為了建構以上演算法我們需要以下函數與 Augmented grammar(加強語法): Augmented grammar: 加入新的起始符號 S’，並且加入 Production S’ -&gt; S closure(I) adds more items to Iwhen there is a dot to the left of a nonterminal (corresponding to ε edges) goto(I, X) moves the dot past the symbol Xin all items in Ithat contain X (corresponding to non-ε edges) function closure(I); begin J := I; repeat for each item A -&gt; α•Bβ in J and each production B -&gt; γ in G such that B -&gt; •γ is not in J do J = J ∪ {B -&gt; •γ} until no more items can be added to J; return J end function goto(I, X); begin set J to the empty set for any item A -&gt; α•Xβ in I do add the item A -&gt; αX•β to J return closure(J) end 4.6.4 SLR Parsing 以下是 SLR 的演算法 procedure SLR(G'); begin for each state Iin items(G') do begin if A -&gt; α•aβ is in I and goto(I, a) = J for a terminal a then action[I, a] = \"shift J\" if A -&gt; α• in I and A != S' then action[I, a] = \"reduce A -&gt; α\" for all a in FOLLOW(A) if S' -&gt; S• in I then action[I, $] = \"accept\" if A -&gt; α• Xβ in I and goto(I, X) = J for a nonterminal X then goto[I, X] = J end all other entries in actionand gotoare made error end 以之前的 Grammar 為例，將其變為 Augmented grammar: 1. E' -&gt; E 2. E -&gt; E + T 3. E -&gt; T 4. T -&gt; T * F 5. T -&gt; F 6. F -&gt; (E) 7. F -&gt; id 透過演算法來找出所有的 Item: 之後將找到的 Item 填入表中: sn: 代表 shift 操作，與前往 State n rn: 代表 reduce 操作，並且使用編號 n 的 Production 填表時注意 In 中是否有 dot 走到最後，如果有就看是哪一個 Production，並填入 rn 如果 Production 是 S’ -&gt; S，那麼就填入 accept 要在什麼欄位填入 rn，就看 FOLLOW(A) 中有哪些 Terminal，就填入哪些 Terminal 這裡我們用一個測試輸入 id + id 來驗證這個表的正確性，注意做完 reduce 後狀態是看前一個 Stack 中的狀態， 再根據 Production 左邊的 Nonterminal 來進行 Goto，所以在 Step 7, 8, 9 是看 State 6 的 goto。 這裡簡單說明上表的範例，以 I2 為例子: 首先注意 E -&gt; T• 代表已經到達 Production 的最後，因此要進行 reduce FOLLOW(T) = { +, $ ,) }，因此在這三個欄位填入 r3，3 代表使用第三個 Production 然後這裡還有 E -&gt; E• * T 還沒處理，因此要進行 shift，這裡要看之前的狀態圖把 shift + 放入哪個狀態 這裡是放入 I7，因此 s7 在這個例子裡，沒有 Nonterminal 可以被寫入，因此不會填到 goto 欄位 另外說明驗證的過程，直接看到 step 6 7 8 9 Step 6: I6 看到 id，因此 s5 進入 I5 Step 7: I5 看到 $，r7 變成 F，下個狀態要看 I6 的 goto(F) 是 3 Step 8: I3 看到 $，r5 變成 T，下個狀態要看 I6 的 goto(T) 是 9 Step 9: I9 看到 $，r2 變成 E，下個狀態要看 I0 的 goto(E) 是 1 這裡要注意的是做完 reduce 要注意 stack 中還剩下那些狀態，使用那個狀態的 goto 在同一欄遇到多個 reduce 那就需要進行嘗試，直到出現 Error 或是 Accept 4.7 More Powerful LR Parsers 4.7.1 LR(1) Parsing Table 4.7.2 LALR(1) Parsing Table The Core of LR(1) Items SLR(1) 雖然很簡單，但會有很多 Grammar 不能處理，因此有了更強大的 LR(1) 和 LALR(1)。 LR(1) Parsing: 可以處理最多的 Grammar，但是他的狀態數量也是最多的 LALR(1) Parsing: 可以處理的 Grammar 接近 LR(1)，但是狀態數量基本跟 SLR(1) 一樣 4.7.1 LR(1) Parsing Table LR(1) Items An LR(1) item of a grammar in G is a pair, (A -&gt; α•β, a), of an LR(0) item A -&gt; α•β and a lookahead symbol a The lookahead has no effect in an LR(1) item of the form (A -&gt; α•β, a), where β is not ε An LR(1) item of the form (A -&gt; α•, a) calls for reduction by A -&gt; α only if the next input symbol is a 我們先來看 LR(1) 的演算法: closure(I) 跟 SLR(1) 不同的是，對於每個 B -&gt; γ，要找出所有可能的 b，b = FIRST(βa)，然後將 (B -&gt; •γ, b) 加入 J 中。 FIRST(βa) 其實就等於先看下一個 Nonterminal 的 FIRST。 function closure(I); begin J := I; repeat foreach item (A -&gt; α•Bβ, a) in J and each production B -&gt; γ of G that each b in FIRST(βa) such that (B -&gt; •γ, b) is not in J do J := J ∪ {(B -&gt; •γ, b)} until no more items can be added to J; return J end goto(I, X) 其實跟 SLR(1) 一樣，只是 item 多了 lookahead(a) function goto(I, X); begin set J to the empty set for any item (A -&gt; α•Xβ, a) in I do add the item (A -&gt; αX•β, a) to J return closure(J) end 下面這是完整的 LR(1) 演算法: function items(G'); begin C := {closure({[S' -&gt; •S, $]})} repeat for each set of items I in C and each symbol X do J := goto(I, X) if J is not empty and not in C then C := C ∪ {J} until no more sets of items can be added to C return C end 整個 Closure 的算法就是: (A -&gt; α•Bβ, a) 先找出所有的 B -&gt; •γ 計算 b = FIRST(βa) B 後面的符號加上 a 的 FIRST 之前找出的 B -&gt; •γ 全部都加上 b 作為 lookahead Example: 1. S' -&gt; S 2. S -&gt; CC 3. C -&gt; cC 4. C -&gt; d 首先看 closure({(S’ -&gt; •S, $)}): 首先計算 (S’ -&gt; •S, $) 以 (A -&gt; α•Bβ, a) 來看會等於 A = S’, α = ε, B = S, β = ε, a = $ 加入 B -&gt; •γ，也就是 S -&gt; •CC 計算 b = FIRST(βa) = FIRST(ε$) = { $ } 加入 (S -&gt; •CC, $) 計算 (S -&gt; •CC, $) 以 (A -&gt; α•Bβ, a) 來看會等於 A = S, α = ε, B = C, β = C, a = $ 加入 B -&gt; •γ，也就是 C -&gt; •cC, C -&gt; •d 計算 b = FIRST(βa) = FIRST(C$) = { c, d } 加入 (C -&gt; •cC, c), (C -&gt; •cC, d), (C -&gt; •d, c), (C -&gt; •d, d) 簡化為 (C -&gt; •cC, c/d), (C -&gt; •d, c/d) 已經沒有 Item 可以加入，所以返回 I0 goto(I0, S): (S’ -&gt; S•, $) goto(I0, C): (S -&gt; C•C, $) 以 (A -&gt; α•Bβ, a) 來看會等於 A = S, α = C, B = C, β = ε, a = $ 加入 B -&gt; •γ，也就是 C -&gt; •cC, C -&gt; •d 計算 b = FIRST(βa) = FIRST(ε$) = { $ } 加入 (C -&gt; •cC, $), (C -&gt; •d, $) goto(I0, c) (C -&gt; c•C, c/d) 以 (A -&gt; α•Bβ, a) 來看會等於 A = C, α = c, B = C, β = ε, a = c/d 加入 B -&gt; •γ，也就是 C -&gt; •cC, C -&gt; •d 計算 b = FIRST(βa) = FIRST(εc/d) = { c/d } 加入 (C -&gt; •cC, c/d), (C -&gt; •d, c/d) 之後就依此類推，直到沒有新的 Item 可以加入，最後就會得到以下的 LR(1) Parsing Table: 這張表的寫法基本上跟 SLR(1) 一樣，只是 reduce 的部分變成要看 lookahead(a) 是什麼，而不是 FOLLOW(A) 4.7.2 LALR(1) Parsing Table The Core of LR(1) Items LALR 透過合併 LR 的狀態來減少狀態數量 將 LR(1) 中 core 相同的狀態合併 procedure LALR(G'); begin for each state I in mergeCore(items(G')) do begin if (A -&gt; α•aβ, b) in I and goto(I, a) = J for a terminal a then action[I, a] = \"shift J\" if (A -&gt; α•, a) in I and A != S' then action[I, a] = \"reduce A -&gt; α\" if (S' -&gt; S•, $) in I then action[I, $] = \"accept\" if (A -&gt; α•Xβ, b) in I and goto(I, X) = J for a nonterminal X then goto[I, X] = J end all other entries in actionand gotoare made error end 以之前的例子為例，將 Core 相同的狀態合併: I3 和 I6 合併就寫作 I36 在合併的同時 Lookahead 也要合併，例如 I47 I4 的 Lookahead 是 c/d，I7 的 Lookahead 是 $，合併後 Lookahead 就是 c/d/$ 這裡可以發現其實直接從 LR(1) 的表格轉換成 LALR(1) 的表格是很簡單的，找到相同的 core 就可以去按照表格合併 4.8 Using Ambiguous Grammars 要注意所有的 Ambiguous Grammar 都不是 LR(1) 的，LR(1) 的 Parsing Table 不會產生 multiply-defined entries A grammar is SLR(1) iff its SLR(1) parsing table has no multiply-defined entries A grammar is LR(1) iff its LR(1) parsing table has no multiply-defined entries A grammar is LALR(1) iff its LALR(1) parsing table has no multiply-defined entries 一個 Grammar 如果產生 SLR/LR/LALR Parsing Table 並且沒有 multiply-defined entries，那麼就可以稱這個 Grammar 是 SLR/LR/LALR grammar， multiply-defined entries 也可以稱為 conflict(衝突)，例如: shift-reduce conflict: 一個狀態同時有 shift 和 reduce 的動作 reduce-reduce conflict: 一個狀態同時有兩個 reduce 的動作 一個 grammar 可以是 LALR(1) grammar 但不是 SLR(1) grammar，例如下面的題目: Exercise 4.7.4: Show that the following grammar S -&gt; Aa | bAc | dc | bda A -&gt; a is LALR(1) but not SLR(1). 先求出 FLLOW Set: FLLOW(S) = { $ } FLLOW(A) = { a, c } 將其轉換成 LR(1) 與 SLR(1) 的 DPDA Graph(Left LR(1); Right SLR(1)): 上圖是 LR(1)/LALR(1) 的 Parsing Table 與 Graph，並沒有 conflict 產生 因為沒有可以合併的 core，所以最後 LALR(1) 將會與 LR(1) 相同，並且 LR(1) 並不會產生 multiply-defined entries， 所以 LALR(1) 也不會產生 multiply-defined entries，因此這個 Grammar 是 LALR(1)的。 上圖是 SLR(1) 的 Graph 但是我們在 SLR(1) 的 I7 可以看到當下一個輸入為 a 時，因為 FLLOW(A) = { a, c }，此時會產生要使用 A -&gt; d 進行 reduce， 還是要 shift a 進入 I10 的 conflict，因為有 conflict 產生，所以這個 Grammar 不是 SLR(1) 的。 4.8.1 Hierarchy of Grammar Classes 如果把 Parsing 能處理的 Grammar 範圍畫成圖就會類似下圖: 如果一個 grammar 以及是 ambiguous 那麼勿論什麼 parsing 都不能處理這個 grammar Top-Down: LL(k) 相較於 LR(k) 能處理的 grammar 少，但沒辦法和 SLR, LALR 做比較 Bottom-Up: 能處理的 grammar 從多到少依序為，LR(k) &gt; LALR(k) &gt; SLR(1) 接下來就依序解釋為什麼會有這樣的差異 LL(k) vs. LR(k) For a grammar to be LL(k), we must be able to recognize the use of a production by seeing only the first k symbols of what its right-hand side derives For a grammar to be LR(k), we must be able to recognize the use of a production by having seen all of what is derived from its right-hand side with k more symbols of lookahead 簡單的說法就是，LR(k) 是 rightmost 推導，所以結合 lookahead 可以更好的知道該執行 shift/reduce LALR(k) vs. LR(k) The merge of the sets of LR(1) items having the same core does not introduce shift/reduce conflicts Suppose there is a shift-reduce conflict on lookahead a in the merged set because of (A -&gt; α•, a) (B -&gt; β•a γ, b) Then some set of items has item (A -&gt; α•, a) , and since the cores of all sets merged are the same, it must have an item (B -&gt; β•a γ, b) for some c But then this set has the same shift/reduce conflict on a 如果一個 LR(1) 沒有 shift/reduce conflict 那麼由合併的 items 也必然沒有 shift/reduce conflict， 但如果本來 LR(1) 就有 shift/reduce conflict 那麼合併後會繼承已有 shift/reduce conflict The merge of the sets of LR(1) items having the same core may introduce reduce/reduce conflicts As an example, consider the grammar S’ -&gt; S S -&gt; a A d | a B e | b A e | b B d A -&gt; c B -&gt; c that generates acd, ace, bce, bcd The set {(A -&gt; c•, d), (B -&gt; c•, e)} is valid for acx The set {(A -&gt; c•, e), (B -&gt; c•, d)} is valid for bcx But the union {(A -&gt; c•, d/e), (B -&gt; c•, d/e)} generates a reduce/reduce conflict 但合併 items 可能會產生 reduce/reduce conflict，因為合併後也會合併 lookahead，這樣有可能使原本沒有 conflict 的 items 產生 conflict， 因此產生 reduce/reduce conflict。 SLR(k) vs. LALR(k) SLR(k) 與 LALR(k) 我們用一個例子來說明: As an example, consider the grammar S’ -&gt; S S -&gt; L = R S -&gt; R L -&gt; * R L -&gt; id R -&gt; L FOLLOW(R) = { =, $ } 先看 SLR(1) 的 Items set，因為是使用 FOLLOW(R) 來進行 reduce，所以在 I2 產生 shift/reduce conflict， 在 I2 可以 shift =，也可以使用 R -&gt; L 進行 reduce，因為 FOLLOW(R) = { =, $ }。 在 LALR(1) 因為從 LR(1) 合併而來，因此有 lookahead 的存在，所以即使在 core 相同的時候，也可以透過 lookahead 來區分是否要進行 reduce。 Last Edit 11-27-2023 16:19 還有一個部分是使用 bison 來實作 Syntax analysis，但這個篇章已經很長了，所以我會放在另一篇講述如何實作"
  },"/jekyll/2023-10-19-cpu_scheduler.html": {
    "title": "OS | CPU Scheduler",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-10-19-cpu_scheduler.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 本章節會主要介紹 Linux Scheduler，Linux Scheduler 現在的的目標是: 如何從「好變為更好」 Noun, Concept definition 2.4 Scheduler 2.6 O(1) Scheduler 2.6 - 5.3 Complete fair scheduler(CFS) Linux Kernel 2.4 是一個非常長壽的版本，持續了大約 10 年左右，但即便這樣一個這麼長壽、穩定的 Scheduler， Linux Kernel 設計者仍然在考慮如何讓她變得更好。 Noun, Concept definition 4.1 Task 4.2 Scheduler Types 4.3 Cooperative multitasking - Novell-Netware 4.4 Preemptable OS 4.5 Scheduler &amp; Context switch 4.6 Scheduling Criteria 4.1 Task 在 Linux 中，Process 和 Thread 都是 Task Process 之間不會共用任何資源，尤其是 Memory Thread 則是幾乎共用所有資源，尤其是 Memory Task 的生命週期中分為兩種情況 Using CPU Waiting，例如: Waiting mutex, I/O … Task 在使用 CPU 時分為: 執行於 User mode/Kernel mode 在 Linux Task 可以執行在 User/Kernel mode，改變模式稱作 Mode change，而 Kernel Thread 專指只有 Kernel mode 的 Task，例如: Device Driver Task &amp; Scheduling 以下是一個 Task 的生命週期，這裡從 Scheduler 角度來看的話主要影響的是兩個部分: Waining(semaphore): 怎麼在 Waiting 時，讓 Task 的使用率最大化 OS 可以分成兩大類: Cooperative multitasking(Non-preemptive, 協同運作式多工) Preemptable OS(Preemptive, 搶占式多任務處理) 4.2 Scheduler Types Non-preemptive OS(△): 就是指只有 Task 自己放棄 CPU 使用權，才會交出 CPU 使用權 Task 執行結束，這樣當然就交出 CPU 使用權 Task 發出 Blocking I/O request 因為要等待 I/O 完成，因此也會交出 CPU 使用權 當然也有 Async I/O，Vectored I/O 等方法，這裡先不討論 Preemptive OS(☐△): 每個 Task 會有一個 Time slice 執行，例如: 1/1000 Sec，執行結束就要做切換 從 Wating 等待完畢 I/O 後，返回 Runtable 時要不要馬上切回該 Task 新的 Task 也可以給予高優先權，讓他馬上執行 Preemptive OS 又分為: Preemptable Kernel, Non-preemptable Kernel 4.3 Cooperative multitasking - Novell-Netware Netware: Netware 所有的程式都在 ring 0 執行，但是這樣就要確保所有的程式都是由 Novell 來控制， 這樣才能確保所有的程式都是可信任的，沒有惡意程式 但是當 CPU 效能變強之後，這樣的設計就不太好，因為所有的程式都要由 Novell 來提供，這不太可能，因此最後由 Windows NT 勝出 4.4 Preemptable OS Preemptive OS: 如果一個 Task 執行過久，OS 會主動將 CPU 控制權交給下一個 Task 如果要設計 Preemptive OS 必須要有 Hardware support，例如: Timer, Interrupt Timer: 用來計算 Task 執行的時間 Interrupt: 使 OS 能獲得 CPU 控制權 所有版本的 Linux 都是 Preemptive OS Preemptive Kernel Non-preemptive Kernel(throughput): 在 2.6 Kernel 之前，Linux 是 Non-preemptable Kernel 當 Task 執行在 Kernel mode 時，其優先權無限大 Context switch 只會發生在 Task 由 Kernel mode 切換到 User mode 時 Preemptive Kernel(latency): 在 2.6 Kernel 之後，Linux 可以設定為 Preemptable Kernel 當 Task 執行在 Kernel mode 時如果沒有任何的 Lock 就可能發生 Context switch 注意如果在 Kernel 中發生 Interrupt，下次 Task A 執行時會直接從 Kernel mode 中繼續執行， 因此在 2.6 Kernel 後編譯時可以選擇 Kernel 注重 throughput 還是 latency， Form Non-preemprive kernel to Preemprive kernel 在 Non-Preemprive kernel 中，所有進入 Kernel 的 Task 可以假設自己不會被 Preempt，因此存取很多共用資料，不需要使用 Lock 在 Preemprive kernel 中，程式設計師需要仔細思考、改寫程式碼，所有存取到共用資料的程式碼都需要使用 Lock-UnLock 來保護 這是一件非常耗費人力的工作 4.5 Scheduler &amp; Context switch Scheduler 決定接下來要執行哪一個 Task 使用 C language 撰寫 Context switch 負責從一個 Task 切換到另一個 Task 主要切換的是普通 Register 如果 Task 使用到一些特別的 Register，例如: 浮點數運算器(Floating Point Unit, FPU)，則需要額外處理(Lazy) Lazy: 只有新的 Task 需要使用到 FPU 時，才會切換 FPU 相關的 Register Context switch 隱含的切換 依照需求切換 Page Table(TLB) 切換 Cache 的內容 4.6 Scheduling Criteria 這裡介紹如何分析一個 Scheduler 的好壞 CPU Utilization(使用率): CPU 維持在高使用率，Task 之間互相有等待的關係，要如何 Schedule? Throughput(吞吐量): 在單位時間內，CPU 可以執行多少 Task 例如: 讓 I/O Task 優先執行 Turnaround time(往返時間): Task 從開始到結束的時間，與 Scheduler 及程式本身的執行程度相關 Waiting time(等待時間): Task 在 Ready Queue 中等待的時間，通常高優先權的 Task 會等待較短的時間 只要一個 Task 能執行，但 OS 使其等待就要算入 Waiting time Response time(回應時間): Task 從發出 Request 到第一次回應的時間 例如: 程式需要輸出回應到螢幕，好的 Scheduler 可以讓 Progress bar 非常即時的反應 CPU Utilization 實際的 CPU 使用率會受到 Task 的高低優先權影響，因此 CPU 使用率會有兩種情況: 可以透過軟體的方式對 Lock-Unlock 做最佳化，例如: Intel vtune, kernelshark 之類的視覺化工具 Response time 假如系統中有 10 個 Task，這 10 個 Task 連接到 10 個 User，如果要讓執行感覺流暢的話，有以下做法: 單向互動: 假如 Task 是播放影片，那就安排適當的 Buffer，解碼後的影片放入 Buffer 供 User task 拿取 只要 Buffer 夠大，即使每 10 秒才輪到一次執行，使用者也不會覺得 Lag 雙向互動: 假如 Task 是語音通話，必須在 150ms 中輪到執行一次，否則會覺得通話品質不好 那每個 Task 一回合只能執行 15ms Scheduler Concepts 4.7 Simple Scheduler FCFS(Fisrt Come First Serve) 依照 Task 的抵達順序，依照順序執行 SJF(Shortest Job First) 依照 Task 的執行時間，執行時間短的 Task 優先執行 RR(Round Robin) 在一群 Task 中輪流執行，每一個 Task 最多執行 X 個 Time slice 在 Linux 中可以看到上面三種方法的影子 FCFS Wating time: P1 = 0, P2 = 100, P3 = 150 Average waiting time: (0 + 100 + 150) / 3 = 83.3 如果先抵達的 Task 執行時間很長，Average waiting time 就會變得比較長 SJF 在 P1 執行中，P2、P3 抵達，放入 Ready Queue，然後依照執行時間排序 Wating time: P1 = 0, P2 = 110, P3 = 100 Average waiting time: (0 + 110 + 100) / 3 = 70 Preemptive SJF 只要有工作進入 Ready Queue，或有工作結束就要決定執行的 Task Wating time: P1 = 60, P2 = 10, P3 = 0 Average waiting time: (60 + 10 + 0) / 3 = 23.3 直觀上會覺得 Preemptive SJF 是比較好的演算法，但注意 Preemptive SJF 有 Context switch overhead 如果只是寫 SJF 通常指 Non-preemptive SJF Preemptive SJF 又稱作 SRTF(Shortest Remaining Time First) 在 Average waiting time 方面，SJF &amp; SRTF 分別是 Preemptable scheduling &amp; Non-preemptable scheduling 的最佳演算法 Estimate Execution Time 前面提到的 Scheduler 都是假設 Task 的執行時間是已知的，但實際上 Task 的執行時間是不知道的，因此需要估計 Task 的執行時間 一個 Task 的生命週期分為兩種情況 Using CPU, Waiting 依照 Task 以前使用 CPU 使用時間的多寡，來預測這次使用 CPU 時間的多寡，例如以下公式 tn: 上一次的 CPU time 𝜏x: 預測的第 x 次的 CPU time Linux Kernel 2.4 使用以下的方法，並且 𝛼 取 1/2 因為可以避免浮點數運算 tn+1 = 𝛼 * tn + (1 - 𝛼) * 𝜏n 𝛼 是一個權重因子界於 0 ~ 1 之間 越靠近 1，表示越重視過去的 CPU time 越靠近 0，表示越重視預測的 CPU time Round Robin 如果使用 RR 那麼 Time slice 要設定為多少? 太長: 會讓 Task 等待的時間變長 太短: 會讓 Context switch overhead 變大 通常 Time slice 會設定為一個 Task 可以在 Time slice 內執行完畢，變成 Waiting 狀態 也就是在 Time slice 中成功把 I/O request 發出去 目前 Linux 的設定為使用者需要多少的 Time slice，可以動態的調整 Time slice 的大小 Linux Scheduler 4.8 Linux 2.4 Scheduler 4.9 Linux 2.6 Scheduler Linux 共有 140 個優先權等級 0 ~ 99: Real-time priority 通常是一些需要 Real-time 的 Task，例如: 影片播放，聲音播放 100 ~ 139: Normal priority 對使用者而言是 -20 ~ +19，預設值為 0，稱作 Nice value Nice value 是由 User 指定，Linux 當作參考用以計算 Dynamic priority，Dynamic priority 會因以下因素影響: 該 Task 是 I/O bound 還是 CPU bound 考慮 Core 的特性 考慮 Multi-thread 的特性 使用者在啟動 Task 時可以指定 Nice value，或在 Task 執行時使用 renice 指令來調整 Nice value 4.8 Linux 2.4 Scheduler 在 2.4 Scheduler 中如何對 I/O 進行優化 思考在 2.4 在 Multi-processor 的環境下欠缺什麼? Non-preemptible kernel Set p-&gt;need_resched if schedule() should be invoked at the ‘next opportunity’(kernel -&gt; user mode). 所以一個正在 Kernel 中運行的 Task 要進行 Context switch 時就會將 need_resched 設為 1 Round-Robin task_struct-&gt;counter: number of clock ticks left to run in this scheduling slice, decremented by a timer. 這是一個 Task 執行的 counter，每個 time tick 就 -1，用完了就不能在這個回合內使用 CPU 2.4 Scheduler - SMP: 當有 CPU 進入 Idle 時，2.4 Scheduler 會從 Ready Queue 中 Search &amp; Estimate，找出最佳的 Task 來執行 Search: 會依照這個 Task 對這個 CPU 有多適合 2.4 Scheduler - Run-Queue: 在 2.4 中所有人都是使用同一個 Run-Queue: Use spin_lock_irq() to lock “runqueue_lock” 因為 2.4 Scheduler 僅有一個 Run-Queue，要在運算時 Lock 然後運算完 Unlock，自然會造成效能瓶頸 Check if a task is “runnable” in TASK_RUNNING state in TASK_INTERRUPTIBLE state and a signal is pending Examine the “goodness” of each process 檢查所有 Task 的 Goodness，並且選出最好的 Task Context switch 2.4 Scheduler - Goodness: “goodness”: identifying the best candidate among all processes in the runqueue list. “goodness” = 0: the entity has exhausted its quantum. 0 &lt; “goodness” &lt; 1000: the entity is a conventional process/thread that has not exhausted its quantum; a higher value denotes a higher level of goodness. if (p-&gt;mm == prev-&gt;mm) return p-&gt;counter + p-&gt;priority + 1; else return p-&gt;counter + p-&gt;priority; A small bonus is given to the task p if it shares the address space with the previous task. Examine the processor field of the processes and gives a consistent bonus (that is PROC_CHANGE_PENALTY, usually 15) to the process that was last executed on the ‘this_cpu’ CPU. 例如: 一個 Multi-thread 程式的 Task，如果有一個 Thread 執行在一個 CPU 上，那麼其他的 Thread 就會有一個加分，讓他們可以在同一個 CPU 上執行。 同樣的如果一個 Process 最後是在這 this_cpu 上運行，那麼他在這顆 CPU 上計算分數時也會有獎勵加分。 2.4 Scheduler 的問題是 Scheduler 要對所有的 Task 計算 goodness，每次都要重算。但其實大多數時間每次計算出的 goodness 都是差不多的， 真的有需要每次都重算嗎? Linux 2.4 Scheduler - Improve I/O performance Defintion: I/O-bound processes: spends much of its time submitting and waiting on I/O requests Processor-bound processes: spend much of their time executing code Linux 傾向於支援 I/O-bound processes，這樣會提供好的 Process response time，但是怎麼對 Process 進行分類? 將 Run time 分為無數個 epoch 當沒有 task 可以執行時就換到下一個 epoch 此時可能有些 task 的 time slice 還沒用完，但這些 task 正在 waiting 2.4 Scheduler 假設所有的 waiting 就是在 waiting I/O 進入下一個 epoch 的時候，補充所有 task 的 time slice 如果是 I/O-bound task，因為在上一個 epoch 在 waiting I/O，還有一些 time slice 沒用完， 因此補充後這些 task 會有較多的 time slice 在 Linux 2.4 中，time slice 就是 dynamic priority 因此 I/O-bound task 會有較高的 dynamic priority 從上面的圖來看: Epoch1: CPU bound 都已經用完 time slice，此時剩下 I/O bound slice，必須進入下一個 epoch 否則會進入 idle Epoch2: 使用 timeSlicenew = timeSliceold / 2 + baseTimeSlicep[nice] 的公式來補充 time slice 依照這樣的運算 Epoch2 的 I/O bound task 一定會比 CPU bound 有更高的 Priority 注意 Kernel 中不會使用 FPU，因此不會有 float point Cauclate time Slice timeSlicenew = timeSliceold / 2 + baseTimeSlicep[nice] 為什麼要除以 2，假如有一個惡意的程式如下: int main() { sleep(65535); while(1) ; } 每次拿到 CPU time 就去 sleep，因此在 sleep 中會被視為一個 I/O bound task，因此拿到很高的 time slice， 這樣醒來時就是一個 CPU bound task 同時也有很高的 time slice，可以搶佔 CPU 造成其他的 I/O bound task 也無法獲取 CPU time。 Main disadvantages of 2.4 Scheduler 計算 goodness 太耗費時間，就算某個 Task goodness 一直沒變，每次還是要重新計算 所有 CPU 共用同一個 Run queue，這個 Run queue 會變成系統的效能瓶頸，因為每次都要 Lock &amp; Unlock Wating 不一定是 I/O，例如: sleep() 在 2.4 Scheduler 中只針對 I/O 做提高優先權 例如 waiting child process 也是一種 waiting，也可以被考慮在內 4.9 Linux 2.6 Scheduler O(1) Scheduler CFS(Complete Fair Scheduler) 2.6 Scheduler Architecture 2.6 Scheduler 首先在架構的改善就是使每一顆 CPU 有自己的 Run queue 即使這樣 CPU 要去 Run queue 拿資料時也要做 Lock &amp; Unlock 因為是 Lock 自己的 Run queue，因此 Lock &amp; Unlock 通常都會成功，不會有競爭的情況發生 當自己有自己的 Run queue 後要考慮的就是 Load balancing(負載平衡) 系統去檢查 Run queue 是否 Loading 過重，如果是就會將 Task 搬移到另一個 Run queue 因此才需要 Lock &amp; Unlock，是為了避免 CPU 在搬移 Task 時出現錯誤 Put: 當 CPU 覺得自己的 loading 太重，將 task 塞給另一顆 CPU Pull: 覺得自己的 loading 太輕，從別的 CPU 拉 task 過來 如何評估 Loading 輕重 比較簡單的方式，查看每個 CPU 的 Task 數量跟 runnable task 數量 每一顆 CPU 上都會有一個 thread 來觀察是否要做 Load balancing，這個 thread 稱作 Balance thread 但假如 A, B 兩顆 CPU 同時要搬移 Task 要給予對方，同時鎖定對方的 Run queue，就會造成互相等待，造成 Deadlock，這部分後面會說明 CPU Affinity 由於每一顆 CPU 都有自己的 Run queue，通常除非 Loading unbalance，否則不會去觸發 Task migration 因此 2.6 Scheduler 可以更有效的使用 Cache Fully Preemptible Kernel 2.6 Kernel 之後，Linux 中每一個 Task 執行於 Kernel mode 時會有一個變數 preempt_count，用於記錄該 Task 是否可以被 Preempt 每當 Lock 一個 Resource 時，preempt_count++ 每當 Unlock 一個 Resource 時，preempt_count-- 如果 preempt_count == 0，Kernel 可以做 Context switch Kernel 要做 Context switch 通常是因為 interrupt，例如: 一個高優先權的 task 正在等這個 interrupt 每次 preempt_count 從 1 變為 0，Kernel 都會檢查一下是否要 Context switch 如果 Kernel 直接執行 schedule()，無論 preempt_count 是多少，都會做 Context switch schedule() 是在 Linux kernel 中的重要函數，會直接進行 scheduler 調度，並且切換到下一個 Task 執行 延伸閱讀: Linux kernel: schedule() function O(1) &amp; CFS scheduler 4.10 O(1) Scheduler 4.11 CFS Scheduler 2.5 ~ 2.6.22: O(1) Scheduler Time complexity: O(1) Using Run queue(an active Q and an expired Q) to realize the ready queue 2.6.23 ~ : CFS Scheduler Time complexity: O(log N) the ready queue is implemented as a red-black tree 4.10 O(1) Scheduler 每顆 CPU 有自己的 Run queue，每個 Run queue 由兩個 Array 組成 active array: time quantum 還沒用完的 task expired array: time quantum 用完的 task Time complexity: O(1) 能在 O(1) 的時間內 access, search, insert, delete 每次用完 time quantum 的 task 被移到 expired array 並在此時計算下一回合的 Dynamic priority 選出最高 Priority 的 Task，就使用求 min 的演算法 對一個 array 求 min 的演算法最佳 time complexity 為使用 heap 建立資料結構，time complexity 為 O(log N)， 但是因為 Linux 的優先權只有 140 種，因此可以使用一些方法來優化到 O(1) struct prio_array { unsigned int nr_active; unsigned long bitmap[BITMAP_SIZE]; // BITMAP_SIZE = 140 struct list_head queue[MAX_PRIO]; // MAX_PRIO = 140 }; typedef struct prio_array prio_array_t; struct runqueue { /* ... */ struct mm_struct *prev_mm; // prev task's mm_struct prio_array_t *active, *expired; prio_array_t arrays[2]; /* ... */ }; prio_array nr_active: 紀錄 active array 中有多少 task bitmap: 用來快速查詢至少有一個 task 的 priority queue: 用來存放相同 priority 的 task runqueue 中維護了兩個 prio_array，分別是 active, expired *prev_mm: 如果 task 是同一個程式的 thread 那麼 mm_struct 指向的位置會是一樣，這樣就可以不用做 Memory context switch 延伸閱讀: Linux 核心設計: O(1) Scheduler, Linux 核心設計: 不只挑選任務的排程器: O(1) Scheduler 在這兩個 Queue 中每個 Task 可以拿到的 Time quantum 大約等於 1 / priority 在 Linux 中 Priority 高有兩個好處: 有較高的 Time quantum(Time slice) 可以更快的搶到 CPU 在 Active queue 中較高優先權的 task 除非放棄或是 time quantum 用完，否則後面的 task 都不會執行 在同一個 priority 中，會依照 Round-Robin 的方式來輪流執行 如果進行 I/O bound 也就是放棄，那麼就會被移到 expired queue，並且在此時計算下一回合的 Dynamic priority 如果是 I/O bound 那就會獲得比較高的 Priority 等到 Active queue 中的所有的 task 都被移到 expired queue 後，就會將兩個 queue 交換 4.10 O(1) Scheduler - bitmap 在 bitmap 中每個 bit 代表一個 priority，如果為 1 表示至少有一個 task Insert, Delete 的演算法如下: Y = priority / 32, X = priority % 32 例如: 編號 9 的 Priority，9 / 32 = 0, 9 % 32 = 9, 即可存取 bitmap[0][9] 設定為 0 或 1 Min(尋找最高優先權的 Task): 從 0 開始找顯然要 O(N) 的時間，不是 O(1) 有硬體支援的話就能直接使用一個 Function ffs() 就能做到 O(1) 在 include/asm-generic/bitops 中有一系列 ffs() 的實作 Disadvantages of O(1) Scheduler 跟 2.4 Scheduler 一樣，使用 Epoch 來區分 I/O bound &amp; CPU bound 因此每個 Task 都要再使用完 Time slice 以後，經過一個 Epoch 才能獲得更多的 Time slice 對於某些需要更頻繁的獲取 CPU time 的 Task 來說，無論 Priority 多高都要等待一個 Epoch 才能獲得更多的 Time slice 例如: 遊戲、多媒體 延伸閱讀: 谈谈调度 - Linux O(1) 4.11 CFS Scheduler CFS source code 目前存在於: linux/kernel/sched/fair.c CFS (Completely Fair Scheduler) 在 2.6.23 之後取代 O(1) Scheduler，但是 O(1) Scheduler 獨特的設計與簡單的算法， 影響了很多系統的設計。CFS 雖然在性能上比 O(1) Scheduler 差，但是在公平性上比 O(1) Scheduler 好。 CFS 獨特的地方在於回填 Time quantum 相較於前面兩種 Scheduler，Priority 高的 Task 回填速度會更快 因此高 Priority 的 Task 會有更多的 Time slice，更好的 Response time Design Concept 將一顆 Physical CPU 依照目前正在執行的 Task 分成多個 Virtual CPU 假如這些 Task 的 Priority 都一樣，那麼每個 Virtual CPU 的效能為 Physical CPU 效能的 1 / N 這表示如果 Task 的優先權越低，那麼他的 Time slice 就會越小 但是每次的執行時間也有下限，不可能依照 Task 的數量無限制的分割 1 / N，所以會有一個臨界值 λ λ = 「希望達到的反應時間」/「# of task」 這個 λ 是可以由使用者設定的 前兩個 Scheduler 都是等到所有 Ready queue 裡面的 Task 都用完 Time slice，Scheduler 才會去計算下一回合的 Time slice， 稱作 Epoch。 藉由 Epoch 可以看 Task 在上一個 Epoch 的行為來判斷他是 I/O bound 還是 CPU bound 但是在 CFS 中，是依照 waiting time 來決定執行順序，waiting time 越長的 Task 優先執行 CFS Architecture 這裡使用 rbtree 來實作 Ready queue，依照 Task 的 vruntime 來排序 vruntime 表示的是一個 task 真正在 CPU 上的執行時間 vruntime 越小表示 Task 在 CPU 上執行的時間越少，因此從公平的角度來看優先權越高 每次執行就取出 rbtree 中最左邊的 Task 執行 執行完畢後就計加上 delta_exec，然後重新放回紅黑樹中因此 Time Complexity 為 Θ(log N) 這樣可以確保每個 Task 都有機會在 rbtree 的最左邊，也就是最優先執行的位置 延伸閱讀: Linux CFS 调度器：原理、设计与内核实现（2023） delta_exec 如何計算的 source code 目前存在於: linux/kernel/sched/fair.c 中的 __calc_delta() Virtual Time 在之前的 Scheduler，Time slice 是不固定的，優先權越高的 Task Time slice 越長，但是在 CFS 中，Time slice 是固定的， 這個 Time slice 是依照系統希望的 Response time 來計算的。 例如下面的例子，如果將 CPU 模擬為 3 個 CPU，分別為 1/2(藍色), 1/4, 1/4 的效能，那麼每次當藍色的 Task 執行完畢後， 計算出的 vrutime 會比 1/4 的還要小，因此在同一個時間單位內，藍色的 Task 會執行更多次。 CFS - I/O 如果有從 Waiting queue 回來的 I/O Task 怎麼把他放到 rbtree 最左邊 將他設定為最小的 vruntime 這樣就能強制 Scheduler 馬上進行 Context switch 執行 I/O Task min_vruntime: CFS 會去維護一個 min_vruntime，表示目前 rbtree 中最小的 vruntime min_vruntime - Δ 設定為從 waiting queue 回來的 Task 的 vruntime 這樣就能馬上執行 也因為這樣的設計，假如有一個 CPU bound Task 在這樣的設計下即使 Priority 最高 -19，也會被搶走 CPU time。 CFS - New Task 將新進入系統的 Task 都設為 min_vruntime 插入到 rbtree 的最左邊，但是如果有一個這樣的程式: while(1) { fork(); } 在 Linux 的解決方法是將剩餘的 CPU time 平均分配給 child, parent，另外也可以設定 ulimit 來限制一個 process 可以 fork 的次數， 超過這個次數就可以認為他是一個惡意的程式。 延伸閱讀: Fork bomb Scheduler Problem 對於現在的 Linux Scheduler 來說還有什麼需求沒有被滿足: 對於 Real-time 的支援 目前最主流的擴充套件是 RTAI 已經可以被使用在加工上 LinuxCNC 對於 Power management 的支援(Power saving) 動態調整 CPU clock rate, voltage 讓 CPU 能進入省電模式，例如: ACPI 定義的 C0, C1, C2 …，Advanced Configuration and Power Interface 對於 BigLittle 等新的 CPU Architecture 的支援 優先權是否可以和 Time slice 拆開，以實現更好的 QoS(Quality of Service) 例如: 實現一個 system call 叫做 balance，可以調整 time slice 和 priority 的比例 Last Edit 12-02-2023 16:03"
  },"/jekyll/2023-10-18-process_thread.html": {
    "title": "OS | Process and Thread",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-10-18-process_thread.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 這章節主要是介紹 Process 跟 Thread Process model Process Life Cycle Communication model between Process &amp; Process Communication method between Process &amp; Process Producer-consumer problem Context switch main overhead (使用 Thread 的動機) Thread model Process model 3.1 Process Concept An OS executes a variety of programs: Batch system – jobs Time-shared systems – user programs or tasks Process ≈ Task ≈ Job Process - a process is an instance of a program in execution +Program code (text section) +Program counter &amp; registers (CPU status) +Stack +Data section Batch system(批次系統): 是指被時間安排在 PC 上運行，不需要與使用者互動的工作 3.2 Process Memory 每個 Process 通常有自己完整的 Address space 32bit 為例，每個 Process 有 4GB 的 Address(Memory) space 64bit 的 x64 CPU，因為成本的考量，通常只會使用 48bit 的 Address space，也就是 256TB (遠超 Disk 的容量) Address space 表示一個 Process 最多能使用多少 Memory，實際上 RAM 通常遠小於 Process 的 Address space 通常將 Process 的 Address space 分為兩個部分，上半部分為 OS Kernel，下半部分為 Process 的 User space 以 64bit 為例，一個 Process 的 Memory address(User space) 為 0~128TB，Kernel 則為 256TB(264) 往下 128 TB 的部分 User space: 00000000 00000000 ~ 00007FFF FFFFFFFF Kernel: FFFF8000 00000000 ~ FFFFFFFF FFFFFFFF Why kernel/user space need half of the memory space 每個 Process 的 Kernel Space 都是共用的，在 SMP Processor 上所有 Process 都共用同一個 Linux Kernel DRAM 有很多用途，例如: 作為 I/O 加速的 Buffer/Cache I/O Buffer: CPU 的資料可以先寫入 DRAM buffer，然後再由 DMA controller 將資料寫入 Disk I/O Cache: Disk 的資料可以寫入 DRAM cache，因為 I/O request 會產生 overhead 讀取 4kb 和讀取 16kb 的速度是一樣的，那乾脆一次從 cache 讀取 16kb OS 會把相關的資料放在 cache，這樣下次讀取就可以直接從 cache 讀取，而不用再次讀取 Disk When Multi-Process running, what does the memory look like to users/programmers 這裡只討論 User space，一次只會執行一個 Process 這三個 Proces 各自有完整的 user space，當 Context switch 時除了 CPU 控制權會被交換外， 也會重新進行 Memory mapping(修改 MMU 的 mapping table) Internal memory configuration method of the Process 一個 Process 是怎麼在 Memeory 中進行分配的狀態 Local variables: 在 Stack 中分配 Global variables initialized value: 在 initialized data 分配 uninitialized value(BSS): 在 unitialized data 分配 Dynamic memory allocation: 在 Heap 中分配 Program code: 在 Text section 分配，例如 Main, malloc function 的指令 通常 OS 一次會給 4096(4K) 大小的 Memory，並且會清空，這樣就不會有安全性問題，但寫程式時最好只預設 BSS 段的值會是 0，例如 Stack 可能會因為因為 Call/Return 的關係，而有一些不可預期的值。 但即使這樣也盡量要給予初始值，例如: int a = 0;，減少不可預期的錯誤發生 The position of variable in the Process 在一個程式執行時 Text/initilized section 幾乎就是直接從 Disk copy 到 Memory Unitialized section 因為沒有資料儲存，所以可以透過一個資料結構來描述，並放在執行檔的 Header 中 Stack/Heap 會隨著程式執行而變大，所以放在最後面並且往下/上成長 但是 Stack 通常會被限制在固定大小，例如: 一開始分配 16KB，當需要成長時就 OS 就再分配 4KB，但最多長到 8MB，這個可以透過 ulimit 查看或修改 如果一個 Memory 被寫入 DRAM 後，但長時間沒有被使用，那麼這個 Memory 就會被 swap out 到 Disk，這樣就可以釋放出 DRAM 給其他 Process 使用 Linux kernel uses Logical meaning to manage memory segments of processes Linux Kernel 透過 task_struct, mm_struct, vm_area_struct 來管理 Process 的 Memory: task_struct: 描述 Task 相關的所有資訊 mm_struct: 描述 Task 的記憶體相關的資訊，例如: 該 Task 的 Memory space 有哪些 area vm_area_struct: 描述該 area 相關的資訊，例如: 該 area 的起始位置、大小、權限等等 例如除了 Text area 是可 Read, Execute(rx)，其他的 area 都是可 Read, Write(rw) Example: Lab main.c 我們用一個簡單的程式 main.c 來做測試: int a = 2; int b; int main() { int c, d; int* e = (int*)malloc(sizeof(int)*1024); printf(\"pid = %d\\n\", getpid()); printf(\"main = %p\\n\", main); printf(\"printf = %p\\n\", printf); printf(\"a=%p, b=%p, c=%p, d=%p, *e=%p\\n\", &amp;a, &amp;b, &amp;c, &amp;d, e); getchar(); return 0; } /* benson@vm:~/OSDI$ ./main.exe pid = 190697 main = 0x55d00a13218a printf = 0x7f94c1974cc0 a=0x55d00a135010, b=0x55d00a135018, c=0x7ffe4cb7e4b8, d=0x7ffe4cb7e4bc, *e=0x55d00aca62a0 */ 然後在 /proc//maps 中可以看到該 Process 的 Memory configuration: Address space layout randomization 如果我們重新執行一次程式，會發現 Address 又不一樣了，這是為了避免被攻擊，就是 Address space layout randomization(ASLR) 這樣可以避免攻擊者使用記憶體裡面的函數，例如: libc 裡面的 system()，如果可以執行 system()，那麼就可以執行任意的指令 OS 會隨機產生每個 Section 的 Address 幾乎所有的 OS 都支援 ASLR，例如: Linux, BSD, Windows, MacOS 但是 ASLR 也有缺點，如果不使用 ASLR 那就可以把常用的 Function 放在固定的位置，這樣就可以加速程式的執行 目前大部分硬體都使用 phy.cache 可以降低這部分的影響 現在的 Linux Kernel 都會使用 ASLR，即是 KASLR Program in Memory 目前大部分的作業系統設計中，執行檔與在 Memory 中的結構幾乎一樣，OS 只需要 Copy(mapping) 就可以執行了 例如: Linux 的 ELF(Executable and Linkable Format), Microsoft 的 PE(Portable Executable) 將執行檔 Mapping 1:1 映射到 Memory，這樣讓 OS 的工作能變得很簡單 延伸閱讀: Memory Layout of Kernel and UserSpace in Linux. Process Life Cycle 3.3 Process Life Cycle 下面是一個 Unix Process Life Cycle，但在這裡加入了一些 Linux 的觀念 Parent Process 通常是 Shell，透過 fork() 產生 Child Process Ready queue: 當一個新的 Process 產生會進入 Ready Queue，等待 CPU 資源 Running: 如果 Scheduler 選擇到該 Process，那麼就會進入 Running 狀態 Waiting: 在 Linux 中 Waiting 分為兩種 Interruptible: 可以被 Signal 打斷 Uninterruptible: 不能被 Signal 打斷，但是少數例外下例如 Kill -9 還是能夠 Interruptible Terminate: 這裡需要由 OS 去回收分配給 Process 的資源，例如: Memory, Kernel 中儲存的 Process 相關資訊 Zombie: Linux 中會剩下一個大約 4KB ~ 8KB 的 Task struct，稱作 Zombie 保留這個 Zombie 是為了讓 Parent Process 可以透過 wait() 取得 Child Process 的資訊， 這裡如果 Parent 沒有正確的回收 Child Process，但還是持續運行，這樣 Zombie 就會越來越多。 但如果 Parent 也結束了，那麼 Zombie 就會被 init process 回收，這樣就不會有 Zombie Process #include &lt;stdlib.h&gt; #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; int main() { pid_t child_pid; /* Create a child process */ child_pid = fork(); if (child_pid &gt; 0) { /* Parent sleep */ sleep(60); } else { /* This is clild process will end immediately */ printf(\"Child pid %d\\n\", getpid()); exit(0); } return 0; } 這個程式會印出 child_pid，此時去 top -p child_pid 就可以看到 child 變成 zombie 狀態。 fork() 會返回 child pid，但在 child process 中 child_pid 會是 0 3.4 Tack Contol Block(TCB, PCB) Process control block ≈ Task control block Process control block(PCB) 就是 OS 用來管理 Process 的資料結構，通常會包含以下資訊: Process state: 執行的狀態，例如: Running, Waiting, Ready CPU information: Process 的狀態，例如: PC, Register Memory information: Memory 狀態，例如: Text, Data section Schedule information: 排程資訊，例如: Priority I/O status information: I/O 狀態，例如: File descriptor Using resource: Process 使用的資源，例如: File, I/O device … 3.5 Three Scheduler Model Scheduler 不是只有 CPU Scheduler，還有 Long-term Scheduler, Mid-term Scheduler: Long-term Scheduler(Job scheduler): 決定哪些 Process 要進入 Ready Queue，通常在很大型的主機上，例如: 台灣杉 Linux 中並沒有 Long-term Scheduler，Task 產生後就會進入 Ready Queue Mid-term Scheduler(Swapper): 當 Degree of Multiprogramming 過高時可能造成(thrashing)，將一些 Task swap out 到 Disk，等到資源足夠時再 swap in Linux Kernel 目前也沒有 Mid-term Scheduler，但 Linux 依照 Task 記憶體的使用情況，在 Memory 不足時會將不活躍的 Task swap out 到 Disk Short-term Scheduler: CPU Scheduler 大部分的 OS 只有 CPU Scheduler，針對各種事件有專屬的 Waiting Queue，例如: 例如 I/O, Semaphore thrashing(輾轉現象) 指的是當虛擬記憶體被使用過度，導致大部分的工作在處理 Page fault 所造成的 Page Replacement，這樣就會造成 CPU 效能下降 3.6 Context Switch(ctx-sw) 目前主流的 OS 都是只有 Task 執行在 Kernel Space 時才能進行 Context switch Context switch 主要是切換 Register 與 Memory 的內容 Context switch 的 overhead 主要是發生在 Cache memory 的更新 首先 TaskA Mode change 到 Kernel mode 然後將 TaskA 的資訊(TCB) 儲存起來 載入 TaskB 的資訊 最後 TaskB Mode change 到 User mode Scheduler 也就是策略的部分主要是用 C 寫的，但切換的部分是用 Assembly 寫的，因為要直接操作 Register 3.7 Processes are divided into I/O and CPU I/O Bound process - I/O time » CPU time 例如: ftp server CPU Bound process - CPU time » I/O time 例如: image processing 如果可以選擇的話，讓系統中同時存在 I/O Bound process 與 CPU Bound process，可以讓系統的效率最大化 通常 I/O Bound 的優先權比較高，因為趕快讓 CPU 發出命令給 I/O device，然後就可以去執行其他的 Task I/O Bound 通常只需要一小部分的 CPU 資源，如果設定成 CPU Bound 優先權較高，反而會造成 I/O Bound 的 Task 在結束一段 I/O 後還要等待 CPU Bound 的 Task 結束 造成 CPU 使用率下降 3.8 Process Creation Linux 中可以透過 fork, vfork, clone 來產生 Process 實際上這三個在 Kernel 中都是呼叫 do_fork() 來完成 Linux 中 pid 0 是 idle process，優先權最低，只負責讓 CPU 進入睡眠狀態 通常也叫做 swapper，每顆 core 有一個自己的 idle task pid 1 是系統中第一個 user space 的行程，負責作業系統的初始化 例如: 當電腦啟動時的 Daemon Process fork 出的 Process 其程式碼與父 Process 完全相同，如果要載入新的程式碼到該 process 中，使用 execve 系統呼叫 如果需要大量的執行 execve，那使用 vfork 會比較好，因為 vfork 會 Block parent process pid 0 Process(idle process) 也是唯一沒有使用 fork() 產生的 Process，因為 pid 0 是系統啟動時就產生的 Process 使用 pstree -p 就能看到，所有的 Process 都是由 systemd(pid 1) 產生的 例如: 從 bash 去執行 ls 會有以下的流程 bash fork 出一個 child process，然後 parent process wait() child process 透過 execve() 去執行 ls ls 執行完後，透過 exit() 結束，回到 parent process 3.9 Process Termination 在 UNIX-Like OS 中，如果一個 Process Terminate，會變成 Zombie 狀態，Zombie Process 是無法被 kill 的，只能透過 Parent Process 使用 wait() 來回收 透過 wait() Parent Process 可以取得 Child Process 的結束狀態，例如: 使用了多少系統資源 基於特定的需求，也可以直接將 Process kill 掉 kill -9 pid 會直接發送 SIGKILL(signal 9) 給該 Process，讓該 Process 立即結束 kill pid 則是發送 SIGTERM(signal 15) 給該 Process，讓該 Process 優雅的結束自己 有些 OS 設計 Parent process kill 掉後，Child process 也會被 kill 掉 UNIX-Like OS 中，如果 Parent process 被 kill 掉，Child process 會被 init process(pid 1) 接管 init 內部有一個無窮迴圈，會不斷的執行 wait()，這樣就可以回收所有的 Zombie Process 例如我們可以透過 nohup 或 screen 來讓 Process 在背景執行，這樣就不會因為 Terminal 被關閉而被 kill 掉 Communication model between Process &amp; Process OS 保證每個 Process 之間都可以獨立的運行 但如果真的 Process 都完全獨立運行，那系統就會變得很難使用 例如: copy-paste 所以 OS 會提供一些方法讓 Process communication 3.10 Interprocess Communication(IPC) IPC 是指可以讓兩個獨立的 Process 互相傳遞訊息，傳遞訊息的目的多半是 傳遞資訊，例如: copy-paste、information sharing 同步，例如: Parallel computing 模組化設計，例如: 將 Request 與 Worker 分開 IPC Model 這裡談的主要是 IPC 的分類，而不是 IPC 的實作 如何在 Process 之間建立 IPC 可以建立多少條 IPC 在 Process 之間 可否多個 Process 同時使用同一個 IPC IPC 有沒有容量限制 IPC 中每一個 Message 的大小是否是固定的 IPC 是單向的還是雙向的 Direct communication(直接傳遞): 每個需要通訊必須明確的指定接收者或發送者 可以是單向也可以是雙向的 例如: Pipe Indirect communication(間接傳遞): Message 是發送到 Mailbox 中然後由 Receiver 自行取出 是雙向的，並且可以建立多條 IPC 或讓多個 Process 一起接收 因為有 Mailbox 所以就要考慮 Buffer 的問題 沒有 Buffer: 那就必須等到 Recv 結束，發送者才能繼續執行 優點是速度通常比較快，透過 Scheduler 或許某些資訊可以放在 Register 中直接傳遞 固定 Buffer: 發送者將資料 Send 到 Buffer 中就可以繼續執行 多個 Buffer: 發送者可以一直送資料，但通常會限制發送的數量避免惡意程式 Direct communication 通常使用 Process id 來將訊息丟給對方: send(P, message) receive(Q, message), receive(&amp;Q, message) Receiver 可以指定是要從哪裡收，或者收任何訊息，由 OS 來告知是誰送的 Feature 不需要特別的建立連接 由於使用 Process id 來傳遞訊息，因此只能是任兩個 Process 之間傳遞訊息 由 P 和 Q 兩個單項傳遞來組合成一個雙向傳遞 Indirect communication 需要由使用者來建立傳輸通道 例如: Linux 的 mkfifo, pipe 例如: TCP/IP (如果在同一台機器上傳輸資料，不會經過 Network card) Feature 溝通的行程可以建立多個通道，可以簡化設計複雜度 可以「多個傳輸行程」對「多個接收行程」，常見於 Server 的設計 雙向，例如: Shared memory 單向，例如: pipe Problems by many-to-many 如果有「多個傳輸行程」對「多個接收行程」 由誰接收 是否由「通道管理程式」決定? 誰先發起，就由誰收 收了訊息之後怎麼處理 移除訊息，通常用於 Server 將 Task 交給一個 Sub-Server 一直存在，類似於廣播 Blocking &amp; Non-Blocking 如果有足夠多的 Buffer 的話，Process 間的通訊可以是 Non-blocking 送出 Message 後 Process 繼續下一個工作 例如: signal 如果 Buffer 不足，或者根本沒有 Buffer 的話，就只能是 Blocking 送出 Message 後必須等待 Receiver 接收完畢，才能繼續工作 這個的好處是可以確認對方已經收到 Message 3.11 Communication method Direct or Indirect Shared Memory 在 Physical memory 上 Process A 和 Process B 是使用不同的區段，但是 Shared memory 就使用同一區段 要注意這裡是 Physical memory，但在 Process 中是不同的 Logical address 在 Linux 上可以透過 mmap() 來建立 Shared memory Message Passing 在 Process A 時呼叫 Kernel copy 資料到 Kernel space Context switch 到 Process B 時，Kernel 再將資料 copy 到 Process B 的 Memory Producer-Consumer problem 這裡先簡單討論 Producer-Consumer Problem 的概念，後面會再討論如何解決 假如有兩個 Process 共享一個固定大小的 Buffer，Producer 會不斷的產生資料，然後放到 Buffer 中由 Consumer 來取出 如果 Buffer 滿了，Producer 就必須等待 Consumer 取出資料 如果 Buffer 空了，Consumer 就必須等待 Producer 產生資料 這樣如果沒有設計好就容易造成 Deadlock 如果是單對單的 Producer-Consumer，可以透過一個環狀 Linked list 解決，詳情請看 OS-CH03-重要的生產消費問題 Thread concept 3.12 Context switch main overhead The overhead of context-switch Store/restore the register file (~1KB) TLB miss (~1KB) CPU cache miss (~1MB) 在 Context 中最主要的消耗就是 Cache miss，這取決於硬體的支援 Virtual cache: 就需要把 Flush Cache，透過 MMU 將 Virtual address 對應到 Physical address Physical cache: 不需要 Flush cache 需要 MMU 轉換 Virtual address 成 Physical address，才能放入 Cache，轉換的過程就會有 Latency 例如: CPU Cache miss 在等待 L2 Cache 抓到資料，或是 L1 miss 之後需要 MMU 轉換 L2 之後才能做存取 例如: Process A/B，進行了一個 A -&gt; B -&gt; A 的切換，它們各自執行的時候都會把資料放入 Cache，A 只能期望 B 沒有覆蓋掉需要的資料 Cache 是否支援 ASID (Address Space Identifier) 在 TLB 中加入一個 Process ID，只有當 ASID 與 Page number 都相同時，才會 Hit 3.13 Thread memory Thread 在同一塊 Virtual memory 中執行，但是有各自的 Stack 因為在同一塊 Virtual memory 中執行，所以 Thread1 可以存取 Thread2 的 Stack 要做這樣的存取要慎重，因為 Stack 會隨著 Function call 而變動 Thread Local Storage 同樣的 Thread 之間也會有各自的 Local variable，這些 Local variable 會放在 Thread Local Storage 中， 這是由 Compiler 來設計的，讓每個 TLS 偏移量都不一樣，這樣就能讓 Thread 存取自己的 Local variable。 3.14 Thread history Many to One One to One Many to Many Many to One 多對一就是兩個 Thread 共用一個 PCB，這樣的話如果其中一個 Thread 跑去做 I/O 的話，那整個 Process 就會被 Block，這樣就會造成整個 Process 都被 Block。 同時由於 OS 不會知道 PCB 上的是兩個 Thread，所以無法再多核心上執行，這樣就會造成效能的下降。 Green thread Green thread 是為了在底層的 OS 不支援 Thread 的情況下，透過 Library 來模擬 Thread 的行為，但這樣就只能使用 Many to One 的模型，例如: Java 的 Thread 通常只有在 OS Kernel 不支援 Multi-thread 的情況下，才會使用 Many to One，由於所有的 User thread 在 Kernel 都只有一個 PCB， 所以如果 Thread 跑去做 Block 的操作會導致其他 Thread 也被 Block。並且就算有很多 Processor 通常也只有一個 Thread 在執行，其他 Thread 都在等待。 One to One 通常是最多 OS 使用的 Model，每個 Thread 都有自己的 PCB，要透過 Memory control block 來判斷是 Thread 還是 Process， 如果共用 Memory control block 的話，那就判定他是一個 Thread。 由於每個 Thread 都有自己的 PCB，所以可以在多核心上執行 大部分都是 Non-blocking，所以在處理 Block 的任務上會很有彈性 Many to Many 上面的稱作 User thread，下面稱作 Kernel thread，對應的方式有很多種，例如下圖代表上面的 User thread 可以同時發出同等 Kernel thread 數量的 System call， 但缺點是非常複雜並寫不好寫，並且不易理解，讓程式設計者很難進行優化。 看起來是最有彈性的 Thread Sun Solaris 9 之前支援 Many to Many Model Sun Solaris 10 之後改為主要支援 One to One Model Last Edit 10-21-2023 18:52"
  },"/jekyll/2023-10-17-container_of.html": {
    "title": "Note | Linux Kernel Macro container_of &amp; offsetof",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-10-17-container_of.html",
    "body": "container_of 這個 Macro 在 Linux kernel 會經常被用到，因此先理解 container_of 絕對非常重要 container_of container_of 的定義在 &lt;include/linux/container_of.h&gt;，並且需要使用 Marco offsetof，container_of 可以透過一個 struct 中的某個成員來獲得該 struct 的起始位置， 這樣的做法會在 Linux kernel 中被頻繁的使用到。 例如 Linux kernel 中的 &lt;lib/rbtree.c&gt;, &lt;include/linux/list.h&gt;，在 list.h 中 container_of 被用來找尋 list_last_entry, list_first_entry， 而在 rbtree.c 中則可以用來找尋父節點。 /** * container_of - cast a member of a structure out to the containing structure * @ptr: the pointer to the member. * @type: the type of the container struct this is embedded in. * @member: the name of the member within the struct. * */ #define container_of(ptr, type, member) ({ \\ const typeof( ((type *)0)-&gt;member ) *__mptr = (ptr); \\ (type *)( (char *)__mptr - offsetof(type,member) );}) typeof( ((type *)0 -&gt; member) ): 先宣告一個 (type *)0 (struct 的 Null 指標)，然後指向 struct 中的該 member，然後透過 typeof() 獲得 member 的 type const typeof( ((type *)0)-&gt;member ) *__mptr = (ptr): 宣告該 member type 的 pointer *__mptr 就能指向 ptr 所指向的位置 (char *)__mptr 將 __mptr 轉換為 char，因為 char 長度為 1 byte，這樣才能正確做之後運算 offsetof(type,member) 會返回從 struct 起始位置到 member 的偏移量(byte) (type *)( (char *)__mptr - offsetof(type,member) ) 最後將 __mptr - offset = struct 的起始位置，然後轉回 type* 下圖是如何透過 Offset 找到 Struct 起始位址的說明: 延伸閱讀: Rationale behind the container_of macro in linux/list.h, What is the purpose of __mptr in latest container_of macro? 延伸閱讀解釋了為什麼要另外去宣告 __mptr，我也好奇如果已經拿到 ptr 為什麼還要特別去使用 (type *)0 -&gt; member，這樣的方式來獲取 member type，如果將其改為以下程式碼， 一樣可以進行使用: Type 的檢查，這樣可以增加安全性，確保 ptr 真的與 member 型別相同 Kernel 使用的 C standard 有可能對這種寫法跳出 Warning #define container_of(ptr, type, member) ({ \\ (type *)( (char *)ptr - offsetof(type,member) );}) Last Edit 10-18-2023 23:55"
  },"/jekyll/2023-10-10-test_case_generation.html": {
    "title": "Testing | Test Case Generation",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-10-10-test_case_generation.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. Test case 的產生方式可能是無窮無盡的，因此也需要一些策略來幫助產生 Test case 以下是本章節主要介紹的目標，這個章節最後的基於限制式的測試會使用 ECLiPSeclp 來進行實作，不會在這裡介紹， 會另外開一篇講述如何使用 Constraint Logic Programming 來生成測試案例。 Test case generation Equivalence class partitioning Boundary value analysis Domain specific information Constraint-based testing Test case generation 的目標是從可能無窮(possibly infinite) Collection of candidate test cases， 選出盡可能少(Few)並且有效(Effective)的 Test case Domain knowledge 在測試特定領域的應用時能起到非常關鍵的作用 Two Main Issues 這就涉及兩個主要的問題，可以透過一些原則來解決: Few(少): 對 Input domains 所有的 Value 進行測試是不可能的，我們只能挑選一部分的 Subset 來測試 Equivalence class partitioning(等價類別劃分) Test coverage criteria(測試覆蓋標準) Effective(有效): 我們希望選擇一個 Subset，能夠找到最多的 Errors Boundary value analysis(邊界值分析) 拿一元二次方程式為例，公式解為: $ax^2 + bx + c = 0$, $r = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 將每個 Input variable 以 float(32 bit) 表示，所有可能的輸入值數量將會是: $2^{32} + 2^{32} + 2^{32} = 2^{96}$ 3.1 Equivalence Class Partitioning 一組精心選擇的輸入值應該能夠覆蓋許多其他輸入值 這代表我們應該把 Input domains(輸入域) 劃分為有限數量的 Equivalence classes(等價類別) 而測試每個 Equivalence class 中的 Representative value(代表值) 就等於測試了 Equivalence class 中的所有其他值 3.1.1 Valid and Invalid Equivalence Classes Equivalence classes 通常透過 Input constratint(輸入限制) 來劃分 Input domain(輸入域) 這裡會有兩種 Equivalence classes，Valid 和 Invalid Valid: 代表程式的有效輸入 Invalid: 代表所有其他可能的狀態 An Example: If an input constraint specifies a range of values (e.g., the count can be from 1 to 999), it identifies one valid equivalence class (1 ≤ count ≤ 999) and two invalid equivalence classes (count &lt; 1 and count &gt; 999) 3.1.2 Partitioning Equivalence Classes 如果程式中的 Valid/Invalid Equivalence classes 並不被程式以相同方式處理，則需要劃分更多的 Equivalence classes 如果我們有一個輸入年齡 Y 的程式: Invalid: 的輸入可能會被劃分為 Y &lt; 0 和 Y &gt; 1000 (因為根本不會有人活到 1000 歲) Valid: 我們也可以把 Y &gt; 65 跟 Y &lt;= 65 區分開來，因為大於 65 歲的退休人士可能有不同的行為 這樣我們就在 Invalid/Valid 中另外劃分了 2 個 Equivalence classes An Example: 回到一元二次方程式為例，方程式的解取決於: $d = b^2 - 4ac$ $The\\;equation\\;has\\;two\\;different\\;real\\;roots\\;if\\;d&gt;0$ $The\\;equation\\;has\\;two\\;identical\\;real\\;roots\\;if\\;d=0.$ $The\\;equation\\;has\\;no\\;real\\;root\\;if\\;d&lt;0.$ 將一元二次方程式依照 Root 的情況劃分為三種 Equivalence class，這樣就能在這三種情況下挑選 a, b, c 的代表值 3.1.3 Input Space, Vectors, Points Input Space: Let x1, x2, …, xn denote the input variables. Then these nvariables form an n-dimensional space that we call input space. Input Vector: The input space can be represented by a vector X, we call input vector, where X = [x1, x2, …, xn]. Test Point: When the input vector X takes a specific value, we call it a test pointor a test case, which corresponds to a point in the input space. 假如有一個 Function，接受兩個 Variable x, y，所有的可能輸入值就會是一個 Input Space， 那麼我們的 Input Vector 就是這個 2D 平面上的所有點，而 Test Point 就是這個我們選擇進行測試的點 3.1.4 Input Domain and Sub-Domain Domain: The input domainconsists of all the points representing all the allowable input combinations specified for the program in the product specification. Sub-Domain: An input sub-domainis a subset of the input domain. In general, a sun-domain can be defined by a set of inequalitiesin the form off(x1, x2, …, xn) &lt; K, where “&lt;” can also be replaced by other relational operators. Domain 就是在程式規格允許下的所有輸入值，而這些輸入值可以被程式中的不等式所劃分為 Sub-Domain Input Domain Partitioning An input domain partitioningis a partition of the input domain into a number of sub-domains. These partitioned sub-domains are mutually exclusive, and collectively exhaustive. 例如: 整數輸入可以被劃分為三個 Sub-Domain，n &lt; 0, n = 0, n &gt; 0，這三個 Sub-Domain 互斥且完全涵蓋了整個整數 Domain 3.2 Test Coverage Criteria 如果 Equivalence classes 的數量還是太多，那我們就需要 Test coverage criteria(測試覆蓋標準)來限制 Test Case 的數量， 在這裡我們還不會詳細談有哪幾種 Test coverage criteria。 Test Case Candidates Reduction: 下圖是一個減少 Test Case 的流程 3.3 Boundary Value Analysis 邊界上的測試案例通常是最有效的，因為邊界是最容易找到錯誤的地方 A boundaryis where two sub-domains meet. A point on a boundary is called a boundary point. Boundary points are input values with the highest probability of finding the most errors. 3.3.1 Definition of boundaries Linear Boundaries and Sub-Domains A boundary is a linear boundaryif it is defined by: $a_1x_1+ a_2x_2+ … + a_nx_n = K$ Otherwise, it is called a nonlinear boundary. A sub-domain is called a linear sub-domainif its boundaries are all linear ones. Open and Closed Boundaries A boundary is a closedone with respect to a specific sub-domain if all the boundary points belong to the sub-domain (&lt;=, &gt;=, =). A boundary is an openone with respect to a specific sub-domain if none of the boundary points belong to the sub-domain (&gt;, &lt;, !=). A sub-domain with all open boundaries is called an open sub-domain; One with all closed boundaries is called a closed sub-domain; otherwise it is a mixed sub-domain 如果一個邊界的 Point 在 Domain 中，那麼這個邊界就是 Close 的，否則就是 Open 的，例如: Domain 1 &lt; x &lt;= 100 邊界 1 並不屬於 Domain，因此這個邊界是 Open 的 邊界 100 屬於 Domain，因此這個邊界是 Close 的 Interior and Exterior Points 屬於 Sub-domain 但不在邊界上的點稱作 Interior point 不屬於 Sub-domain 並且不在邊界上的點稱作 Exterior point 而兩條以上的邊界相交的點稱作 Vertex point General Problems with Input Values Some input values cannot be handled by the program. These input values are under-defined. Some input values result in different output. These input values are over-defined. These problems are most likely to happen at boundaries. Under-defined input values(未定義的輸入值): 也就是程式無法處理的 Input value，例如: 除以零 Over-defined inpyt values(過度定義的輸入值): 也就是程式可以處理但是可能有不同輸出的 Input value，例如: 一個投票系統，可能會因為地方的法律而有不同的投票年齡限制 3.3.2 Boundary Problems 這裡列出 5 個主要的 Boundary Problems: Closure Problem(閉合問題): whether the boundary points belong to the sub-domain. Boundary shift/tilt Problem: where exactly a boundary is between the intended and the actual boundary. Boundary shift Problem: f(x1, x2, …, xn) = K, where a small change in K. Boundary tilt Problem: f(x1, x2, …, xn) = K, where a small change in some parameters. Missing/Extra boundary Problem: Missing: a boundary missing means that two neighboring sub-domains collapse into one sub-domain. Extra: An extra boundary further partitions a sub-domain into two smaller sub-domains. 3.4 Test Case Generation Strategy Weak N x 1 / 1 x 1 Strategy 都是一種用於邊界測試的策略，這裡會介紹這兩種策略的差異與優缺點 3.4.1 Weak N x 1 Strategy 3.4.3 Weak 1 x 1 Strategy 3.4.1 Weak N x 1 Strategy In an n-dimensional space, a boundary defined by a linear equationin the form off (x1, x2, …, xn) = K would need nlinearly independent pointsto define it. We can select nsuch boundary points, called ON points, to precisely define the boundary. We can also select a point, called an OFF point, that receives different processing. The OFF Points 閉合的邊界: 那麼它的 Off point 會位邊界的外部 開放的邊界: 那麼他的 On point 會位邊界的內部 0 &lt;= N &lt; 21 在這個例子上有兩個邊界，其中 0 是 Close boundary、21 是 Open boundary ON Points 是 0, 21 這兩個位於邊界上的點 0 是 Close boundary, Off point -1 位於外側 20 是 Open boundary, Off point 20 位於內側 Distance of the OFF Points 需要 Off point 的理由是，他與邊界非常的接近，以至於邊界的細微變化都將影響 Off point 的處理 實際應用上，使用 distance ε 作為 Off point 與邊界的偏移距離 For integers, ε = 1. For numbers with nbinary digits after the decimal point, ε = 1/2n. 例如: 有一個 0.001 作為邊界值，那麼 ε 就是 1/23 = 1/8 Position of the OFF Points Off point 應該要位於所有 On point 的中央 對於一個 2D 的空間來說，他應該選擇的方式如下: 選擇位於兩個 On point 的中點 根據這個邊界是 Closed 或 Opend 向外或向內移動 ε 的距離 Total Test Points 除了 ON/OFF Points 我們也會再選擇一個 Interior Point(內部點)做為該 Equivalence Class(Sub-Domain) 的代表， 因此一個 N Dimensional domain 將會有 (n + 1)*b + 1 個 Test Points。 Example: 假設有一個稅收級距如下，注意其中 Close 與 Open 的條件，這裡都是 Integers 這樣的話旁邊的 Sub-Domain OFF Points 剛好會重疊在一起可以省略掉， 並且 Open domain 也可以省略一個邊界的值，因此原本應該要有 3 * 2 + 5 * 3 = 21 個點，但是有 4 個邊是重疊的因此 21 - 4 * 2 = 13， 最終僅用上 13 個 Test Points。 Tax Rate: 0%: 0 &lt;= x &lt; 10000 (0~9999) 10%: 10000 &lt;= x &lt; 1000000 (10000~999999) 20%: 1000000 &lt;= x &lt; 100000000 (1000000~99999999) 30%: x &lt;= 100000000 (100000000~) 那如果假設一個 2D Sub-domain，並且四個邊都是封閉的，將會是 (2 + 1) * 4 + 1 = 13 個 Test Points，這裡忽略了與旁邊的 Sub-domain 重疊的點 3.4.2 Boundary Problem Detection of Werk N * 1 Strategy 這裡說明 Weak N x 1 Strategy 在處理 Boundary Problem 時能做到什麼，不能做到什麼 Closure problem 定義邊界是是否所有可能的邊界都被包含在內 Boundary shift problem 邊界是否有正確設置在應該的位置 Boundary tilt problem 邊界是否有正確的對齊或平行 Missing boundary problem 是否所有的邊界都有被定義 Extra boundary problem 是否有多餘的邊界 Weak N x 1 Strategy 可以很好的處理其他 Boundary problem 但無法完全偵測到 Extra boundary problem 3.4.3 Weak 1 x 1 Strategy Weak 1 x 1 Strategy 在每個邊界上只放置一個 On point 與一個 Off point，減少 Test Points 數量，但是也會有缺點 One of the major drawbacks of weak N x 1 strategy is the number of test points used, (n+1)xb+1 for ninput variables and boundaries. Weak 1 x 1 strategy uses just one ON point for each boundary, thus reducing the total number of test points to 2xb+1. The OFF point is just ε distance from the ON point and perpendicular to the boundary. Weak 1 x 1 Strategy 也可以處理 Boundary Problem，但是在傾斜上的表現不如 Weak N x 1 Strategy，並且跟 Weak N x 1 Strategy 一樣無法完全偵測到 Extra boundary problem 在 2D 平面上比較能表示出兩種策略的差異，可以看到 Weak N x 1 Strategy 在同個 Domain 的邊界上會有 3 個 Points，而 Weak 1 x 1 Strategy 則只有 2 個 Points， 下圖左右分別是 Weak N x 1 Strategy 和 Weak 1 x 1 Strategy: 3.5 Looking for Equivalence Classes 找尋 Equivalence Classes(等價類別)的方法與需要注意的點 Don’t forget equivalence classes for invalid inputs. Organize your classifications into a table or an outline. Look for ranges of numbers. Look for membership in a group. Analyze responses to lists and menus. Look for variables that must be equal. Create time-determined equivalence classes. Look for variable groups that must calculate to a certain value or range. Look for equivalent output events. Look for equivalent operating environments. Don’t Forget Equivalence Classes for Invalid Inputs 通常 Invalid Inputs 是最容易產生 Bugs 的來源 例如一個能接受 1 到 99 之間任何數字的程式，那就至少有四個 Equivalence Classes 1 &gt;= x &lt;= 99 x &lt; 1 x &gt; 99 Not a number(Is this true for all non-numbers?) Organize Your Classifications into a Table or an Outline 把分類整理成表格或者大綱 會發現有這麼多的 Input/Output constraints，跟相關的 Equivalence Classes，需要一種組織方法 最常用的方法就是 Table(表格)或者 OutLine(大綱) Look for Ranges of Numbers 如果找到一個數字的範圍，例如: 1 到 99，這些範圍就是 Equivalence Classes 通常會有三個 Invalid Equivalence Classes，小於 1、大於 99、以及不是數字的情況 當然也會有多個範圍的情況，例如: Tax Look for Membership in a Group 如果一個 Input 必須屬於某個 Group，那麼一個 Equivalence Class 則包含該 Group 的所有成員 而另一個 Equivalence Class 則包含所有不屬於該 Group 的成員 例如: 要求輸入一個國家的名稱，Valid Equivalence Class 就是所有國家的名稱，Invalid Equivalence Class 就是所有不是國家的名稱 但是，abbreviations(縮寫)、almost correct spellings(幾乎正確的拼寫)、native language spellings(母語拼寫)、name that are now out of date but were country names(曾經存在過的名稱)又該如何處理? 應該分別測試這些情況嗎? 通常 Specification 不會提到這些情況，但是在測試中可能會發現這些錯誤 Analyze Responses to Lists and Menus 對於必須從 List 或 Menu 中選擇的 Input，每個選項都是一個 Equivalence Class 每個 Input 都是其自身的 Equivalence Class Invaild Equivalence Class 則是所有不在 List 或 Menu 中的選項 例如: “Are you sure? (Y/N)”，一個 Equivalence Class 就是 Y，另一個 Equivalence Class 就是 N，Invalid Equivalence Class 就是其他所有選項 Look for Variables That Must Be Equal 例如一個可以輸入任何顏色的程式，但必須是黑色，那麼所有的顏色都是 Invalid Equivalence Class，而黑色就是 Valid Equivalence Class 有時這種限制在實際應用中可能會出現意外情況: 例如黑色已售罄，只剩下其他顏色 這種曾經有效但現在不再有效的選擇，應該為它們建立一個 Equivalence Class 例如: 在閏年時 February 有 29 天，但是在非閏年時 February 只有 28 天，這樣就會有兩個 Equivalence Classes Create Time-Determined Equivalence Classes 例如一個程式還沒有從 Disk 完成讀取，在進行中與結束上按下空格鍵是不同的 Equivalence Classes 這種情況通常會有三個 Equivalence Classes，一個是還沒開始讀取的情況，一個是讀取中的情況，一個是讀取完畢的情況 Look for Variable Groups That Must Calculate to a Certain Value or Range 例如輸入 Triangle 的三個邊長 在 Valid Equivalence Class 中，它們的總和應該等於 180° 而 Invalid Equivalence Class 則會有兩個分別是大於 180° 與小於 180° Look for Equivalent Output Events 在此之前我們強調的都是 Input 與 Invalid Input，這是因為 Output 通常更複雜，因為通過程式處理後的 Output 會有很多種可能的情況 例如我們有一個由程式控制的繪圖機，他最多可以一次畫 4 公尺的線條 怎麼判斷一個線條是 Valid Equivalence Class 還是 Invalid Equivalence Class? 有可能繪製了超過 4 公尺的線條 可能根本沒有繪製線條 也有可能繪製了根本不是線條的東西，例如: 圓形 在測試中不只要關注輸出的情況，也要關注輸出並找到 Equivalence Classes Of Output Look for Equivalent Operating Environments 同時對於環境的變化也要找到 Equivalence Classes 例如: 一個程式要求至少要 64K - 256K 的可用 Memory 這樣就會有三個 Equivalence Classes，符合規範與小於 64K、大於 256K的情況 Last Edit 10-28-2023 16:46 剩下的部分是 Constraint-based testing，會另外講述如何使用 CLP 來生成測試案例"
  },"/jekyll/2023-10-09-operating_system_structure.html": {
    "title": "OS | Operating System Structure",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-10-09-operating_system_structure.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. 在這個章節有以下主要大綱 OS Service Graphical User Interface &amp; Text-based User Interface System calls OS Structure Profiling/Debugging an OS Kernel OS Service 2.1 Purpose of OS 一台作業系統的主要服務是使對於 User 而言 Hardware 變的更好使用，對 Designer 而言能提高系統使用率 Simple to use Communications 同一台電腦內部 Process 之間的 Communications(IPC)，例如: copy and paste 跨電腦之間的通訊，例如: Network neighborhood, Network file system(NFS) Error detection 當軟體發生錯誤時，OS 能採取適當的措施「處理」這個錯誤，例如: 使用者光碟機沒有「收合」，提示使用者 提供一般程式的除錯機制，例如: ptrace, core dump … OS Kernel 的除錯機制，例如: kgdb, kdb ptrace 是一種 System call，允許一個 Process 觀察另一個 Process，gdb 的底層就使用 ptrace 實現 提供使用者操作介面，例如: Gnome, KDE … 載入應用程式的能力，例如: execve() 多功能的作業系統，最少要能識別「執行檔」的檔案格式 部分嵌入式系統 Kernel 與 User application 編譯在同一個 mage，task 即為這個 mage 中的一個 Function， 這時候就不需要「執行檔」 處理 I/O 的能力，例如: 各種 Driver 檔案系統，例如: OS 將 Disk 上的一個 Block 抽象為 File，再將 File 抽象為 Folder，Folder 中可以放入 File Increase efficiency 分配各種資源，例如: Memory 僅有 4GB，A 程式需要 3.2GB，B 程式需要 1.4GB，如何分配「有限」的 Memory Debian 上可以透過 free -h，cat /proc/swaps 查看 swap 使用狀況 統計資源的使用率，例如: 可以使用 htop 監視 CPU、DRAM、I/O 等效能，使用率 Protection，確保 Process 只能擁有 OS 所分配的資源，Process 各自獨立，不會受到非法的干擾· Security，確保 User Login 後只能存取自己的資源，例如: 存取他人的家目錄 2.2 User interface GUI: GUI 將許多命令以 Icon 的形式表示，並且大部分可以 Drag and drop(拖動)的方式 再輸入方式包含: 滑鼠、觸控面板、觸控螢幕、多點觸控的直覺化控制 CLI: Text-mode 雖然需要時間學習，但能很準確的下達命令 也可以將命令組合成 Batch program，例如: shell script Text-mode 比 GUI 更穩定 可以使用輕量級的 Remote connection，例如: ssh Text-mode 也可以結合指標裝置(例如: 滑鼠)，也可以使用 Library(例如: ncurses) 模擬 GUI 介面(例如: Linux kernel memuconfig) System Call 2.3 System call System call 是 OS Kernel 對外開放的 API 注意恐龍書將 System call 定義為 OS 對外開放的 API 但 OS 涵蓋非常廣泛，因此 System call 應該限定於 Kernel 提供的 API Linux system call Linux 所有的 System call 可以到這個路徑去尋找 /arch/x86/entry/syscalls/syscall_64.tbl，或是到 Linux 的 syscalls(2) — Linux manual page， 前四個 System call 為: 1. read, 2. write, 3. open, 4. close Call System call 可以透過 libc(C standard library) 呼叫 System call 例如: read, write, open, close Linux 的 Man page 中，volume 2 即為 System call 的說明 例如: man 2 read，這裡要注意是否有安裝 sudo apt-get install manpages-dev 少數 System call 並未包含在 Linux 的 libc 內，這時候需要自已寫出來 #define_GNU_SOURCE /* See feature_test_macros(7) */ #include &lt;unistd.h&gt; #include &lt;sys/syscall.h&gt; /* For SYS_xxx definitions */ long syscall(long number, ...); gettid() - libc 未實現的 System call 在 Linux 中: Process 都有一個 pit_t 可以透過 getpid() 獲得 POSIX Thread(pthread) 也有 pthread_t 可以透過 pthread_self() 獲得， 但是如果我想要 P1 的 thread2 與 P2 的 thread1 通訊，我們就需要一個真實的 Thread ID(TID)，這時候就要透過 System call 來獲得 tid，使用 syscall(SYS_gettid) #define_GNU_SOURCE #include &lt;unistd.h&gt; #include &lt;sys/syscall.h&gt; #include &lt;sys/types.h&gt; #include &lt;signal.h&gt; int main(int argc, char *argv[]) { pid_t tid; /* gettid() returns ths caller's thread ID(TID). There is no glibc wrapper for this system call. */ tid = syscall(SYS_gettid); /* int tgkill(int tgid, int tid, int sig); send a signal to a thread. */ syscall(SYS_tgkill, getpid(), tid, SIGHUP); } Call System Call Method(From assembly language) Call System call 的方式如下(Calling convention): arch/ABI syscall# arg1 arg2 arg3 arg4 arg5 arg6 ret. val. arm64 x8 x0 x1 x2 x3 x4 x5 x0 x86-64 rax rdi rsi rdx r10 r8 r9 rax syscall# System call 編號 ret. val. System call return value Call System call 的組合語言如下: ARM: svc #0, x86-64: syscall movq $60, %rax; // Call NO.60 System call, exit() movq $2, %rdi; // arg1 = 2, means exit(2) syscall; // Change to kernel mode, call System call 延伸閱讀: The Linux Kernel Module Programming Guide: System call 2.4 Special cases of system calls 首先要討論 System call overhead 這是為什麼把某些 Module 放在 Kernel 的速度會提升許多的原因 System call 主要的 Overhead 來自於: CPU 進行 Mode change 的時候，此時 CPU 有同時數個指令在執行，切換時或許需要 flush 所有執行到一半的指令 Kernel 需要將 User space 的所有暫存器存在 Kernel stack（每一個 Task，於 Kernel 中有自己的 Stack，注意，不是 User mode stack） 檢查權限，在 System call 之前還要檢查 Task 是否有權限執行該 System call 依照 Kernel 內部的 Calling convention，呼叫實現該 System call 的 Function，例如：sys_read() =&gt; do_read() 因為這些 Overhead 使得在 User space 執行的程式，速度會比在 Kernel space 執行的程式慢許多 vDSO 因此 Linux 的設計者會希望盡可能地降低 Overhead，vDSO(Virtual Dynamic Shared Object) 是可以將 Kernel space 的資訊，直接映射到 User space 如果該 System call 並沒有牽涉「安全性」，那就直接把 Kernel space 中的資訊寫入 User space，讓程式可以透過 Function call 的方式取得該資料 會時常變動，但又沒有機密性的資訊: __vdso_clock_gettime __vdso_getcpu __vdso_gettimeofday __vdso_time 不會變動，但也沒有機密性的資訊: 第一次呼叫時，真的產生 System call，libc 記錄下該 Function 的值 第二次呼叫時，由 libc 直接回傳，例如：getpid() /proc/pid/maps /*...*/ 7ffe6b98e000-7ffe6b9af000 rw-p 00000000 00:00 0 [stack] 7ffe6b9fa000-7ffe6b9fd000 r--p 00000000 00:00 0 [vvar] 7ffe6b9fd000-7ffe6b9ff000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] 這裡每行都由以下內容組成: address、perms、offset、dev、inode、pathname vsyscall: 功能等同於 vdso，但是較為古老，並有安全性問題(No support ASLR) vdso: 存放可呼叫的 vDSO Function，例如: clock_gettime() vvar: 存放 Kernel space mapping 到 User space 的資料，例如: clock_gettime() 的 cur_time 延伸閱讀: Understanding Linux /proc/pid/maps，ASLR If not using vDSO 下圖左是沒有 vDSO clock_gettime() 想要取得時間的流程，要去呼叫 timekeeping_update() 更新 cur_time，因此需要 Mode change 進入 Kernel mode。 而圖右就將這個資料結構直接映射到 User space，clock_gettime() 呼叫 timekeeping_update() 一樣會去更新 cur_time，但直接去讀 vDSO 中的資料，這樣的話速度就跟 Function call 一樣快了。 vDSO Problem 資料存放在 vvar 不一定就是 User 要的資料格式，例如: vvar 中放的是從開機到現在經過多少個 Machine cycles，但是 gettimeofday() 的回傳值是自 1970/1/1 至今經過多少秒，所以這裡是有座資料轉換的 vDSO 內部的程式碼會做適當的資料轉換 OS Structure 2.5 Monolithic system 許多著名的 OS 都是 Monolithic system，例如: Linux, FreeBSD, Solaris 目前這些作業系統都支援動態載入 Kernel module 的功能 Linux 的 lsmod 可以列出目前已經載入到 Kernel 的 module 這裡列出 Linux Kernel 中與 File system 相關的 module $ lsmod | grep fs autofs4 45056 2 btrfs 1294336 2 xor 24576 1 btrfs raid6_pq 114688 1 btrfs libcrc32c 16384 1 btrfs 2.6 In Linux Object Linux 雖然是使用 C 撰寫的，但是在 Kernel 中充滿了 Object-oriented(OO) 的概念，Object-oriented analysis and design(OOAD)物件導向分析與設計 下面是一個 Linux Kernel 中常見的 OO 概念，並且使用 container_of 來取得這個 Linked list 的起始 address(Linked list head) struct parport_driver { const char *name; /*property*/ void (*attach) (struct parport *); /*method*/ void (*detach) (struct parport *); /*method*/ struct list_head list; /*inherit list_head*/ }; struct list_head { struct list_head *next, *prev; }; #define list_entry(ptr, type, member) container_of(ptr, type, member) container_of /**  * container_of - cast a member of a structure out to the containing structure  * @ptr:        the pointer to the member.  * @type:       the type of the container struct this is embedded in.  * @member:     the name of the member within the struct.  * */ #define container_of(ptr, type, member) ({              \\ const typeof( ((type *)0)-&gt;member ) *__mptr = (ptr);    \\ (type *)( (char *)__mptr - offsetof(type,member) );}) container_of 主要是透過成員來獲取該 struct 的起始位置，詳細可以看 Linux Kernel Macro container_of 跟延伸閱讀 延伸閱讀: Linux 核心原始程式碼巨集: container_of Linux kernel 雖然是使用 C 語言寫的，但在裡面充斥著 OO 的概念，當然有部分要跟底層溝通所有沒有完全 OO 2.7 Layered approach Layered approach 在 OS 的缺點是，並不一定能切出 Layer，跟 Network 不太一樣 將系統分成 N 層 第 N 層可以使用第 N-1 層的功能，不可以使用 N+1 層的功能 例如 I/O memagement 需要 Buffer 因此需要 Memory management，Memory management 有時也需要將 Memory 寫到 Disk，因此需要 I/O management，這樣就很難分層 2.8 Hardware Abstraction Layer 常見的 Andriod OS 與 Windows 架構如下: Andriod: 可以看到 Libraries 被放置在中間層，是因為為了能給各家廠商商業化，而最上方是 Apach License 就是隨意給人更改的部分， 可以看到 Hardware Abstraction Layer(HAL) 被放置在中間層，照常理來說應該放在 Hardware 與 Software 之間，放在這裡或許是想要把 Kernel 有替換的的彈性。 Windows: Windows 的 HAL 就被放置在 Kernel 與 Hardware 之間，抽象層的目標是，例如: 希望 Kernel 中沒有 Assembly，不要太跟 Device 相關。 Kernel 之上包含著 Virtual Memory Manager(把硬體相關的部分抽出，另外一個目錄) Last Edit 10-10-2023 23:21"
  },"/jekyll/2023-09-23-software_testing_introduction.html": {
    "title": "Testing | Software Testing Introduction",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-09-23-software_testing_introduction.html",
    "body": "Software testing course notes from CCU, lecturer Nai-Wei Lin. 軟體測試是確保軟體品質的重要過程，這個過程確保軟體產品符合預期的需求，並確保軟體產品無缺陷，這裡介紹軟測的基礎概論。 1.1 What Is Software Testing 測試是確表品質的方法之一，Software quality assurance(軟體品質保證)涉及 Validation(確效)/Verification(驗證)軟體 Validation: 確效客戶的 Requirements 與設計出的 Specification 一致 Do we build the right software? Verification: 驗證開發的 Implementation 與設計出的 Specification 一致 Do we build the software right? 而軟體測試主要著重在 Verification 這部分，Validation 是設計端要處理的問題 延伸閱讀 Wiki: Verification and validation 1.1.1 Software Verification Verification 也分為 Statically(靜態)與 Dynamically(動態) Statically: 靜態驗證並不執行軟體下進行，它包含 review, inspection, walkthrough, analysis 等技術。 靜態驗證主要關注預防缺陷，通常需要有一定的開發經驗的測試人員來進行 Dynamically: 動態驗證會執行軟體，它包含各種測試技術 動態驗證主要關注找出和修復缺陷，動態測試系統的功能行為，Memory/CPU 使用情況以及系統的整體性能 而在 Software testing 這門課中大部分關注的是 Dynamically 這部分的實作 延伸閱讀 Static Testing vs. Dynamic Testing 1.1.2 Software testing 要注意的是軟體測試並不能證明軟體是完全正確的，軟體測試僅能從體中找到盡可能多的錯誤。因為軟體測試只是識別軟體中潛在的錯誤，而不是證明軟體是正確的。 Error, Fault, Failure, and Incident Error(錯誤): 是人為所犯的錯誤 Fault(故障): 是文件或程序中的 Error 的結果 Failure(失敗): 當 Fault 被執行時就會發生 Failure，Fault 的執行導致程式無法執行預期的功能或結果 Incident(事件): 當 Failure 發生時，用戶可能不會馬上發現，一個 Incident 提醒用戶 Failure 的發生 延伸閱讀 Software Testing – Bug vs Defect vs Error vs Fault vs Failure Test Case Software testing 是執行一組測試案例(Test Case) 的行為，以便能夠找出程式中的 Fault。 一組 Test case 包含一個測試輸入列表和一個相應的預期輸出列表 每個 Test case 都設計來檢查程序的某種特定功能或行為 軟體測試的生命週期，代表了各個步驟所產生的錯誤與錯誤追蹤 Why Do We Need Software Testing Software prevails in our living environment. Quality of software significantly influences our quality of life. Software faults in critical software systems may cause dramatic damages on our lives and finance. Carefully made programs have 5 faults per 1000 lines of code (LOC). Windows XP has 45M LOC, so it may have 225000 faults. 1.2 How Do We Do Software Testing 但是在進行 Testing 前應該要先了解 Testing 到底在測試什麼? Test case 理想的狀況下應該是 Specification ∪ Implementation 的範圍，這樣就能找出所有不合規範的 Fault。 Faults of comission(錯誤的委任): 實際的軟體開發往往都會有超出規格的部分，可能是需求變更或者是在實現功能時遇到了未預見的挑戰。 Faults of omission(錯誤的遺漏): 同樣的開發中也有可能會有規格被遺漏的情況，可能是規格上的錯誤、技術挑戰、時間壓力等原因造成。 1.2.1 Test case Test Case 涉及兩個主要問題，如何 Test case generation(產生測試案例)、如何 Test case execution(執行測試案例) Test case execution: 輸入 Input 至 Software 後得到 Expected output 與 Output 進行比對來決定是 Incident/Correct Excution 目前幾乎都依賴於測試框架來幫助執行，這點之後會再介紹 Test case generation: 要確認測試案例有兩種方式 Black-box testing(Function testing): 軟體被視為一個黑盒子，從規格中描述的功能確定測試案例 White-box testing(Stucture testing): 軟體被視為一個白盒子，從實施的程式的結構確定測試案例 Fuctional Testing vs Structure Testing Black-box 從 Specification 的角度來設計 Test case 因而較難覆蓋到未被規定的行為(Faults of comission) White-box 從 Implementation 的角度來設計 Test case 因而較難覆蓋到未被實現的行為(Faults of omission) 因此兩種方法都不足夠，只有兩種方法都使用才能盡可能的覆蓋 Specification ∪ Implementation 試想在未學習軟體測試前是怎麼去寫 Testing? 我幾乎都是從結構去出發，已現有的程式去開發測試案例，因為壓根就沒設計完整的規格。 1.2.2 Tracking of incidents Incident tracking system(事件追蹤系統)負責追蹤所有需要修復的 Incidents(事件)，確保所有事件都得到妥善解決 需要知道事件的相關人員，應在事件報告後不久得知 這意味著系統應該能夠迅速地通知相關人員有關新的事件，確保所有相關人員都能及時獲得最新的信息，並可以立即開始處理事件 不會有事件因為被遺忘而未修復 系統會持續追蹤每一個事件，直到它被修復，防止任何事件被忽視或遺忘 不會因單一程序員的一時興起而未修復某個事件 修復事件的決定不應該只取決於一個人的主觀意願，而應該基於對事件的客觀評估和團隊的共識 減少因為溝通不良而未修復的事件 這表示系統應該促進良好的溝通，以防止因為溝通問題導致事件未能被修復 1.2.3 Regression testing Regression testing(回歸測試) 重複使用測試案例來測試更改後的軟體，確保之前正常運行的部分沒有被影響造成新的錯誤，既有功能應繼續如常運行。 回歸測試可以在不同的測試階段應用，例如整合測試或系統測試，具體取決於測試案例的細分程度和需求。通常它們被放置在整合測試和系統測試中。 Regression testing 有以下特性，使其在軟體測試中有重要地位: 確保穩定性: 回歸測試確保新的軟體變更不會對現有的功能造成負面影響，確保軟體的整體穩定性和品質 節省時間和成本: 自動化回歸測試可以節省大量的測試時間，特別是對於長期的軟體開發專案或需要頻繁進行版本更新的情況。因為不需要手動執行重複性的測試案例，有助於降低測試成本 快速反饋: 回歸測試可以在每次軟體變更之後迅速運行，提供關於變更對軟體的影響的即時反饋，有助於快速識別並解決問題，從而提高開發效率 Regression testing 也具有一些挑戰: 初期自動化成本: 為了實現自動化回歸測試，需要將測試案例轉化為自動化程式，會造成相當大的工作量和成本 維護成本高: 維護回歸測試套件需要時間和資源。當軟體變更頻繁時，測試套件更新會產生相當的維護成本 執行時間: 如果回歸測試的測試案例變得過多，可能需要較長的時間才能完全執行，可能會對開發流程產生延遲，需要仔細計劃和管理回歸測試的執行時間 延伸閱讀 【D13】測試類型介紹:回歸測試 1.2.4 Levels of testing Levels of testing(測試的各個階段)主要包括以下幾個: Unit Test(單元測試): 這是最基本的測試階段，主要針對最小單元進行測試，確保每個獨立的部分都能正常運作 Integration Testing(整合測試): 此階段針對跨物件或模組進行測試，以確保各個模組之間的交互作用能夠正常運作 System Testing(系統測試):系統測試是一種風險測試，目的是確定整個系統是否滿足特定的功能性和非功能性需求，測試環境需盡可能和正式上線的環境一致 Acceptance Testing(驗收測試): 也被稱為 UAT(使用者接受度測試)，這是最後一個測試階段，會模擬真實使用者情境來驗證軟體是否符合使用者的需求和期望 每一個階段都有其特定的目標和重點，且需要根據具體情況來選擇最適合的策略和方法2。 延伸閱讀 【D11】 實例簡述:測試四階段與測試方法 1.3 Costs of Software Quality 軟體測試的成本可以分為兩種，Control Costs(控制成本)，Failure of Control Costs(失敗控制成本) Control Costs: Prevention costs(預防成本): 包括投資於品質基礎設施和品質活動的費用，這些投資並未針對特定的項目或系統，而是對整個組織通用 Appraisal costs(評估成本): 包括為特定項目或系統執行的活動的費用，目的是為了檢測軟體錯誤 Failure of Control Costs: Internal failure costs(內部失敗成本): 包括修正設計審查、軟體測試和驗收測試中檢測到的錯誤的成本，在軟體安裝到客戶端之前完成 External failure costs(外部失敗成本): 包括修正客戶或維護團隊在軟體系統安裝後檢測到的所有失敗的成本 應該保持在 Optimal software quality level 這個標準之上，Control costs 減少不會讓軟體品質的總成本下降 Test coverage criteria(測試覆蓋率標準)，這是一種衡量軟體測試深度的指標，用於確定已經測試了軟體的哪些部分，以及還有哪些部分尚未進行測試，它可以幫助我們確定何時可以停止軟體測試 軟體品質成本影響軟體品質水平，投入確保軟體品質的資源會直接影響軟體的最終品質 例如: 如果我們投入更多的資源進行測試，那麼可能會發現更多的錯誤，從而提高軟體的質量 根據可用的軟體品質資源來確定何時停止軟體測試， 例如: 如果我們的資源有限，那麼我們可能需要在達到一定的測試覆蓋率後就停止測試 NOTE 本篇只是講述軟體測試的概論，後續會再討論各個章節的細節 Last edit 09-24-2023 15:50"
  },"/jekylls/2023-09-21-lexical_analysis.html": {
    "title": "Compiler | Lexical Analysis Notes",
    "keywords": "Compiler Jekylls",
    "url": "/jekylls/2023-09-21-lexical_analysis.html",
    "body": "Compilers course notes from CCU, lecturer Nai-Wei Lin. Lexical analysis(語彙分析) 將文本轉換為有意義的語彙標記(Token)，這通常是 Compiler 步驟的第一步。 Compilers: Principles, Techniques, and Tools 介紹使用 Regular Expression(RE, 正規表達式)描述 Lexemes 的方法，並透過一個 Lexical-analyzer generator(語彙分析器生成工具)來進行代碼生成， 使我們可以專注在如何描述 Lexemes。 因此會先學習 RE 的使用方法，RE 能被轉換為 Nondeterministic Finite Automata(NFA, 非確定有限狀態自動機)問題在轉換為 Deterministic Finite Automata(DFA, 確定有限狀態自動機)問題， 之後就能用程式碼模擬自動機運作。 3.1 The Role of the Lexical Analyzer Lexical analyzer(語彙分析器)主要任務就是讀取 Source code 的輸入字符(characters)，並將其組成為 Lexeme，並輸出一個 Token 序列，並在識別到 Identifier 時要將其添加到 Symbol table 中。 下圖顯示一個 Syntax analyzer(語法分析器)與 Lexical analyzer 互動的過程，呼叫 getNextToken 來使語彙分析器不斷讀取字符，直到識別出下一個 Token 將其返回給語法分析器。 語彙分析器可以被劃分為兩個骨牌效應的過程: 掃描不需要轉變為 Token 的部分的過程 例如: 過濾 Comments, Whitespace (blank, newline, tab …) 實際的 Lexical analysis，從掃描的輸入中產生 Token 語彙分析器還可以將 Compiler 的錯誤訊息與 Source code 的發生位置聯繫起來，例如: 紀錄換行符號的行數，以便在出錯時給予一個行數 某些編譯器中會將 Source code 複製一份，並將錯誤訊息插入該位置 3.1.1 Lexical Analysis Versus Parsing 把 Lexical analysis(Scanning) 與 Syntax analysis(Parsing) 分開有三個原因: 簡化編譯器設計，分離可以更好的專注在不同任務上 如果我們正在設計一種新的語言，將詞法和語法問題分開也可以使整體語言設計更加清晰 提高編譯器的效率 Lexical analyzer 獨立後我們就可以去更方便的優化 I/O 的處理 提高編譯器的可移植性，輸入設備特定的特性可以限制在詞法分析器中 例如: Windows 的換行符是 \\r\\n，Linux 上的是 \\n 延伸閱讀 Input Buffering in Compiler Design 3.1.2 Tokens, Patterns, and Lexemes 在討論 Lexical analyzer，這裡有三個需要了解的術語: token(language): a set of strings if, identifier, relop Pattern(grammar): a rule defining a token if: if identifier: letter followed by letters and digits relop: &lt; or &lt;= or = or &lt;&gt; or &gt;= or &gt; Lexemes(sentence): a string matched by the parrern of a token if, Pi, count, &lt;, &lt;= 假設有以下 Clang code，依照 Figure 3.2 print 與 score 是 Token id 所匹配的 Lexeme，\"Total = %d\\n\" 則是與 literal 匹配的 Lexeme。 printf(\"Total = %d\\n\", score); 在很多程式語言設計中，大部分 Token 被分成以下幾類: Reserved words(保留字)都有一個 Token，保留字的 Pattern 與保留字相同 Operators 的 Token，可以表示單個運算符，也有像 comparison 有多個同類別的運算符 Identifier 只用一種 Token 表示 Constants 有一個或多個 Token，例如 number、literal Punctuation symbol 都有各自的 Token，例如 (, ), ,, ; 3.1.3 Attributes for Tokens Attributes 是用來區分 Token 中的不同 Lexeme，例如 0, 1 都能跟 Token number 匹配，因為 Lexcial analyzer 很多時候不能僅返回給 Syntax analyzer 一個 Token name， Token name 影響 Syntax analyzer，而 Attributes 會影響 Parsing 之後的 Semantic analyzer。 &lt; if, &gt; &lt; identifier, pointer to symbol table entry &gt; &lt; relop, = &gt; &lt; number, value &gt; 3.3 Specification of Tokens Token 的一種重要的表示方式(規格)就是 Regular expression，RE 可以高效的描述處理 Token 時要用到的 Pattern。 3.3.3 Regular Expression Regular Expression(RE, 正規表達式)是由較小的 RE 按照以下規則遞迴建構，下面的規則定義了某個 Alphabet ∑ 的 RE: ε 是一個 RE，L(ε) = {ε}，也就是該語言只包含空字串 如果 a 是 ∑ 中的符號，那麼 a 也是一個 RE 代表 L(a) = {a}，也就是說這個語言僅包含長度為 1 的字串 a。 Suppose r and s are RE denoting L(r) and L(s) (r) (s) is a RE denoting L(r) ∪ L(s) (r)(s) is a RE denoting L(r)L(s) (r)* is a RE denoting (L(r))* (r) is a RE denoting L(r) Example: a | b {a, b} (a | b)(a | b) {aa, ab, ba, bb} a* {ε, a, aa, aaa, ...} (a | b)* the set of all strings of a's and b's a | a*b the set containing the string a and all strings consisting of zero or more a's followed by a b Order of operations: Priority Symbol Highest \\ High (), (?:), (?=), [] Middle *, +, ?, {n}, {n,}, {n,m} Low ^, $ Second lowest concatenation Lowest | Algebraic laws: 3.3.4 Regular Definitions 為了方便表示，我們可能會給某些 RE 別名，並在之後的 RE 中使用符號一樣使用這些別名，例如: Name for regular expression $d_1 \\rightarrow r_1$ $d_2 \\rightarrow r_2$ $…$ $d_n \\rightarrow r_n$ $where\\;r_i\\;over\\;alphabet\\cup ( d_1, d_2, …, d_{i-1} )$ Examples: $letter \\rightarrow A | B | … | Z | a | b | … | z$ $digit \\rightarrow 0 | 1 | … | 9$ $identifier \\rightarrow letter(letter | digit)*$ 上面的 Examples 定義了一個僅能由 letter 開頭但的 identifier 3.3.5 Extensions of Regular Expressions RE 後續有其他的擴展，用來增強 RE 表達字串的能力，這裡會介紹最常被使用的幾種擴展 One or more instances (r)+ denoting (L(r))+ r* = r+ | ε r+ = rr* Zero or one instance r? = r | ε Character classes [abc] = a | b | c [a-z] = a | b | … | z [^a-z] = any character except [a-z] Examples: $digit \\rightarrow 0 | 1 | … | 9$ $digits \\rightarrow digit^+$ $number \\rightarrow digits(.digits)?(E[+-]?digits)?$ 上面的 Examples 定義了從 digit 到 digits 最後到 number 的過程 3.6 Finite Automata 這裡 3.6/3.7 章不會依照課本順序，而是依照課程進度。 NFA 的 Transition function 可以指向多個 State，DFA 的 Transition function 只能指向一個 State 會先介紹相對簡單的 DFA 再介紹 NFA，這樣可以更容易理解 NFA 的運作 要注意自動機的幾個特性: 自動機是 Recongnizer(識別器)，他們只能對輸入的字串進行判斷 “Yes” or “No” Finite automata 分為兩類 Nondeterministic finite automata (NFA, 非確定有限狀態自動機) A symbol can label several edges out of the same state, the empty string(ε) is a possible label. Deterministic finite automata (DFA, 確定有限狀態自動機) For each state, and for each symbol of its input exactly one edge with that symbol leaving that state. 3.6.1 Nondeterministic Finite Automata An NFA consists of: A finite set of states A finite set of input symbols, default empty string is not in the set. A transition function (or transition table ) that maps (state, symbol) pairs to sets of states A state distinguished as start state A set of states distinguished as final states 上圖左是 NFA’s transition graph 在狀態 0 有 a, b 兩種狀態轉移，圖右是他對應的範例 3.6.2 Transition Tables NFA 可以表示為一張 Transition table(轉換表)，例如: 轉換表可以更容易看出 NFA 的狀態轉移，缺點是當 NFA 狀態(Alphabet)很多時，轉換表會變得很大佔用空間 3.6.3 Acceptance of Input Strings by Automata NFA accept 輸入字串 s，如果從 Start state 開始，有一條路徑可以走到 Final state，這條路徑的轉移符合這個 Automata 所定義的語言 3.6.4 Deterministic Finite Automata 這裡會先談一個 DFA 怎麼用程式碼模擬，因為相較於 DFA 簡單許多 Deterministic finite automata (DFA, 確定有限狀態自動機) 是 NFA 中的一種特例，其中: There are no moves on input ε For each state s and input symbol a, there is exactly one edge out of s labeled a. Algorithm 3.18 : Simulating a DFA. from Compiler: Principles, Techniques, and Tools p.150 Input: An input string ended with eof and a DFA with start state s 0 and final states F. Output: The answer “yes” if accepts, “no” otherwise. begin s := s0; c := nextchar; while (c != EOF) do begin s := move(s, c); c := nextchar; end; if (s ∈ F) then return \"yes\"; else return \"no\"; end; 3.6.5 Simulation of an NFA NFA 與 DFA 在模擬上的演算法幾乎一樣，最大的區別在於 ε-closure() 的建構，因為 NFA 在給定輸入的狀況下可以存在多個 State， 因此在模擬上需要處理 State set。課本中會在 3.6.4 提前介紹 Algorithm 3.18 : Simulating a DFA. - p.151 Algorithm 3.22 : Simulating a NFA. from Compiler: Principles, Techniques, and Tools p.156 Input: An input string ended with eof and an NFA with start state s 0 and final states F Output: The answer “yes” if accepts, “no” otherwise. begin S := ε-closure({S0}); c := nextchar(); while (c != EOF) do begin S := ε-closure(move(S, c)); c := nextchar(); end; if (S ∩ F != ∅) then return \"yes\"; else return \"no\"; end; 上面的 Pseudocode 模擬 NFA 的運作，其中: move(s, c): 從 state s 輸入 c 可以到達的 NFA state set move(S, c): 從 state s set S 輸入 c 可以到達的 NFA state set ε-closure(s): 沒有輸入字元，從 state s 僅通過 ε-transitions 可以到達的 NFA state set ε-closure(S): 沒有輸入字元，從 state s set S 僅通過 ε-transitions 可以到達的 NFA state set nextchar(): 回傳下一個輸入字元 注意上面的 S 是 NFA state set，而 s 是 NFA state 上圖左邊的最後的 S 與 Final state {3} 有交集，因此回傳 “yes”，右邊的則沒有交集，回傳 “no” 注意上圖的 NFA 並沒有加入 ε-closure() 因為沒有任何 ε State，因此可以只透過 move() 來模擬 NFA 運作 3.6.6 Computation of ε-closure 從上面的例子可以說明 move() 是如何運作，接下來這裡會講解 ε-closure() 是如何運作，用一個 DFS 來找出所有可以到達的 ε-State，返回一個 T set Computing ε-closure(T) Input: An NFA and a set of NFA states S. Output: T = ε-closure(S). begin push all states in S onto stack; T := S; while stack is not empty do begin pop t, the top element, off stack; for each state u with an edge from t to u labeled ε do begin if u is not in T then begin add u to T; push u onto stack; end; end; return T; end; 上面的例子看似複雜，其實只是組合了 move() 和 ε-closure() 的運作 3.7 From Regular Expressions to Automata 從 RE 轉換為 NFA，再從 NFA 轉換為 DFA，這裡會用這樣的順序來介紹 3.7.1 Construction of an NFA from a Regular Expression 使用 McNaughton-Yamada-Thompson construction algorithm，可以將 RE 轉換為 NFA。 以上說明了 (ε), (a), (s|t), (st), (s), 的轉換過程，跟使用 (a|b)abb 作為例子來一步步轉換 在 Compilers: Principles, Techniques, and Tools p.161 - Example 3.24 有類似的轉換過程 3.7.2 Conversion of an NFA to a DFA 使用 Subset construction algorithm，可以將 NFA 轉換為 DFA。 Subset construction 的概念是 DFA 的每個 State 都對應 NFA 的一組 State，也就是 DFA 的每個 State 都代表 NFA 在讀取相同輸入後可能存在的所有狀態。 但是這樣的話 DFA 的 State 數量會變得非常多，因此 Subset construction 會將相同的 NFA State set 合併成一個 DFA State。 a DFA state ≡ a set of NFA states Find the inital state in the DFA Find all the states in the DFA Construct the transition table Find the final state of the DFA 例如一個 NFA 有 3 個 State，那麼他的 DFA 最多會有 23 = 8 個 State 才能表示所有的 NFA State set， 但是在實際的語言處理中通常不會看到這種指數增長，並非所有的 NFA State 組合都會出現在實際的輸入序列中。 Algorithm 3.20 : The subset construction of a DFA from an NFA. from Compiler: Principles, Techniques, and Tools p.153 Input: An NFA N. Output: A DFA D with states Dstates and trasition table Dtran begin add ε-closure(s0) as an unmarked state to Dstates; while there is an unmarked state T in Dstates do begin mark T; for each input symbol a do begin U := ε-closure(move(T, a)); if U is not in Dstates then add U as an unmarked state to Dstates; Dtran[T, a] := U; end; end; 上面是一個將 NFA 轉換為 DFA 的例子 透過 ε-closure(), move($state, $symbols) 找出所有的 NFA State set 將相同的 NFA State set 合併成一個 DFA State 這樣就能透過 DFA State 來繪製出一張 DFA 3.7.3 Tiem Space Tradeoffs RE to NFA, simulate NFA time: O(|r| * |x|), space O(|r|) RE to NFA, NFA to DFA, simula time: O(|x|), space: O(2|r|) Lazy transition evaluation transitions are computed as needed at run time; computed transitions are stored in cache for later use. Lazy evaluation(惰性求值)，目的是要最小化計算機要做的工作。可以在空間複雜度上得到極大的優化，從而可以輕易構造一個無限大的數據類型。 Last Edit 10-02-2023 17:50"
  },"/jekyll/2023-09-20-compiler_introduction.html": {
    "title": "Compiler | Compilers Introduction",
    "keywords": "Compiler OS Jekyll",
    "url": "/jekyll/2023-09-20-compiler_introduction.html",
    "body": "Compilers course notes from CCU, lecturer Nai-Wei Lin. 編譯器這門課可以讓人更深入的了解 Programming language，如果能知道編譯器如何將 High level language 轉換為 Machine code 與背後的工作原理， 就能更有效的去編寫程式。在修 OS 的時候更有感覺，有些是針對編譯器與平台的優化去更改寫法，有些小小的改動就能減少數行的指令去提升效能。 linux/lib/rbtree.c 在 6.4 版做了一個非常簡單的 commit，將 bitwise | 換成 +，這個替換使 x86 平台上可以使用 lea Assemble， 將兩道指令變成一道指令。正是了解 Compiler Optimization 才能知道這樣修改有什麼用。 Human use nature languages to communicate with each other Human use programming language to communicate with computers 1.1 Language Processors 廣義的說 Compiler 就是一個可以將一個 Language 翻譯成另一個 Language 的工具，同時 Compiler 的另一個重要功能是發現翻譯過程中 Original language 的錯誤。 上圖展示了一個 Compiler 與 Interpreter 的差異，另外 Java language process 結合了兩者的過程，既有 Target code 也有用於執行程序的 Interpreter。 但是一個程式語言從 Compile 到 Execute 除了 Compiler 還有很多其他的處理程序，如: Preprocessor、Assembler、Linker、Loader。但這裡專注於 Compiler 的部分， 在 1.2 再詳細說明 Compiler 的結構。 1.2 The Structure of a Compiler 首先我們可以把 Compiler 分為 Frontend/Backend 兩個部分: Analysis(Front-End): 將 Source code 分解成多個組成要素，並在這些要素之上加入語法結構。使用這個結構來建立 Intermediate code，並且可以檢查原始程式是否符合正確的語法與語意，並且提供資訊給使用者修改。 並且把 Source code 的資訊收集為 Symbol table，之後將 Intermediate code 與 Symbol table 一起送給後端。 Synthesis(Back-End): 根據 Intermediate code 與 Symbol table 來建立目標程式 上圖都是 Compiler 的結構，右圖表述了每個步驟之間的更多細節 有些編譯器在前端與後端之間會有 Machine-independent optimization(機器無關的最佳化)步驟，這個最佳化的目的是在 Intermediate code 之間進行轉換。 1.2.1 Lexical analysis Lexical analysis(語彙分析)也被稱做 Scanning(掃描)，是編譯器的第一個步驟，進行讀取原始程式的字元串流，並依據 Lexeme(詞素)來產生 Token(詞彙單位)作為輸出。 Lexeme: 是 Source code 中具有相同意義的字符序列，如 int, return, = 都是 Lexeme。 Token: 是 Lexical analysis 後的結果，它的形式可能像 &lt;token-name, attribute-value&gt; 例如 Position = initial + rate * 60 在經過 Lexical Analysis 後會變成: &lt;id, 1&gt; &lt;=&gt; &lt;id, 2&gt; &lt;+&gt; &lt;id, 3&gt; &lt;*&gt; &lt;60&gt; 這樣的 Token 其中 Position 對應 &lt;id, 1&gt;，id 代表 identifier，而 1 指向 Symbol table 中所對應的條目 1.2.2 Syntax analysis Syntax analysis(語法分析)也被稱做 Parsing(解析)，使用 Lexical analysis 產生的 Token 來建立 Syntax tree。之後會介紹 Context free grammar 來描述程式語言的語法結構， 並自動為某些類型的語法建構高效率語法分析器的演算法。 1.2.3 Semantic analysis Semantic analysis(語意分析) 使用 Syntax tree 和 Symbol table 中的資訊來檢查原始程式是否符合程式語言的規則，並且在這裡收集型別的資訊。 Type checking: 這是 Semantic analysis 的重要部分，檢查每個運算子是否具有一致的運算元。例如: Array 的 Index 應該要為 int，若有 float 就應該回報錯誤。 Coercion: 程式語言也可以做型別轉換，例如 Position = initial + rate * 60，而所有變數都已經宣告為 float，此時就能將 60 轉換為 60.0。 1.2.4 Intermediate code generation 在 Source code 變成 Target code 的過程中可能會產生一個到多個的 Intermediate representation(IR, 中間表述)也可以稱作為 Intermediate code(中間碼)， Syntax tree 也可以算做是一種 IR，這些中間表述應該要有兩個重要的性質: Easy to produce(易於生產), Easy to translate(易於轉譯為 Machine language) 例如使用類似 Assembly language 的一種三位址碼作為 Intermediate code: t1 = inttofloat(60) t2 = id3 * t1 t3 = id2 + t2 id1 = t3 使用 Intermediate code 還能使我們更好的分離前端與後端，並且也增加了移植性與優化的可能性 使用多層的 IR 可以使每層都專注在不同的目標上，這樣可以使編譯過程分隔後更易於模塊化 如果不使用 Intermediate code，我們可能要去應對多種對應不同平台的轉換 1.2.5 Code optimization Code optimization 的目的在於將程式碼變得更「好」，這裡指的並不只是效能上的提升，例如更短或者占用資源更少的目的碼。 分為 Machine-independent(機器無關) 和 Machine-dependent(機器相關)的最佳化: Machine-independent: 發生在 Compiler 的中間階段，也就是生成 Intermediate code 的時候可以進行優化。這種優化並不依賴於特定的平台，因此可以在不同的硬體平台上重用。 Machine-dependent: 發生在 Compiler 的最後階段，也就是將 Intermediate code 轉化為 Target code 的時候進行優化，此時就要考慮不同的機器有不同的 CPU 架構與指令集， 此時就能利用平台的特性來幫助優化。 例如 1.2.4 展示的三位址碼，我們可以對其進行平台無關的優化，直接將 60 轉為 60.0 替代整數就可以消除 inttofloat 運算，並可以少去 t3 = id2 + t2 的運算， 把值直接傳給 id1，這樣就能得到一個更短的 Intermediate code。 t1 = id3 * 60.0 id1 = id2 + t1 Compiler Optimization 通常會另外開一門課特別講述，目前越來越強大的現代編譯器所做的程式碼最佳化已超出許多人預料。延伸閱讀: 你所不知道的 C 語言：編譯器和最佳化原理篇 1.2.6 Code generation Code generator(代碼生成器)將會以 Intermediate code 作為輸入，並將其映射至 Target code，例如 Assembly language。 Target code 若是 Assembly language，就必須為 Intermediate code 的變數分配 Memory address 或 Register A crucial aspect of code generation is the judicious assignment of registers to hold variables 例如 1.2.5 的優化過後的中間碼，這裡進行翻譯成組合語言 LDF R2, id3 // id3 的內容載入 R3 Regiester MULF R2, R2, #60.0 // R2 與 60.0 進行乘法運算 LDF R1, id2 // id2 的內容載入 R2 Regiester ADDF R1, R1, R2 // R1 與 R2 的值相加存到 R1 STF id1, R1 // R1 的內容存入 id1 中 這裡忽略了對於 Identifiers 儲存分配的問題，在後面會討論到 1.2.9 Compiler-construction tools 跟其他軟體開發一樣，開發 Compiler 也可以利用許多現代開發工具，除了通用的軟體開發工具之外也有一些更加針對 Compiler 的工具。 Scanner generators: 可以根據一個語言的 Lexemes 的正規表達式(Regular Expression)描述來生成語彙分析器 Lex, Flex Parser generators: 可以根據一個程式語言的語法(Context free grammars)描述自動生成語法分析器 Yacc, Bison Syntax-directed translation engines: 用於 Traversal syntax tree 並使用 Attribute grammars 生成中間代碼 Code-generator generators: 根據中間語言翻譯成目標機器的機器語言的規則(Tree grammars) 來生成代碼生成器 Data-flow analysis engines: 可以幫助收集 Data-flow(程式中的資料傳遞)，是 Compiler 優化的重要部分 Compiler-construction toolkits: 可用於構造編譯器不同階段的工具 1.3 Formal Language Theory Compilers: Principles, Techniques and Tools 書中 1.3 談論的是程式語言歷史，這裡改為討論語言的定義與自動機。 1.3.1 Language definition 在談論 Formal Language(形式語言)前首先要談的是 Alphabet, String, Language 的不同定義: Alphabet: a finite set of symbols. {0, 1}: binary alphabet String: a finite sequence of symbols from the alphabet. 1011: a string of length 4 ε: the empty string Language: a set of strings on the alphabet. {00, 01, 10, 11}: the set of strings of length 2 ∅: the empty set 對於 String 與 Language 有以下的基本運算: 1.3.2 Grammars &amp; Metalanguage Grammars: The sentences in a language may be defined by a set of rules called a grammar. 例如有語法規則 G: the set of binary strings of length 2 那麼 L : {00, 01, 10, 11} 就是符合該語法規則的句子 Metalanguage : a language used to define another language 如果透過一種語言來定義另一種語言，那麼該語言(Metalanguage) 必須是有明確的規則才能清楚作出清楚的定義，這樣才有可能實作下個階段的 Automata 1.3.3 Automata 我們能在 Compiler 中需要實作的就是 Automata，Automata 往往與 Formal language 密切關聯，自動機被用作可能是無限的形式語言的有限表示。 因此可以在實作上透過 Automata 使語言輸入並通過(Accept) 與 (Transform)轉換。 Acceptor(接受器): 一種自動機，用 Grammar 確定輸入的字符串是否為該語言的句子 Transducer(轉換器): 一種自動機，依照 Grammar 的定義來轉換輸入的字符串成為另一種語言。 狀態機透過 State, Event, Output, Input 來達成如何精確地描述和處理可能無窮大的信息集合。 1.3.4 Compiler-Compiler 既然 Compiler 是透過 Grammars 來進行對一種語言的通過(Accept) 與 (Transform)轉換，這個 Grammars 必定是一種精確的規格(Specification)， 這就讓我們可以透過 Specification 來撰寫 Automata，使我們可以透過 Grammars 來自動生成(Generate automatically) Automata 那麼定義 Grammars 的元語言(Metalanguage) 必然也是有精確的規則存在，那我們當然也可以透過 Matelanguage 來進行 Compiler 的自動生成， 這就是 Compiler-Compiler(編譯器的編譯器) 或 Compiler-Generator(編譯器生成器) 使用不同的 Matelanguage 來定義 Compiler 不同階段的元件，我們就能以此來自動生成這些元件 這是我們在各個階段可以使用的 Matelanguage，以及透過這些 Matelanguage 我們可以怎麼去實作 Automata Lexical syntax: Regular expression: finite automata, lexical analyzer Syntax: Context free grammars: pushdown automata, parser Semantics: Attribute grammars: attribute evaluators, type checker Intermediate code generation: Attribute grammars: intermediate code generator Code generation: Tree grammars: finite tree automata, code generator NOTE This is first note of compilers, focus on the introduction of compilers. Last edit 09-23-2023 13:32"
  },"/jekyll/2023-09-12-operating_system_introduction.html": {
    "title": "OS | Operating System Introduction",
    "keywords": "OS Jekyll",
    "url": "/jekyll/2023-09-12-operating_system_introduction.html",
    "body": "Operating System: Design and Implementation course notes from CCU, lecturer Shiwu-Lo. Introduction 1.1 Why need OS 1.3 User mode/Kernel mode 1.5 User space/Kernel space 1.6 Memory management 1.7 Change mode &amp; System call 1.8 Signal &amp; Systemcall 1.9 Monolithic kernel 1.10 Kernel module 1.1 Why need OS 作業系統使電腦更易於使用 磁碟是由 Block(通常為4K) 所組成，OS 將磁碟劃分為 File，再將檔案歸類為 Folder 才易於使用。 可程式化(Programmable) 變得更容易，OS 能執行執行檔，CPU 執行不同程式碼就會有不同功能。 OS 使程式碼抽象為執行檔，能夠從各個地方載入可執行的程式碼，並且賦予邏輯上的支援。 硬體抽象化(Hardware abstraction) 如滑鼠、觸控板被 OS 抽象化為一個指標裝置，使使用者能統一操作。 OS 上不會只運行一個程式，因此必須有應用程式之間的通訊，如: Copy &amp; Paste。 使電腦的硬體使用更有效率(資源分配) 一台電腦可能有多個硬體存在，OS 可以使這些硬體一起工作。 例如在足夠記憶體的情況下可以同時載入多個執行檔並執行，並使用硬碟堆放暫時用不到的記憶體，以空出記憶體給真正需要的程式。 透過 CPU Scheduler，使 I/O、CPU 都能維持在高使用率。 1.3 User mode/Kernel mode 大部分的作業系統以雙系統(Dual-Mode)(Linux, Windows)，可分為 User mode 與 Kernel mode: Dual mode operation User mode: CPU 所提供的執行模式，只能存取有限的硬體資源，如: 普通運算所需的暫存器、部分記憶體內容 Various applications: libreoffice, gnuplot, pdf viewer GUI: X11, Gnome, KDE System manager: bash, vi, ls, mount, passwd Development tools: gcc, gdb System service: sftpd, sshd Basic inetrnet communication software: Browser, ftp Other library: courses, math Standard function library: Parts defined in POSIX like, libC, pthread Kernel mode: CPU 所提供的執行模式，可以對硬體做任何的變更，在 Kernel mode 能額外控制的部分如下: 控制暫存器(Control register)，例如控制 MMU(Memory management unit) 的相關暫存器、所有記憶體 Memory management Schedule and thread management Inter-Process communication Virtual memory Network communication Scheduler File system Safety, Authority management I/O Subsystem 這種模式下 Kernel mode 才能完全的掌控硬體，今天如果 User mode 上的程式想要存取硬碟則需要透過 System call 來進行操作。在 Linux 的設計原則是速度第一，當然程式越靠近硬體就會更快。 但如果 User mode 想要操作硬體要透過 System call 來進行也就是改變模式(Change mode)，但這樣會產生一定的消耗。而在 Kernel mode 中進行操作硬體就只等於 Function call 的消耗而已。 User mode 想要進行切換就可能需要保存當前狀態以回復、清除 Pipeline、清除 TLB 和 Cache 這些額外消耗。 Dual mode 通常需要硬體額外支援，例如提供一個 Mode bit 的暫存器來決定現在是哪一種 Mode。 例如一個網頁，可以將靜態頁面放在 Kernel mode，動態頁面在 User mode。 但不是將所有程式都放在 Kernel mode，因為只要是程式就會有 Bug，在 Kernel mode 中發生了 Bug 很可能導致整個系統的崩潰。 1.5 User space/Kernel space 虛擬記憶體(Virtual memory)也分為 User/Kernel space，這主要是為了保護 Memory 與 Hardware。 CPU 在切分記憶體時每個單位會附加一些屬性，其中一個重要的屬性就是指出該單位為 User/Kernel space。 Task 之間不能讀取各自的 User space。 Kernel 才能改變權限，I/O，並且擁有所有的存取權。 可否存取 Kernel space User space Privilege instructions Kernel mode ✓ ✓ ✓ User mode   ✓   1.6 Memory management Virtual memory 的管理單位可以分為兩種，分頁(Paging)，分段(Segmentation): Paging: 將連續的記憶體在邏輯上變成 4K 大小的 Page 方便軟、硬體對記憶體進行管理 這是目前最常見的做法，在管理上可以較好的分配記憶體 作業系統會盡量使用 Huge page(大分頁)，一個 Huge page 可以有 2M 到 1G 的大小，因為這樣需要的 Page 數量較少， 在硬體管理上會希望 Page 盡量大一點。 但是 Linux 在 User space 幾乎沒有使用大分頁，除非特別去設定要使用 Huge page，因為在軟體上管理 Page 反而 4K 可以減少記憶體浪費。 Segmentation: 將連續的記憶體在邏輯上變成各個大小不一的 Segment，每個 Segment 對應到程式的特定用途 x86 在 32 位元模式支援 segmentation Segment 在配置上比較困難，但在嵌入式系統中因為沒有什麼動態配置的需求因此較常用 以上的方法都需要硬體支援，是因為 CPU 與硬體的處理速度因此需要硬體支援才能使速度提升。 1.7 Change mode &amp; System call 從 User mode 切換到 Kernel mode，是透過 syscall 這個組語來進行: OS init 時會告訴 CPU 當使用者呼叫 syscall 的時候，指令指標(%RIP Register)應該設定為何(system call 的進入點)，syscall 做兩件事: 保存當前的程式狀態，以便之後返回 將模式切換為 Kernel mode，例如將 Mode bit 設為 0 準備要被呼叫的 System call，例如在x86-64架構中的 %RAX Register 放入要被呼叫的 System call 編號 切換 Stack: User space stack 一開始只配置 16K，不夠再送 Signal 給 Kernel 一次多要 4K Page 最多成長到 8M Kernel space stack 的大小並不會很大，並且 System call 也不會使用產生堆疊的寫法 User/Kernel mode 的堆疊是分開的，Kernel 不能產生 fault，所以在 Kernel mode 使用的是 Kernel space stack 穩定性: 例如 User mode 已經故意執行了一個 7.8M 堆疊的程式，此時 Change mode 但沒有使用 Kernel space 這樣就會遇到 Segmentation fault 保密性: 同時 User space stack 如果執行完沒有進行清空也有可能洩漏 Kernel 的資料 在 syscall 準備 system call 的時候同時還要準備執行環境，例如執行 write(clang) 就要準備好要使用的 stack 來呼叫 C 函數 如果在沒辦法切換堆疊的系統上，也要盡量表留一個給 Kernel mode 使用的堆疊大小 16K 從 Kernel mode 回到 User mode，需要用到 sysret 指令來進行，此時 OS 會返回地址(接下來執行的 User mode 程式碼)，sysret 做兩件事: 返回 user mode 的程式碼位置，例如 x64 放在 %RCX Register 切換為 user mode，例如將 Mode bit 設為 1 不是所有的 System call 都會進行完整的 Context switch，例如 getpid()，就不需要把當前的程式狀態保存完整。 System call handler 必須確保從 Kernel 返回 User mode 時，程式將來可以繼續執行 Super user 也是使用相同的 System call 進入點，大部分的 System call 會判斷權限但 uid=0 的時候就直接通過，General user 的 uid &gt; 1000。 Linux 定義了約 400 System call(Function)，大部分都透過 syscall 進入 Kernel。 1.8 Signal &amp; Systemcall 當 Kernel 有特別事件需要主動通知 Process，就使用 Signal 機制，Signal 接受兩個重要參數: 事件編號、發生該事件時呼叫該程式所定義的特定函數 事件編號: 例如 SIGINT 的編號是 2，表示鍵盤中斷(例如 Ctrl + C)，每種 Signal 都有唯一的編號，OS 中已被定義 該事件發生時所處理的函數，可以是程式自己定義的函數或是系統默認的函數呼叫 Linux kernel 只允許一個 Process 同時間發出一個 Blocking system call，需要等待 Kernel 完成該工作(System call)才回傳的系統呼叫。 當 Process 發出 Blocking system call 後，該 System call 還未結束前如果發生 signal(例如 Ctrl + C)，Kernel 該如何處理? 不理會該 Signal，繼續完成 System call 處理該 Signal，該 System call 變成失敗，通常作業系統會重新起始(Redo)該 System call 延伸閱讀 Linux Signals、Interruption of system calls when a signal is caught 1.9 Monolithic Kernel 假如一個人一天可以讀 1000 行程式碼，Linux kernel 的成長速度可能是一天超過 3000 行。 Monolithic kernel 大部分的系統功能都設計在 Kernel mode 中，這樣的好處是執行效率，各個模組間的溝通僅為 Function call， 但同樣的 Kernel 也變得越來越複雜，也容易產生錯誤。 Micro kernel 盡可能的將 OS 的系統服務執行於 User mode，讓系統變的較為穩定， 但是 User mode 的 Process 通訊需要觸發 Context switch 與 Mode change，效能較為低落。 設計模式 系統程式 效能 核心大小 通訊消耗 OS Monolithic 大部分在 Kernel mode 高 大 Function call 的消耗 Linux, BSD Micro 大部分在 User mode 低 小 Context switch 的消耗 Minix, L4, Android Linux 在安全性思考上可以思考 root 的權限是否太大了，root 有權看到所有的資訊，在安全性上可以透過加密來做到。 Context switch &amp; Mode change 這兩個要分清楚: Context switch: 原本在執行 Process A，現在要換成 Process B Mode change: Process A 原本執行在 User mode，因為要執行 System call 現在要切換到 Kernel mode 1.10 Kernel Module Kernel moduel 在意義上就是核心的插件，可以擴充核心的功能，最直觀的插件就是驅動程式。 Monolithic 設計下的 Kernel module 運行在 Kernel mode，Micro kernel 則相反。 Kernel module 在需要的時候再載入，例如 USB 如果是是一個 ext4 file system， 插入電腦後 Linux Kernel 認出後就能載入 ext4 module 來讀取該 USB lsmod 列出目前的 Kernel module，並且 Kernel module 之間會有相依性，例如如果 Network 是一個寫好的 Kernel Module， 那它就會依賴於 Network card 的 Kernel module。 Main Memory 1.11 Main memory usage 1.12 Memory and storage consistency 1.13 Linux base file system management 1.11 Main memory usage Linux 將主記憶體分為三種主要用途: Cache memory(Page cache): 將 Disk 或 SSD 等儲存裝置上的內容暫存於記憶體中，以提高存取速度 檔案系統的 Metadata!? Buffer memory: 與 I/O 之間的的資料交換，DMA 主要是 CPU 的速度高於周邊速度，格式的轉換等等(如網路卡) Program memory: 將記憶體分配給程式使用，如: 程式可以透過 malloc(背後的 System call 常常是 brk)、nmap 等函數索取記憶體 執行檔必須載入主記憶體後才能執行 Linux 原則上會盡可能的使用掉所有記憶體以加速 I/O，當記憶體不足時 Linux 會釋放 Cache memory 和 Buffer memory。 1.12 Memory and storage consistency 主記憶體採用 DRAM 斷電後失去內容，因此需要非揮發性的第二層儲存裝置(Secondary storage，Disk, SSD)。 而電腦必須將第二層儲存裝置的內容載入記憶體後才可以供 CPU 運算 可以透過明確的程式碼存取資料，如: open, read, write 也可以透過 mmap 的方式將檔案 Maping 到應用程式的記憶體空間加以處理 程式與 OS 都必須定期的將資料回存到下層裝置 OS 與 I/O 函數庫都可能透過 Buffering 的機制，以批次的方式寫出資料以增加效率 應用程式可以呼叫函數強制將資料寫出，如: sync, fsync, fdatasyc 效能上的考量 read &gt; write，read 不能延遲，延遲將導致程式無法執行。 結果上的考量 write &gt; read，當使用者使用 Ctrl+S 進行儲存時，若在中途當機資料並沒有真的寫入 Disk，使用者將無法接受。 write 往下寫入時不一定馬上把資料寫入 Disk，有可能只是先暫時存在 Memory/Buffer 中，同時裝置上可能也會有 Buffer 例如 SSD。 這裡有例如 Write-through，Write-back 等不同方式的寫入是 OS 能進行操作的。 Write-through: 寫入 Main memory 時同時寫入 Cache 這樣可以保證資料的一致性 Write-back: 寫入數據時先寫入 Cache，並將資料標記為 dirty(已修改但未寫回主記憶體)，然後在稍後的時間點將資料寫回記憶體，例如: Cache 需要新的空間來儲存新的數據 深入思考，如果裝置上能有電池的話就能保證寫入時部份的安全性，如果已經寫入裝置上的 Buffer 此時電源出現意外也能短時間內進行儲存 延伸閱讀: Write-through vs Write-back 1.13 Linux base file system management 九字檔案權限管理，分為檔案與資料夾，詳情可看鳥哥的介紹 第七章、Linux 磁碟與檔案系統管理 Linux 的檔案系統支援 ACL(Access control list) ACL 可以對各個使用者或各個群組分別設定權限 ACL 依賴底層的檔案系統支援，如: EXT4, BTRFS 等都支援 ACL 可以使用指令 setfval 與 getfacl 分別設定及讀取檔案或目錄的權限 但是有時後 Normal user 也需要更改權限來執行某些程式，例如: passwd, 他會在執行中使用 setuid 來暫時改變使用者權限， 直到 passwd 執行結束(或者是該程式放棄 root 權限) Computer world 中的權限往往與現實世界的權限不相等 Linux 中 root 擁有最高權限，root 可以對檔案系統、應用程式(記憶體)進行各式樣的操作 root 可以存取所有使用者的檔案(甚至是機密檔案)，這與真實的權限不符合(老闆 = Normal，系統管理者 = Super) 目前大部分的 Linux 提供資料夾加密的功能，可以部分改善問題 I/O Subsystem 1.14 I/O Subsystem &amp; Control 1.15 I/O Subsystem transfers data 1.16 CPU and DMA compete for memory access 1.14 I/O Subsystem &amp; Control 周邊裝置可能有不同儲存資料的方式，例如滑鼠鍵盤可能僅需要內建暫存器，而網路卡、硬碟可能有自己的內部記憶體，儲存的資料與周邊的晶片來運作裝置。 I/O Subsystem 是 OS 的一個重要部分，負責管理程式的 I/O 請求。 I/O Subsystem 的控制流程通常如下: 向周邊下達命令 於記憶體及周邊之間做資料傳輸 通知處理器「工作已完成」 Memory mapped I/O 目前主流使用的 I/O 方式 將周邊的控制「暫存器、記憶體」映射到 CPU 的「記憶體映射空間(Memory space)」 例如使用指令: MOV CX, 0xFFFFFFFF; 假設 0xFFFFFFFF 是裝置記憶體，將 CX register 的值放到位置 0xFFFFFFFF 就等於向下儲存資料 上圖模擬一個可能的 MMIO，DRAM 與不同的 Device 分別被映射到不同的記憶體區段。裝置有自己的控制暫存器、 去設定讀取的指令來在裝置的晶片上執行，在裝置暫存器讀寫的程式就被稱作(Device driver)驅動程式 Port mapped I/O 目前在一些嵌入式處理器上使用，因為 x86 是較古老的架構因此也有支援 PMIO 使用特別的指令，將資料傳輸到特定的「Port」，注意: Port 和 Memory space 是分開的定址空間 x86-32 的 I/O Port 的定址空間只有 0~65535 x86-32 的記憶體定址空間只有 0~4G 使用的指令形式如下: out 0x255, AX; 將 AX register 寫到 0x255 Port，對裝置寫入資料 in AX, 0x100 從 0x100 Port 將資料寫到 AX register，將裝置資料取回 Device、Memory 分別用不同的定址方式，I/O Bus 為 64K 即最多 65536，Memory 則可以到最大 4G，這種架構下指定的速度通常比較慢，因為 MMIO 可以更簡單與直接的進行 I/O。 延伸閱讀 I/O對應的方式 1.15 I/O Subsystem transfers data 傳輸資料上，一般來說不會使用 CPU 進行，因為 CPU 要做更重要的事。DMA(Direct memory access) 是相對簡單的硬體， 專門用來做主記憶體對主記憶體的傳輸或裝置記憶體對主記憶體的傳輸。 簡單的 DMA 只要計數器，被搬移資料的開始位置，要搬入資料的開始位置，這樣三個 Register 就完成一個能搬移連續資料的 DMA DMA 可以屬於 Bus 的一部分，如: ISA，或者是裝置的一部分，如: PCI (DMA and Cache)Coherency problem 只要有兩種介面存取同一個儲存裝置就一定會有如何同步的問題 DMA 直接存取記憶體，但 CPU 透過 Cache 存取記憶體，這造成資料不一致(Cache 的資料一定比 Main memory 新) 從記憶體寫出資料到裝置時，必須將 cache 的資料 flush 到記憶體(寫入主記憶體) 從裝置讀取資料到記憶體時，必須先 invalid 相對應的 cache line Alignment DMA 傳輸的資料的開始及結束位置，通常要和 Cache(Cache line) 和 Memory 的寬度進行對齊 這部分的限制可能來自於 DMA Controller 直接將資料以「固定的大小」更新到 Cache。例如: x86 是 64 個 Byte，那傳輸資料就跟 64 Byte 對齊。硬體在這裡沒有特別做處理，使得非對齊的資料也可以部份更新 Disk 與 DRAM 之間的交換也要使用 buffer 來進行交換，才能避免低速讀寫去占用寶貴的 Bus 資源。當 bufdisk 將資料傳給 bufRAM 後此時要處理的就是 bufcache 與 bufRAM 之間的一致性 Cache 與 DMA 的資料不一致性解決之道 (DEV =&gt; CPU) 使用硬體解決，硬體自動會將 DMA 的傳輸更新到 Cache 內(Cache coherence algorithm) 某些處理器，例如早期的 ARM 處理器，這部分需要特別的指令設定該段記憶體的「屬性」 例如直接將該段資料直接從 cache 上完整移除(flush)，這樣就能確保 CPU 讀取記憶體時從 Main memory 讀取 用硬體來進行處理的話可以分段去處理，這樣就能在分段中偷偷傳輸資料，總有一些 Cycle 沒有被使用 (CPU =&gt; DEV) 設定讓 CPU 在該記憶體區段進行寫入時，使用 Write through 或 Noncacheable，直達裝置上的記憶體 Noncacheable: 直接將資料寫入該段記憶體而不透過 Buffer，這樣能確保 DMA 的資料是最新的，但會降低效能。 延伸閱讀 Cache和DMA一致性 1.16 CPU and DMA compete for memory access 如果 CPU 發生 Last-level-cache(LLC) miss 時，CPU 可能會合 DMA 爭奪存取權: 如果 CPU 訪問 LLC 時，如果發現沒有所需要的數據，CPU 可能會嘗試直接存取 Main memory，若此時 DMA 也正在進行存取， 就會產生爭奪存取權。 DMA 每次都只傳輸小量資料，那就可以很快的禮讓控制權給 CPU，但這樣的傳輸模式很沒效率 DMA 做大量傳輸可以提升 I/O 的效能，但可能會造成 CPU 等待 DMA 完成傳輸而閒置 延伸閱讀 Burst mode 目前在 PCI 上每個 Device 都有自己的 DMA Arbiter 用來分配不同的 Device 的存取權，通常是速度越快的裝置優先權越高 DMA 的其他議題 DMA 的定址空間 部分 DMA 的定址空間可能只有 32bit，OS 必須盡可能的將 DMA 能使用的記憶體保留給 DMA 使用 I/O MMU 主要讓 DMA 存取實體位置不連續的記憶體 避免惡意的裝置或驅動程式 DDIO (Data Direct I/O) 在某些 Intel 平台上，DMA 的傳輸可以跳過 DRAM 直接傳輸到 cache，例如: Xeon, DDIO 例如 ARM 上的 ACP(Access to Shared Caches)，即使用 DMA 對這個裝置進行操作， 其內容也會被同步到 cache memory 裡面，包括 L1 和 L2 cache Access to Shared Caches vs. Traditional methods I/O Subsystem notify the CPU 1.17 Interrupt hardware concept 1.20 Interrupt vector table 1.21 Interrupt Service Routine 1.22 Bottom half 1.23 Bottom half and Top half 1.24 Polling 1.25 Buffering OS 交付工作給周邊裝置後，於工作完成後通知 CPU 的方法可以使用: Interrupt Polling Interrupt + Polling 1.17 Interrupt hardware concept Interrupt 是一種改變程式正常執行流程的機制，可以由 Device 或者 CPU 本身產生 Legacy Interrupt 由實體線路構成，每個裝置連接到實體的中斷線，中斷線連接到 Programmable interrupt controller(PIC, 可程式化中斷控制器)，PCI 再向 CPU 的 INT 腳位發出中斷訊號 PC 中斷線也就共 15 條，但是 Device 通常不只 15 個，因此必須數個裝置共用一條中斷線 Message Signaled Interrupts 所有 Devices 共用一組中斷線路，裝置在中斷線路上寫入自己的中斷編號，就會觸發 CPU 中斷 這樣就類似在一條線上傳輸編碼，經過解碼器後推向 CPU，讓 CPU 對自己送出中斷，例如: PCI 的 MSI-X 支援 2048 個中斷編號 CPU 會設計好 IVT 的位置，由 OS 放入適當的 ISR 這部分也是驅動程式(Device Drver) 的一部分 1.20 Interrupt vector table Interrupt 處理流程: Interrupt Request: 當中斷請求發生時，將所有中斷 Disable，將 CPU 切換到 Kernel mode Store State: 暫停目前的 Process 並保存此 Process 的執行狀況 Interrupt Vector Table: OS 根據 Interrupt ID 查詢 IVT，並取得 Interrupt Service Routine 的開始位置 Interrupt Service Routine: 執行 ISR Restore State: 恢復之前 Process 的執行狀況 這時候不一定恢復原本的 Process，因為 System call 可能改變 Process 的狀態，由 Scheduler 來決定下一個執行的 Process(不一定是原本的 Process) IVT 放置的是 ISR 的開始位置 ISR 開頭都是用組語寫的，如果 C 的執行環境設定好也可以用 C 1.21 Interrupt Service Routine 發生 Interrupt 時是 Disable Interrupt，但在 ISR 的時候可以視情況決定是否要 Enable Interrupt，要允許哪寫 Interrupt，如果再 ISR 中 Enable interrupt 表示允許巢狀中斷(Nested Interrupt) Nested Interrupt: 「一個中斷，被另一個中斷給中斷」 Linux 中，ISR 只處理必須立即處理的部分，剩餘的部分交由 Kernel thread 處理。例如: ksoftirqd ISR 中不能呼叫任何會造成「wait」的函數，例如: semaphore 中的 wait() 從設計上 wait() 的主體是 task (can context swtich)，ISR 只是一個有自己堆疊的函數，不是 task 從邏輯上 ISR 是處理 I/O 中的「必要部分」(緊急性)，因此不應該 wait() 如果程式邏輯上必須要 wait()，就要考慮是否把這部分留給 Bottom half 解決 1.22 Bottom half Bottom half 可以分為三種 softirq, tasklet, work-queue Softirq 可以在多個 CPU 上平行運行，必須在編譯時靜態註冊 Tasklet 是建立在 softirq 之上的一種機制，tasklet 可以動態註冊和銷毀，相同類型的 tasklet 不能在多個 CPU 上同時運行 Work-queue 是一種完全不同的機制，work-queue 可以確保同一種類型的 driver 只會在同一個 CPU 上運行 從效能高低上是從左往右排序，從易寫程度上是從右往左排序 理論上這三個都應該由 Kernel thread 來呼叫 如果是 Kernel thread(task) 那就可以 wait() 實際上 softirq, tasklet 在 Linux kernel 可以由 Linux kernel 提供的一小段程式碼在 ISR 結束時呼叫 這樣的話他是執行在 ISR 中，這樣是不能 wait() 會設計成 ISR 也可以呼叫的原因是，如果這次 ISR Loading 並不重的時候就乾脆全部的工作都在 ISR 完成，這樣的消耗會比呼叫 Kernel thread 更少 在 Driver Architecture 篇章會更詳細的說明 ksoftirqd 最多寫 32 個驅動程式在裡面，只有高速裝置會掛在 ksoftirqd 例如: Network Card 延伸閱讀: Difference between SoftIRQs and Tasklets, tasklet, taskqueue, work-queue – which to use? 1.23 Bottom half and Top half Bottom half 的 softirq 每個 Core 就只有一個 ps -e | grep softirq 在這裡 Top half, Bottom half 分別指的是: Top half: 是指來立刻響應的 Interrupt 時處理的中斷函數，在這個階段執行快速且必要的硬體操作，如: 保存狀態，和呼叫 Bottom half Bottom half: 跟 Top half 的區別是執行期間 Interrupt 是啟用的 這裡進入 Bottom half 的工作會被包裝成一個 Struct 兩個 Pointer，包含 Function, Data，然後將這個 Struct 加入一個 Linked list， 之後等待 CPU 有空閒時 softirq 會從中取出 Struct 來執行 Bottom half。 Function pointer: 該函數定義了 Bottom half 實際的執行工作，包含: 處理資料，啟動另一個 I/O 等等 Data pointer: 指向工作相關的資料，硬體的資料，要計算的參數，需要修改的 Memory 等等 在 Linux 中，當 top half 決定將一些工作推遲到 bottom half 時，這些工作會被包裝成一個結構，該結構包含兩個指標：一個函數指標和一個資料指標¹。 1.24 Polling 如果系統的 Loading 很輕，並且系統請求的時間比較沒規則時 Interrupt 會比較好，但如果負載高 Interrupt 被不斷的送出， 這樣可能會導致 CPU 不斷的去處理 Interrupt，此時 Polling 會比較好 與 Interrupt 不同，Polling 是 OS 每隔一段時間主動去探詢裝置的狀態 如果數個 Device 共用同一條 Interrupt 那麼當 Interrupt 發生時，OS 必須 某些裝置同時支援 Interrupt 和 Polling，例如: Network Card，可以在負載量高/低時做切換 像滑鼠跟鍵盤就通常是採用 Polling，除非有特殊需求，例如: 電競滑鼠 1.25 Buffering and Kernel bypass read(fd, buf, 200) 會用掉四個 Register read(): 本身就是參數(No. 3 Interrupt, $AX 要設定為 3) fd: 從哪裡讀資料 buf: buffer 的起始位置，200 Buffer size read 呼叫後 Kernel 會配置 krl_buf，DMA 傳輸會將資料從 dsk_buf 搬移到 krl_buk 最後由 Kernel 將資料 Copy 到 Userspace Kernel bypass: 是指不需要透過 Linux kernel 的功能，使用自己實現的相同功能的程式碼直接將 Device 的資料 Copy 到 Userspace 這可以幫助解決在高 concurrent 下由於 Interrupt Handling, Memory Copy, Context switch, Locality miss, CPU Affinity, Memory Management 所造成的性能瓶頸 代表的技術有 DPDK, eBPF 等等 延伸閱讀: kernel-bypass 内核旁路技术介绍 Scheduler and File System 1.26 Process and Thread of Linux 在 Linux 提供下列 System call fork(), vfork() 產生 Process clone() 產生 Task(Process, Thread) execve() 將一個執行檔內的程式載入該 Task 中 對 Linux 來講 Process, Thread 都是 Task Linux 使用 Task struct 來描訴 Process, Thread Process, Thread 的差異只是「共享資源的多寡」，尤其是記憶體是否共用 fork, vfork, clone 都是呼叫 Kernel 中的 do_fork 有一些 Thread 只執行於 Kernel mode，Linux 稱為 Kernel thread 1.27 Scheduler 如果只說 Scheduler 通常指的是 CPU Scheduler 傳統上 Scheduler 希望達成以下目標 依照優先權賦予優先權的公平性 所有的 Task 都可以在合理的時間內，再次獲得 CPU 使用權 優化 I/O 效能，例如: 處理 I/O 的設備可以先執行，接下來工作交給 I/O 設備 Linux 更進一步的達到以下目標 Multi-core 上能做到 Load balance Scheduler 本身不至於造成 Multi-core 的效能瓶頸 對多媒體、遊戲有更好的支援，CFS Linux 還未達到的目標 Real-time system 的支援 Linux 在手機上的 Scheduler 比 iOS 的 Scheduler 更耗電(iOS 並不是真正的多工 OS) 1.28 File system 在檔案系統要注意到這些: 新技術如： SSD、管理的問題(大檔案、零碎檔案)、存取行為(循序/隨機存取)、檔案的重複性問題、混合硬碟(SSD\\HDD) Linux 使用 Virtual file system(VFS), 兼容多種檔案系統，大部分這些檔案系統都使用「i-node」的概念描述檔案 i-node: 用於描述檔案或目錄，裡面儲存資料的屬性和位置，例如: 所有者、許可權、大小、最後修改時間等 VFS: 透過統一的介面，使無論底層的檔案系統是什麼，都能透過介面處理，例如: open(), read(), write() File system: 用於管理底層裝置資料存取的機制，例如: ext4, NTFS, FAT32 … Linux 官方的 File system 是 ext4’ ext4： 是 Journaling file system，因此當發生意外，檔案系統不至於完全損毀，並且能快速恢復(fsck.ext4) Linux 支援 btrfs，btrfs 借用了 zfs 的很多概念，主要包括: snapshot, copy-no-write, hot plugging HDD/SSD, difference disk load balance, backup support Hardware progress 1.29 SMP and CMP 1.30 Single-ISA heterogeneous multi-core 1.31 UMA and NUMA 目前已經很難提升 CPU 的 Clock 相同架構下，提升 Clock 是最直接方式來的提升 Performance 提高 Clock 會遇到大量的熱，也是提升效能的瓶頸 Processor, Menory, Storage 等等藉由不斷的「複製，貼上」產生平行運作 平行運作往往需要軟硬體結合 很多時候，硬體提供「多種選擇」給軟體進行優化 寫平行化程式，程式碼每年可以有 23% 的效能提升，否則只有 4.6% 平行處理架構指的是「同時使用多個 Process(Core)」例如: PC 常見的 Multi-Core Process 他的優點: 提高產能: 在工作可以平行化時，產能將以倍數提升 成本考量: 相較於高時脈的 Processor，工作平行化後，數個時脈低的 Processor 能得到一樣的效果 增加可靠度: 高階伺服器可以提供 Processor, Memory 熱插拔的功能，例如: Linux, 可以在不停機的情況下更換 CPU 1.29 SMP and CMP Symmetirc multiprocessor(SMP, 對稱多處理) 每個 Processor 的地位是等價的，應用程式可以在這些處理器上做轉移(migration) 在這個架構上所有 Processor 都共享一個共同的 Main memory，有可能會導致記憶體存取衝突而影響效能 Chip multiprocessor(CMP, 單晶片多處理) 也稱作 Multi-core 將多顆 (Core)Processor 集成至一顆晶片 每個 Core 都有自己的 Cache 可以減少對共用 Memory 的存取需求 共享最下層的 Last level cache(LLC)，因此不同 Core 之間的轉移比較快速 Simultaneous multithreading(SMT, 同步多執行緒) 在一顆 Processor 上用硬體模擬出 N 顆 Process，硬體的主要成本是 Logical register 數量增加 N 倍 能更有效的利用 Processor 內部的 Function unit，如: 加法器，乘法器等 Intel 宣稱他們實現的 SMT(Hyper-threading) 增加 5% 的晶片面積，可以獲得 15% ~ 30% 的效能提升 AMD 於 2017 推出的 Zen CPU 也實現了 SMT 上方右圖有兩個 Instruction flow(Register set) 去競爭資源，例如: Floating point unit, Adder, Loader, Storer SMT 不是去增加系統的速度，是增加系統的使用率，前提是有足夠的 Task 跟進行的運算可以 Loop unrolling， 例如: Fibonacci 因為每次都必須算出前一個數字才能繼續後面數字的運算 延伸閱讀: Algorithm Efficiency - Is partially unrolling a loop effective if it requires more comparisons? 1.30 Single-ISA heterogeneous multi-core 在學術上是多個處理器，這些處理器使用同樣的指令集，但是處理器的內部設計(Micro-Architecture)不同 例如: Clock，Pipeline depth，Funciont unit number 有些程式指令平行度高，適合 Pipeline、Superscalar 有些程式指令平行度低，適合 In-order issue、High clock processor 因 Instruction set 相同，OS 可以幫合適的 Task 挑選適合的 Processor ARM 上實現的 Big-Little 架構是上述的一個實現 雖然 Processor 運算速度不同，但其 Instruction set 相容 OS 可以依照需求使用高效能 Processor(Big) 或省電型 Processor(Little) 在 Big.Little 中可以做 Task migration，這部分是 OS 跟 Programmer 需要去考慮的 SMT 是由 DM Tullsen, SJ Eggers, HM Levy 所提出 Simultaneous multithreading: Maximizing on-chip parallelism, ISCA, 1995. Intel 於 2002 實現這個構想，並將它稱之為 hyperthreading Big.LITTLE 的概念由 R Kumar, KI Farkas, NP Jouppi, P Ranganathan, DM Tullsen 提出 Single-ISA heterogeneous multi-core architectures: The potential for processor power reduction, MICRO, 2003. 更厲害的是：這二個架構的創想來自於同一個實驗室 https://cseweb.ucsd.edu/~tullsen/ 1.32 UMA and NUMA 目前為止(2018)所使用的 Processor memory architecture 大部分為 Uniform memory access(UMA) 大致上可以說: 每個 Processor 存取任何位置的 Memory 的速度都是相等的 UMA 架構使用一套記憶體插槽，也就是共享 Memory bus 對於 UMA Processor 而言，當 Processor 增加時，記憶體頻寬是效能瓶頸所在(Intel core i9 是有 18C36T 的 UMA Processor) Non-Uniform memory access(NUMA) 傳統上是插兩個以上的 CPU，如部分機架式伺服器: HP DL380 Gen7 AMD 的 threadripper 將兩顆 CPU 封裝在一起，threadripper 上可以看到兩個 DRAM 插槽 對 NUMA 來說記憶體分為 Local, Remote，存取 Local 時的速度會較快 這樣的做法兩邊都會有獨立控制的 Memory bus，避免了部分的記憶體頻寬的效能瓶頸 Driver Architecture Driver 中可延遲處理(Bottom half) 的形式就分成三種，這部分在 Bottom half 簡單提過: softirq(軟中斷) softirq 支援 SMP，同一個 softirq 可以在不同的 CPU 上同時運行，softirq 必須是 Reentrancy(可重入的) softirq 是在編譯期間靜態註冊，不像 tasklet 那樣能被動態註冊或去除 HI_SOFTIRQ, TIMER_SOFTIRQ, NET_TX_SOFTIRQ, NET_RX_SOFTIRQ, SCSI_SOFTIRQ, TASKLET_SOFTIRQ 基本原則是使用在高速裝置或該裝置不能被延遲，如: Network RX/TX, Timer inputerret, Disk, tasklet softirq 在 Kernel 編譯是就已經定義、註冊好，通常是不會去做改寫。會使用 softirq 的原因主要是因為發現 I/O 的效能瓶頸是在 CPU， 前提是要有足夠的 Processor。 tasklet tasklet 不允許兩個相同類型的 tasklet 同時執行，即使在兩個 work-queue 由 Kernel theard 來實現，所以可以被 context switch(前兩種只能被 ISR 打斷) 適合需要長時間執行，或需要 seelp 默認可以被 Interrupt，不持有任何 Locked 延伸閱讀: V-Softirq in Linux Device Driver – Linux Device Driver Tutorial, Linux softirq, tasklet, workqueue 下面是 Interrupt 離開時觸發 softirq 的流程與程式碼: ksoftirq 是在 Kernel thread 會去呼叫 Loop，這個 Loop 會不斷去拿工作來做，這個 Loop 可以被 ISR 或 ksoftirqd 呼叫， 如果是由 ksoftirqd 呼叫那就可以 sleep，但如果是由 ISR 呼叫 Loop 則是執行在 Interrupt context 那就不能 sleep。 會設計 ISR 可以呼叫 Loop 的原因是 Linux 為了優化，如果 Interrupt 所觸發的工作並不多，就乾脆在 ISR 中處理完 若是由 ksoftirq 呼叫就會讓 Interrupt 先結束，後續讓 Scheduler 來安排 Task 進行工作 invoke_softirq() 會去 Loop 中拿取工作來執行 force_irqthreads 用這個變數來判別是 Interrupt context 呼叫還是 Kernel thread __do_softirq(), do_softirq_own_stack() 都是去執行 softirq，差異在 IRQ STACK 詳細可見延伸閱讀 wakeup_softirqd() 則是叫醒 ksoftirqd 把剩下的工作交給 ksoftirqd 來排程 延伸閱讀: Linux kernel的中断子系统 softirq 相較於 softirq, work-queue 的流程單純很多，不會運行在 Interrupt context: General rules for driver writing 共用的資料跟誰共用，雙方是否會同時執行: 會: 就要使用 spinlock(Mulit-Processor)、semaphore(Signle-Processor) 不會，那要注意是否會 Preempt: 誰會 Preemmpt 誰 如果單一方向如 A Preempt B，B 去 Disable A 就不會有 Rest condition，例如: Scheduler, Local IRQ, Bottom half 如果是雙向的 Preempt 那就是會同時執行 要很清楚程式中隱藏的意思 例如: malloc 是否會造成 Context switch, write 會做 I/O 動作會不會 Context switch Start the OS 1.33 Start the OS 為什麼不能直接從 Disk 啟動 OS，這是因為 Disk 也需要 Driver 來啟動 CPU 能直接控制 DARM, ROM 是因為這兩者都是 Byte address CPU 只要在 Address bus 上放入要讀取的資料的 Address 就能從 Data bus 上讀取想要的資料 Disk 屬於 Block device 需要下達命令告訴 Disk 需要第幾個 Block，之後 Disk 再講 Block 寫到軟體指定的位置，如： (ATA-8, ATAPI) 如果沒有軟體驅動的情況下 CPU 無法直接讀取 Disk，因此需要在 PC 架構中在 Boot rom 中放入「BIOS」， BIOS 的重要目的就是讀取 Disk 上的 Boot sector。 Bootstrap BIOS(ROM) 讀取 Boot sector(通常是 Disk 上的第 0 個 Block) BIOS 會帶起來的是 GRUB(Boot loader)，現在也可以從 BIOS 使用 UEFI(BIOS 也認得 OS) 如果帶起的 OS 是 Linux 那麼就必須把開機相關的檔案放在 /boot 下，並且 GRUB 認得該目錄使用的檔案系統 Linux kernel 先掃描裝置狀態，根據裝置狀態配置記憶體(Virtual memory layout)，之後再啟動 Cache 先設定 Virtual memory 是因為這裡必須將映射到 I/O 的部分設定為 Non-cached Linux kernel 啟動後，啟動第一個 User space 的程式，傳統上是 init，也有許多 Linux 改用 systemd，他們的 pid 都是 1， 負責 Linux 後續的初始化 這裡是因為對應 I/O 的區段(例如: DMA)，不希望這些操作被 Cache，如果被 Cache 可能會導致資料不一致，因此要先設定 Non-cached BIOS limitations BIOS 必須要認得 Disk 上的開機 Block，如果 BIOS 不支援該 Disk，將無法啟動 OS(例如: 容量超過 BIOS 的定址範圍) BIOS 內含多種驅動程式，例如要支援 USB 大部分 BIOS 不支援藍芽裝置，因此開機時的藍芽滑鼠，鍵盤等等都不能與 BIOS 互動 Last Edit 10-09-2023 18:27"
  },"/jekyll/2023-09-07-test_case_generation_based_on_constraint_logic_graph.html": {
    "title": "Paper | Test Case Generation Based on Constraint Logic Graph",
    "keywords": "software software_qualitiy generate_test_case Jekyll",
    "url": "/jekyll/2023-09-07-test_case_generation_based_on_constraint_logic_graph.html",
    "body": "Chiao-Yi Huang, “Test Case Generation Based on Constraint Logic Graph”, 2015. 本論文描述一種基於限制邏輯圖所進行的限制式案例產生技術的黑箱測試，並將該測試案例產生器實作成一個 Eclipse 的外掛套件。 1. Introduction 1.1 Motivation 根據 Standish 團隊的 2015 CHAOS 報告指出目前開發高品質的軟體系統十分困難，確保軟體品質的主要方法即是軟體測試。 這裡指出了幾個測試流程: Unit testing(單元測試) Integration testing(整合測試) System testing(系統測試) Acceptance testing(驗收測試) 而 Tase case 的產生可以分為兩種方法: Black-box testing(黑箱測試) 在測試前先依照規格慘生測試案例，是本論文使用的方式，透過 UML Class diagram 搭配 OCL 來描述規格 White-box testing(白箱測試) 其中針對單元測試的 Tase case 可以分成兩種情況: Valid test case(符合前置條件的測試案例) 符合程式預期輸入與輸出的測試案例，需要先得知測試前後的系統狀態(system pre-state/post-state)，參數(argument)，回傳值(return value) Invalid test case(不符合前置條件行為測試案例) 可對程式產生錯誤的測試案例，也就是在 Java 中所發生的例外狀況(exception)。 1.2 Method 限制式測試案例產生(constraint-based test case generation)技術是一種重要的測試案例自動產生技術， 將測試案例產生問題制定為限制滿足問題(Constraint Satisfaction Problem)，以此有四個主要問題需要解決: 軟體行為規格的描述 軟體等價行為的分割 軟體測試覆蓋標準的滿足 軟體等價行為所對應的限制滿足問題的敘述 第一個問題，本論文將物件限制語言運算式(Object constraint language expression)轉換為限制邏輯圖(CLG)，以 CLG 來表現受測函數的程式行為。 第二個問題，本論文定義一個完整的限制邏輯圖路徑就代表一個完整的程式行為，即為一個等價類(Equivalence class)， 透由 CLG，原本是無限組合的析取正規式(DNF)的受測函式程式行為，變成可數的受測函式程式行為，將這些程式行為分割成一個可以被管理的等價類集合， 等價類內的全部測試案例都被當作找錯誤的能力都相同，只要從每個等價類中挑出一組測試案例，即可產生出必要的測試案例。 第三個問題，這裡分為兩個部分來討論： 第一為規格覆蓋標準，由選擇的規格覆蓋標準評估規格覆蓋度需要到甚麼程度，我們提供了三種覆蓋標準。 Decision coverage (DC) Decision condition coverage (DCC) Multiple condition coverage (MCC) 第二部分則是針對已經產生的限制邏輯圖是否產生足夠數量的可視完整路徑，稱之為限制邏輯圖覆蓋標準。 第四個問題，將可實行路徑上的限制式收集，並將這些限制式轉換成限制邏輯程式， 就可以使用限制邏輯語言找解器(Constraint logic programming solver)(ECLiPSeclp) 求出測試資料(測試資料包含測試輸入與預期輸出與系統狀態)， 最後將測試資料轉換成需要的平台的測試案例，而在本篇論文中可以針對 Java 的平台轉換成 JUnit 測試案例。 圖二為依照這四個問題所建構的測試工具整體架構，以下說明各個架構的功能 限制邏輯圖轉換器: 描述如何根據我們得到的受測函式的 物件限制語言(OCL) 產生他相對應的 限制邏輯圖(CLG) 路徑條列器: 從 限制邏輯圖(CLG) 產生 可實行路徑(feasible path) 的方法，一條完整的路徑也不代表這條路徑可以產生測試案例， 必須能分辨它是否為可實行路徑 限制邏輯程式產生器: 我們需要的資料不僅是受測函式相關資料，還需要產生執行此受測函式時相對應的執行前與執行後的環境， 我們根據 UML 的關聯中對於物件的限制找到適合的系統狀態，並且每個物件都會滿足自身定義的恆定條件。 而在待測函式的路線中，可能會呼叫其他的函式，為了讓函式呼叫能正常的運作並且符合應有的限制式，我們還會在測試案例中補上其他函式的模擬。 測試資料解析器 得到測試資料後，由於測試資料只是純文字，透過此解析器得到測試資料的詳細資訊。 測試腳本產生器 這裡的測試資料是與平台無關的，需要透過測試腳本產生器使測試資料實體化成為測試案例，這裡使用 JUnit。 2. Related Technology Research 2.1 - 2.7 為使用的工具與技術介紹，2.8 介紹了相關的研究， 2.3 Constraint Logic Programming(CLP), 2.6 JUnit, 2.7 GraphViz 介紹請參考原論文 2.1 Unified Modeling Language(UML) 2.2 Object constraint language(OCL) 2.4 Coverage Criteria 2.5 Test quality assessment 2.8 Related research 2.1 Unified Modeling Language(UML) 關於 UML 這裡有介紹，這裡不再贅述。 2.2 Object constraint language(OCL) OCL 可以更嚴謹的描述 UML 中有關系統規格的所有資訊，是 UML 的擴展。OCL 使用三種限制式(Constraint)描述類別行為: Class invariant 此類別在整個生命週期中都應該滿足的條件 Method pre-condition Method 執行前應該滿足的條件 Method post-condition Method 執行後應該滿足的條件 2.4 Coverage Criteria Coverage Criteria(測試覆蓋標準)是用來衡量測試嚴謹的程度，測試覆蓋標準越嚴謹代表所需開發成本提高但軟體品質也提高: Decision coverage(DC, 決策覆蓋) 程式的控制流程圖中每一個決策結構點的真與假值都必須執行過，因此每個 Edge(邊界)都會被執行過 Condition coverage(CC, 條件覆蓋) 一個決策有可能包含一個以上的條件，所有的條件都必須執行過，但不需要特別包含 DC 標準 Decision condition coverage(DCC, 條件決策覆蓋) 條件與決策覆蓋都需要滿足，代表所有條件與決策的真與假值都必須執行過一次，但不用包含所有的條件組合 Multiple condition coverage(MCC, 多重條件覆蓋) 每個條件的真與假值都必須執行過一次，且每個條件組合都必須執行過一次，如果條件為 n 個，需要執行 2n 次 粗體是本論文會使用到的測試覆蓋標準 2.5 Test quality assessment 這裡使用 Mutation testing 來評估測試案例的品質，Mutation testing 是一種測試案例的品質評估方法， 將待測程式改變幾個 Operation 來測試 Test case 能否找出這些改變。 延伸閱讀: Paper An Analysis and Survey of the Development of Mutation Testing. 2.8 Related research 2.8.1 Test Case Generation Based on UML/OCL 在 [17] 他們擴大了HOL 為基底的測試框架，[HOL-TestGen] [20]，可以支援 UML/OCL 的規格。他們其中一個主要的貢獻， 是擴充以規格為基底的測試案例產生器推向物件導向的規格。 [17] A . D. Brucker, P. K. Matthias, L. Delphine, W. Burkhart, “A Specification-Based Test Case Generation Method for UML/OCL,” Models in Software Engineering, vol. 6627, pp. 334-348, 2011. [20] A . D. Brucker, W.Burkhart, “Interactive Testing with HOL-TestGen,” in Proceedings of the 5th International Conference on Formal Approaches to Software Testing, 2006. 在 [18]中，他們展示了TOTEM(Testing Object-orienTed systEms with the unified Modeling language)，一個實用的測試方法。測試需求從早期的開發文件中取得， 如用例圖(use case diagram)、用例說明(use case description)、與每個用例與類別相關的交互關係圖(interaction diagram)， 他們在活動圖(activity diagram)中捕捉在用例之間的連續關係，因此允許用例序列的規格被測試。 [18] L . Briand, L. Yvan, “A UML-Based Approach to System Testing,” in Proceedings of the 4th International Conference on The Unified Modeling Language, Modeling Languages, Concepts, and Tools, 2001. 在 [19]中提出屬性為基底的測試(property-based testing)，結合了軟體模型(UML) 與限制屬性(OCL) 來討論，他們斷定了系統元件的兩種不同的屬性， 例如無狀態與狀態相關，可以產生完整屬性為基底的測試套件，產生的測試套件可以表達成 QuickCheck[21] 的形式。 [19] M . A. Francisco, M. C. Laura, “Automatic Generation of Test Models and Properties form UML Models with OCL Constraints,” in Proceedings of the ACM 12th Workshop on OCL and Textual Modeling, 2012. 2.8.2 Test Case Generation Based on Constraint [22] 是將基於限制式的研究用在測試上的先驅，他們提出了透過基於限制式測試，自動產生測試輸入用在基於錯誤的測試(fault-based testing)。 [22] R . A. DeMillo and A. J. Offutt, “Constraint-Based Automatic Test Data Generation,” IEEE Transcations on Software Engineering, vol. 17, no. 9, pp. 900-910, 1991. [23] 提出了第一個從C 程式轉換成靜態單賦值形式(static single assignment form)，使所有變數都只會被賦予最多一次的值， 接著可以透過這個方法有系統地取得控制流程圖中每一條不同執行路徑的限制式，進行符號執行(symbolic execution)。最後， 透過限制找解器(constraint solvers) 自動的產生所有可實行路徑的測試輸入。 [23] A . Gotlieb, B. Botella and M. Rueher, “Automatic Test Data Generation Using Constraint Solving Techniques,” in Proceedings of the 1998 ACM Symposium on Software Testing and Analysis, 1998. [24] 使用符號執行來收集在Ada 程式中不同路徑的限制式。他使用限制邏輯程式(constraint logic programming) 來自動的產生針對所有可實行路徑的測試輸入。 [24] C . Meudec, “ATGen : Automatic Test Data Generation Constraint Logic Programming and Symbolic Execution,” Journal of Software Testing, Verification and Reliability, vol. 11, no. 2, pp. 81-96, May, 2011. [25] 提出了可以從VDM的規格中自動產生測試案例，他們將待測程式的 VDM 規格中的符號轉換成一階邏輯演算(first-order logiccalculus) 的析取範式(disjunctive normal form)。 每一個合取(conjunctive) 的演算都是等價類(equivalence class)，而他們也有考慮關於產生一系列函式呼叫時要讓整個程式處在一個適當的系統環境中做測試。 [25] J . Dick and A. Faivre, “Automating The Generation and Sequencing of Test Cases from Model-Based Specifications,” in Proceedings of the 1st International Symposium on Formal Methods Eurpoe, 1993. 3. Constraint Logic Graph Generator 3.1 OCL Syntactic analysis 3.2 AST Post-Processor 3.3 Constraint Logic Graph Generator Architeture 3.4 Definition of Constraint logic graph 3.5 Generation of Constraint logic graph 這裡介紹如何將 Method 的 OCL 轉為 CLG，架構圖如下: MUT means Method unit testing 主要流程如下: 使用 DresdenOCL(OCL 分析器)讀取 OCL 的規格 AST 透過 OCL 分析器結果來建立 AST 經過 Post-Processing(AST 重構器) 因為 OCL 可能因為 User 的習慣造成轉換 GLC 的困難，因此需做重構 根據不同的 Coverage Criteria 將 AST 轉換為 CLG 每個限制式產生一個 CLG subgraph，再根據各個 Function 將 CLG subgraph 結合成一個 Complete CLG 一個 Complete CLG 可以分成兩種，符合/不符合 Pre-condition 的 Complete CLG 3.1 OCL Syntactic analysis OCL 有三種限制式(invariant, pre, post) 這樣每種情境就要建立一個 Abstract syntax tree(AST)， 這裡會透過 Dresden OCL 來分析 OCL，其中每一個 Operator 都會產生一個 Subtree。 3.1.1 Structure of Abstract syntax tree AST 的每一個 Node 都代表一個 OCL 的運算式，以下是作者所設計的 Node: public ASTNode(Constraint obj) { super(); this.id = node_count++; // Auto-increment id this.constraint = obj; // Type provided by DresdenOCL parents = new ArrayList&lt;INode&gt;(); // Point to parents } ASTNode 常用函數 public abstract CLGNode toCLG(Criterion criterion); 產生 CLG 的函數，回傳此 ASTNode 所產生的 CLG 的第一個 CLGNode public abstract Classifier getType(); 每個 ASTNode 都是運算式，函式回傳此 ASTNode 的回傳型態 public abstract String getState(); 取得此ASTNode 的是在哪個狀態(inv、pre、post) public abstract ASTNode toDeMorgan(); 根據 De Morgan’s law 對此 ASTNode 取反值 public abstract ASTNode toPreProcessing(); 將ASTNode 轉換成適合產生限制邏輯圖且比較簡單的 ASTNode，再根據新的 ASTNode 結構來產生 CLG Constraint: 針對限制式所定義的物件型別，供內部需使用到限制式時使用 public Constraint(Model model, tudresden.ocl20.pivot.pivotmodel.Constraint obj, ASTNode spec) { super(obj); this.dresden_constraint = obj; // Type provided by DresdenOCL this.model = model; // Point to UML model this.spec = spec; // Point to AST root node this.spec.addPreviousNode(this); } 這裡只舉能產生分支的 ASTNode 為例子，全部的 Node 請參考論文 IfExp: 指向 thenExp 與 elseExp public IfExp(Constraint obj, ASTNode conditionExp, ASTNode thenExp, ASTNode elseExp) { super(obj); this.conditionExp = conditionExp; this.thenExp = thenExp; this.elseExp = elseExp; conditionExp.addPreviousNode(this); thenExp.addPreviousNode(this); elseExp.addPreviousNode(this); } IfExp 的抽象語法樹如圖，根節點為IfExp conditionExp：ASTNode 第一個子樹，為condition 的運算式 thenExp：ASTNode 第二個子樹，為then 的運算式 elseExp：ASTNode 第三個子樹，為else 的運算式 OperationCallExp: this.parameters: 第二個子樹開始，不限個數的參數 this.isMethod: 是否為函數呼叫 public OperationCallExp( Constraint obj, ASTNode source, String name, Classifier type, boolean isMethod, Collection&lt;ASTNode&gt; parameters ) { super(obj, source, name, type); this.parameters = new ArrayList&lt;ASTNode&gt;(parameters); this.isMethod = isMethod; } IteratorExp: 迭代器依靠呼叫 IterateExp 來產生迭代的 AST global_iterate_id: 來產生迭代的次數 addPreviousNode: 將此次迭代變為下次迭代的 Parent public IterateExp(Constraint obj, ASTNode source, String name, ASTNode accInitExp, ASTNode bodyExp) { super(obj, source, name, accInitExp.getType()); this.accInitExp = accInitExp; // Initial value of the iterator this.bodyExp = bodyExp; // Body of the iterator accInitExp.addPreviousNode(this); bodyExp.addPreviousNode(this); this.iterate_id = global_iterate_id++; } 3.1.2 Example Triangle 以下是一個 Triangle 的例子，這裡有三個限制式(pre, post, post)要傳換成 AST: package tcgen context Triangle::Triangle(sa : Integer, sb : Integer, sc : Integer): OclVoid pre EdgeErrorException: sa + sb &gt; sc and sb + sc &gt; sa and sa + sc &gt; sb and sa &gt; 0 and sb &gt; 0 and sc &gt; 0 post: sideA = sa and sideB = sb and sideC = sc context Triangle::category() : String post: result = if sideA@pre = sideB@pre then if sideB@pre = sideC@pre then 'Equilateral' else 'Isosceles' endif else if sideA@pre = sideC@pre then 'Isosceles' else if sideB@pre=sideC@pre then 'Isosceles' else 'Scalene' endif endif endif 上圖是 Triangle 的 Post 被轉化為 AST 的結果，其他的轉換可以參考論文 3.2 AST Post-Processor 因為 User 在描寫物件時會有一些口語上的習慣，因此會將 AST 再重構成結構簡單的 AST，他的條件如下: 不符合前置條件的重構，以此產生不符合 Pre-condition 的 CLG Flat IfExp(扁平化 IfExp) Flat Logic Operator(扁平化 Logic Operator) iterate Operator simplify，只能出現 variable = set of variables -&gt; iterate(…) 3.2.1 AST reconstruction 以下說明 AST 重構中每個條件的方法 不符合前置條件的重構 作者認為如果 Function 本身沒有錯誤但跳出 Expection 時，代表 Function 的 Pre-condition 不符合才會造成 Expection 的發生， 所以就將 Pre-condition 轉換為 (not, 非前置條件)，來產生不符合前置條件的 CLG。 上圖是一個不符合 Pre-condition 的 Triangle，可以看到所有的條件判斷都是相反的 Flat IfExp 如果在不產生分支的運算式底下有 IfExp，則必須將 AST 扁平化，使 IfExp 拉到運算式 1 的上層，例如下面的範例: context Class::method() : return_type pre: exp1 op (if exp2 then exp3 else exp4) 這個例子可以用三元運算式來理解，A = (B ? C : D)，在某些語言中沒有 ? 就會使用 if 取代 扁平化後將 if 拉到最上層，如下: context Class::method() : return_type pre: if exp2 then exp1 op exp3 else exp1 op exp4 Flat Logic Operator 邏輯運算的扁平化主要有兩種狀況: Binary operation(2元運算), not operation(not運算) Binary operation 的狀況跟 IfExp 類似，如在不產生分支的 Operator 下有邏輯運算式，就將 IfExp 拉到最上層: context Class::method() : return_type pre: exp1 op (exp2 or exp3) &lt;!-- Flat --&gt; context Class::method() : return_type pre: if exp2 or exp3 then exp1 op true else exp1 op true not operation 則透過 De Morgan’s laws 來拆解，下表是針對 Operation 而定義出的 DeOperation: 如果有 exp1 and exp2 做 not 運算，則會變成 (not (exp1)) or (not (exp2))，上表的可靠性可以透過真值表來做驗證: Iterate Operator 的格式簡單化 由於 iterate 的圖較為複雜，為了讓限制邏輯圖中的限制式簡單化，我們只接受如重構條件 4 的格式。 若使用者提供的物件限制語言中的運算式不符以上的格式，在我們的系統中會強制重構。 variable = set of variables -&gt; iterate(…) 3.3 Constraint Logic Graph Generator Architeture 3.3.1 不符合前置條件的完整限制邏輯圖 不符合前置條件的限制邏輯圖在 AST Post-prossing 就將前置條件重構，因此不需要在 CLG 產生器架構中做額外動作， 但是一個 Funciton 可能有多個不同的 Execption 跳出，這時就需要寫多個前置條件，這樣就會有兩個獨立的 CLG。 context Class::Method(Parameter : Type) pre Constraint_Name_1 : Constraint_1 pre Constraint_Name_2 : Constraint_2 多個前置條件來實現多個例外描述，並且以限制式的名稱來做為 Exception 的名稱 3.3.2 符合前置條件的完整限制邏輯圖 根據 OCL 設計的三種限制式，把所有前置與後置條件結合 Function 的 CLG 就是該 Function 的所有可能的限制邏輯圖， 而在解析中一個前置條件就是一個限制式，所以這裡需要把所有前置條件與後置條件銜接起來。 CLG 的做法就是找出待測 Function 的 Pre/Post-Condition 的集合。 3.4 Definition of Constraint logic graph 限制節點(Constraint Node): 以方框表示 唯一含有限制式的 AST 的 Node，在限制節點中的一定都是布林運算式，並只有以下可能: Relational Operator(關係運算式) True/False Symbol Boolen Operator(布林運算式) Not Operator(否運算式) 連接節點(Connection Node): 以菱形表示 作為連接用，CLG 中唯一會連接到分支的圖形 起始節點(Start Node): 黑色圓形 CLG 的起點，所有 CLG 都需要由此 Node 開始 結束節點(End Node): 黑色圓形帶外框 CLG 的結束點，所有 CLG 都需要由此 Node 結束 3.5 Generation of Constraint logic graph 每一顆 Abstract syntax tree(AST) 都是一個運算式，所以每個運算式都可以透過 Call toCLG(criterion) 來透過走訪產生限制邏輯子圖。 如果這顆樹沒有分支就代表該樹下的所有節點都產生在同一個 CLG 限制節點內，如下圖: 在這之後的討論會以三種不同的 Covaerage criteria 與會產生分支的運算式來做討論，下表說明什麼運算式會產生分支: 3.5.1 Decision Coverage IfExp IfExp 與原始定義相同，如果 If 內為 True 就執行 Then 運算式，否則執行 Else 運算式，因為使用 DC 標準所以不會再展開 IfExp，但需要一個 False 的分支。 toCLG(Criterion criterion) { ASTNode astNotNode = new OperationCallExp(node.conditionExp, \"not\"); return ifCLG( node.conditionExp.toCLG(criterion), node.thenExp.toCLG(criterion), astNotNode.toCLG(criterion), node.elseExp.toCLG(criterion) ); } ifCLG( (CLGNode condNode, CLGNode condEnd), (CLGNode thenNode, CLGNode thenEnd), (CLGNode notCondNode, CLGNode notCondEnd), (CLGNode elseNode, CLGNode elseEnd)):(CLGNode, CLGNode) { ConnectionNode beginConnecting = new ConnectionNode(); ConnectionNode endConnecting = new ConnectionNode(); beginConnecting.connect(condNode); beginConnecting.connect(notCondNode); condEnd.connect(thenNode); notCondEnd.connect(elseNode); thenEnd.connect(endConnecting); elseEnd.connect(endConnecting); return (beginConnecting, endConnecting); } 上圖是 IfExp AST &amp; CLG 的轉換對照圖 IterateExp 因為 CLP 的變數是一個固定值，因此會在變數後面加上 Iterate 的編號 左邊的迴圈在條件終止前會不斷迭代 累加器(IterateAcc) Init 迴圈計數器(IterateIndex) Init 迴圈條件判斷 Collection 取出這次迭代的值 IterateAcc = IterateBody IterateIndex++ 前綴 # 代表是作者創造的變數，後面的數字是為了區分是第幾個 Iterate 的運算式 上圖是 IterateExp AST &amp; CLG 的轉換對照圖 DC Example of Triangle: DC 只需要展開 Decision 的真假值 以下是一個 Triangle 的前置條件 AST 符合 Pre-Condition CLG 的結果，並且之後的測試覆蓋標準都會以 Triangle 為例: 以下是符合/不符合 Pre-condition 的兩種 CLG: DC 中不符合 Pre-condition 的 CLG 會將所有的運算式做 DeOperation，並且在 DC 中不會展開 Boolean(or) 運算式，因此只會有一條路徑: 而符合 Pre-condition 的 CLG 中 Boolean(and) 在 DC 標準中也不會展開因此只會有 Pre-condition 的限制式與 Post-condition 的限制式: Triangle::category 因為只有 Post-condition 故沒有接其他限制邏輯圖: 3.5.2 Decision condition coverage DCC 與 DC 的不同在於需要展開有分支的限制邏輯子圖，所有條件與決策的真與假值都必須執行過一次，但不用包含所有的條件組合 and &amp; or and 運算式來說 Exp1 and Exp2，兩者為真才為真，因此在 CLG 的角度來看就是兩個節點連在一起 or 運算式則是 Exp1 or Exp2，其中之一為真即為真，故從 CLG 的角度來看 Exp1/2 可以視作兩條路徑上的節點， 並且根據 DCC 的定義，至少有一次的 True 與 False，因此會如下圖所示: xor &amp; implies xor 運算式為 Exp1 xor Exp2，兩者不同才為真，因此在 CLG 的轉換圖如下，其中 not(Exp1) 代表取 Exp1 的反值 implies 運算式為 Exp1 implies Exp2，視作 (not exp1) or exp2 DCC Example of Triangle: 以下是一個 Triangle 的前置條件 AST 不符合 Pre-Condition CLG 的結果，因為 DCC 需要展開 Boolean(or) 運算式，因此不符合 Pre-condition 的 CLG 需要展開: 每遇到一次 or 產生一個條件為假其餘為真的路徑，而 AST 共有六個 or 運算式，因此 CLG 會有六條路徑，如下圖: 而在使用 DCC 時產生的符合 Pre-condition 的 CLG，因為前置條件皆為 and 故只有一條明顯路徑: 3.5.3 Multiple condition coverage MCC 與 DCC 不同的是，MCC 需要展開所有的條件組合，因此會有 2n + 1 條路徑，其中 n 為條件的數量 or 因此在真值表中可以發現 Exp1 or Exp2 一共會有三種可能讓此運算式為真，因此 CLG 會如下圖: DCC Example of Triangle: 因為這樣產生的 CLG 非常複雜所以這裡就不放上，但這個 CLG 因為有五個同一層級的 or 因此會有 25 = 32 + 1 條路徑。 4. Equivalence Class CLG Path Generator 4.1 CLG Paths lister 4.2 Path Post-Processor 4.3 Constraint Logic Program Coverage Criteria 4.4 Triangle Example - Paths and Test Case 這裡描述如何將 CLG 中的等價行為做分割，也就是從 CLG 中分割出不同的 Complete Path，產生測試路徑的架構圖如下: 當滿足以下兩個條件的其中一個，路徑條列就會結束: 可實行路徑組滿足覆蓋標準 失敗的路徑次數已經超過可容許錯誤次數 整個 CLG 的路徑條列器工作流程如下: 透過 CLG 覆蓋標準找尋完整路徑 經由 Path Post-Processing 交給 CLP Generator 然後由 Eclipseclp 求解(測試資料) 收集路徑與測試資料直到條件達成 這裡所使用的 CLP Generator 使用 Y.-T. Lan, Automatic Conversion from UML/OCL Specification to CLP Specification, 2015. 所提出的方法 4.1 CLG Paths lister 作者使用 BFS 來對 CLG Traverse 4.1.1 PathFinder Object Implementation 作者實作了一個名為 FeasiblePathFinder Class，他的屬性與參數與方法如下: FeasiblePathFinder(CoverageCriterion criterion, CLGNode graph, Model model) CoverageCriterion criterion; //覆蓋標準 Queue&lt;List&lt;CLGNode&gt;&gt; path_queue; //未完整路徑佇列 Model model; //UML 類別圖 Path getNextPath() 會從此 CLG 的 StartNode 加入 path_queue，在之後的每次迭代把往下的 Node 全部加入 path_queue，如果有多條 path 就複製現有的 List&lt;CLGNode&gt; 並把不同的分支加入不同的 List&lt;CLGNode&gt;， 並檢查是否有已經完整的 Path，有則暫停並 Return path isCompletePath(List&lt;CLGNode&gt; path) 檢查是否為完整路徑，完整路徑的最後節點必然是 EndNode。 如果已經產生過的 Complete Path ，再次被產生就判斷已經沒有路徑可以產生，因此會回傳 null，這樣就可以避免無限迴圈 4.2 Path Post-Processor 一個完整路徑需要做後處理才能轉換成 CLP，Path Post-Processor 對路徑做三件事: 邊界值尋找器(Boundary value finder) 隱含後置條件的復原器(Implicit post-condition meger) 以靜態單賦值形式(Static Single Assignment Form，SSA Form) 再次給予各個變數名稱 4.2.1 Boundary value finder 透過邊界值理論，對於 Path 中的每個限制式都視做 Domain 的一條邊界，透過呼叫 getBoundaryCombinationVariants() 產生符合的路徑如下: 上圖左為原始邊界，圖右分別為 內部點，兩條封閉邊界上的兩個點，以要遍歷 &lt;, = 可以計算為 1 + nop - 1 4.2.2 Implicit Post-Condition Restorer 在 CLG 中如果在 Post-Condition 中沒有特別指定物件的屬性是否有改變，就代表受測函式執行前後的屬性是一樣的，因此在產生測試路徑後可以先判斷 Post-Condtion 是否有定義 Pre-State 的改變， 如果沒有就在完整路徑上補上隱含的 Post-Condtion，如下: &lt;!-- Complete Path --&gt; self@pre.sideA &lt;&gt; self@pre.sideB self@pre.sideA = self@pre.sideC result = “Isosceles” &lt;!-- Add Post-Processing --&gt; self.sideA = self.sideA self.sideB = self.sideB self.sideC = self.sideC 下面三個 Post-Processor 產生的限制式會被加入完整路徑中 4.2.3 Static Single-Assignment Processor 因為 CLP 中每個變數都只能有一個值，因此遇到迴圈時就需要另外處理變數名稱，例如以下的例子: 每次迭代就在後面加上數字代表這次迭代的變數，並將其收集就能知道一次迭代上的所有限制式。 4.2.4 Path Object Implementation 以下是 Path Class 的屬性、參數與方法如下: Path(List&lt;CLGNode&gt; nodes, Model model) List&lt;CLGNode&gt; nodes; int id; List&lt;ASTNode&gt; actual_asts; Model model; Constraint dresden_constraint; List&lt;ASTNode&gt; getASTNodes() 取得全部限制節點中的抽象語法樹(回傳 actual_asts) void analysisASTNodes() 會先呼叫 prepareNewSymbolTable() 來準備變數的符號表，針對#IterateAcc、#IterateIndex、#IterateElement 三個變數做特別處理， 若這三個變數在 Operator 的左邊則將變數名稱替換成 OriginalName + Number private List&lt;OperationCallExp&gt; getEqualExpsForAttributeConsistency() 偵測是否有屬性在 Post-Condtion 中有改變，若無則回傳一系列的 Attribute = Attribute@Pre private HashMap&lt;String, Integer&gt; prepareNewSymbolTable() 準備所有 Variable 的符號表，String 為變數名稱，Integer 為變數的位置，1 是 self, 2 開始是 parameter, 3 是 parameter 的 return value private Set&lt;Attribute&gt; findUnchangedPropertyForParameter(final HashMap&lt;String, Set&lt;PropertyCallExp&gt;&gt; changedPropertyCallExprs, Parameter parameter) 偵測是否有屬性在 Post-Condtion 中有改變，若無則回傳一系列的 Attribute = Attribute@Pre public List&lt;Path&gt; getBoundaryCombinationVariants() 透過呼叫 calculateVariants() 取得來源路徑中符合邊界值覆蓋的組合，接著複製相對應數量的路徑並將組合中的 OperationCallExp 取代原本的，並輸出全部組合的 Queue private List&lt;List&lt;Pair&lt;Integer, String&gt;&gt;&gt; calculateVariants() 會將 CLG 中的限制節點的 AST Node 中的邊界取出，並以一個固定規則來轉換 例如之前的 A &lt;= B, B &lt;= C，會將 &lt;= 的位置取出，以 «, &lt;=, =&lt;, == 的組合來取代，並刪除不合理的組合 == 4.3 Constraint Logic Program Coverage Criteria 這章節會介紹 Coverage Criteria 的 Interface，要注意這裡談的是為了產生路徑的測試覆蓋標準，在 CLG Coverage Criteria 中作者目前僅有 Edge Coverage， 也就是對所有邊界覆蓋。 4.3.1 Coverage Criteria Interface void addFeasiblePath (List&lt;CLGNode&gt; path) 當 Path 已經確定有解，這條 Path 放入 Feasible Path 中 void addInfeasiblePath (List&lt;CLGNode&gt; path) 當 Path 無解，這條 Path 放入 Infeasible Path 中 void analysisTagetGraph(CLGNode graph) 在產生完完整的限制邏輯圖之後，我們會使用此函式來幫忙分析這張完整的限制邏輯圖會需要覆蓋那些資訊 boolean meetRequirement() 此函式是幫忙檢測是否已經將該覆蓋到的地方都已經覆蓋到了 boolean isVisitedFeasiblePath (List&lt;CLGNode&gt; path) 此函式是幫忙檢測輸入的這條路徑是否已經被標註為可實行路徑 boolean isVisitedInfeasiblePath (List&lt;CLGNode&gt; path) 此函式是幫忙檢測輸入的這條路徑是否已經被標註為不可實行路徑 4.3.2 Edge Coverage Set&lt;ImmutablePair&lt;CLGNode, CLGNode&gt;&gt; all_branches; //全部的邊(點與點的配對) Set&lt;ImmutablePair&lt;CLGNode, CLGNode&gt;&gt; visited_branches; //已經走訪過的邊 Set&lt;List&lt;CLGNode&gt;&gt; infeasible_path; //已走訪過不可實行路徑 Set&lt;List&lt;CLGNode&gt;&gt; feasible_path; //已走訪過可實行路徑 透過比較 all_branches 與 visited_branches 兩個集合內的邊，用來查看是 否全部的邊都已經被走訪過。 因為不同測試標準會有不同的 CLG 因此在走訪完全部路徑就代表達成測試覆蓋標準 4.4 Triangle Example - Paths and Test Case 在這裡我只舉用了 Triangle Consturctor 的符合/不符合 Pre-condition 為例子: 4.4.1 Decision coverage testcase Triangle 在 DC 測試標準下符合 Pre-condition 的測試資料只有一條，如下: Path Parameter Return value Post-condition 1 1, 1, 1 void Triangle(1, 1, 1) Triangle 在 DC 測試標準下不符合 Pre-condition 的測試資料會有，但是在 Boundary value finder 上會把 or 做展開，因此會有七條路徑，如下: Path Parameter Execption 1 -10, -10, -10 void 7條 Paths minumum solutions 都相同，這裡不全部列出 4.4.2 Decision condition coverage testcase Triangle 在 DCC 下不符合 Pre-condition 的測試 5. Test Case Generator 5.2 Performance of Coverage Criteria 由於 Coverage Criteria 會產生不同的 CLG，產生路徑的時間與涵蓋到的內容也不一樣，我們將範例資訊顯示在下表， Class info 分別代表: Class Num, Association Num, Function Num, Can be excption function Num: IntegerRange: iterate 測試 RecursionExample: fibonacci, factorial Triangle, Date: 表現複雜的分支測試 Laboratory: 關聯多個類別的物件 Example Class info AST Node Iteration IntegerRange 1/0/2/0 38 Yes RecursionExample 1/0/2/2 49 No Triangle 1/0/2/1 213 No Date 1/0/8/1 816 No Laboratory 3/3/8/1 124 No 然後分別展示三種 Coverage Criteria 的產生結果，主要分別討論符合/不符合 Pre-condition 的產生結果: 就跟預料的一樣所花費時間 DC &gt; DCC &gt; MCC Date 是花費時間最長的範例，因為 Date 中有更多的 if, or 運算式 Laboratory 是花費時間第二長的範例，因為 Laboratory 中有許多關聯物件，導致需要一併產生才能完成 同時在 DCC, MCC 中會產生許多條不可實行路徑，例如以下的例子: context Date::Date(y : Integer, m : Integer, d : Integer) pre DateErrorException: if (y.mod(400) = 0) or (y.mod(4) = 0 and y.mod(100) &lt;&gt; 0) then d &lt;= 29 DCC 會產生一條左為真，右為假的例子: y.mod(400) = 0, y.mod(4) &lt;&gt; 0, y.mod(100) &lt;&gt; 0 沒有數字能夠符合這三個條件，因此這條路徑是不可實行的 context Date::Date(y : Integer, m : Integer, d : Integer) pre DateErrorException: if ((m = 1) or (m = 3) or (m = 5) or (m = 7) or (m = 8) or (m = 10) or (m = 12)) then d &lt;= 31 MCC 為了產生所有的組合，會有 m=1, m=3 這樣的例子出現 5.3 Quality of Coverage Criteria 使用 PIT 來驗證三種 Coverage Criteria 下產生的 Test Case 的品質，DC 在各方面都表現得較差，而 DCC, MCC 雖然 MCC 可以產生更多的測試案例， 但兩者的突變分數幾乎一樣，因此一般情況下使用 DCC 即可做到 MCC 差不多程度的覆蓋度。 但是在幾個特定案例下，都無法達到 100% 的覆蓋度，因為透過 CLP 找解時會優先找出最簡的解(最小值)，因此例如: parm &lt; 10 突變為 parm &lt;= 10 時， 就無法找出其中錯誤。 6. Conclusion and Future Work 作者實作出了可以輔助測試驅動開發的系統，基於限制邏輯圖的測試案例產生器，透過規格文件產生黑箱函式測試案例。 OCL 僅能支援部分 String 與部分 Collection 與完整的 Integer 型態。 並在撰寫時需要明確寫出使用到的物件皆為前置狀態(意指obj@pre)，避免我們誤判成後置條件有被更動。 另外就是如何判斷無效的測試路徑，如 5.2 所述的無效路徑將浪費大量的時間，因為這裡無效的判斷方式是以 CLP 的求解與超時來決定， 如果有多個無解路徑將必然消耗固定的等待時間。 NOTE Last edit 11-15-2023 12:55"
  },"/jekyll/2023-09-07-Introduction_OCL.html": {
    "title": "Note | Object Constraint Language Concepts (Unfinished)",
    "keywords": "software tool Jekyll",
    "url": "/jekyll/2023-09-07-Introduction_OCL.html",
    "body": "Object Constraint Language(OCL) 物件限制語言。 1. Introduction 通常只使用 UML 是不足以完全規範一個軟體系統 所以還需要 Constraint 來完全規範一個軟體系統 Constraint 是對軟體系統的一個或多個值的限制 Object Constraint Language(OCL) 是一種基於 Text 的語言，用於描述這些 Constraints 上圖展示了一個 Flight 的 Class invariant，如果 type == passenger，那麼 airplane.type 也必須為 passenger，同樣的 cargo 也只能對應 cargo 這意思就如果是客運航班就要對應客機，貨運航班就要對應貨機。 2. Basic Object Constraint Language 2.2.1 Kinds of Constraints 因 UML 類別圖中無法包含一些細節資訊，若以自然語言描述，常造成開發者與使用者間認知差異，所以使用物件限制語言嚴謹的描述 UML 類別圖中有關系統規格的所有資訊，為 UML 標準的擴充機制。 物件限制語言使用三種限制式(constraint)來描述類別的行為: Class invariant(類別恆定條件): a constraint that must alwaysbe met by all instances of the class. 針對一個類別而言，此類別的任何物件在整個生命週期皆須滿足自身定義的恆定條件 Method pre-condition(函式前置條件): a constraint that must always be true beforethe execution of the operation. 針對一個函式而言，在此函式被呼叫之前須滿足前置條件，才能確保動作正確 Method post-condition(函式後置條件): a constraint that must always be true afterthe execution of the operation. 針對一個函式而言，若在此函式被呼叫之前滿足前置條件，則此函式在被呼叫後，一定滿足後置條件 Other constraints(其他限制式) 在 OCL2 之後還有其他的限制式加入如: Derive, Init, Let, Def, Package 例如: 一個校園借書系統，那麼 Class invariant 必須為 Student，因為校外人士不能借閱， 而還書時 Pre-condition 必須為至少要有那本要還的書，Post-condition 則必須沒有已經還回去的書。 2.2.2 Constraint Context and Self Every OCL expression is bound to a specific context. The context is often the element (classor method) that the constraint is attached to. The context may be denoted within the expression using the keyword ‘self’. ‘self’ is implicit in all OCL expressions. Similar to ‘this’ in Java. 例如: 以下的 OCL，context Person 宣告了這個 Constraint 是屬於 Person 這個 Class，而 self 則專指這個 Person 的 attribute context Person inv: self.age &gt;= 0 2.2.3 Notation OCL 可以單獨寫在一個文件中，也可以寫在 UML 的 Class diagram 裡面，這些表示方法都是相同的 2.2.4 Elements of an OCL Expression In an OCL expression these elements may be used: basic types: String, Boolean, Integer, Real. classifiers from the UML model and their features attributes, and class attributes query operations, and class query operations (i.e., those operations that do not have side effects) associations from the UML model Including Rolenames at either end of an association OCL 同樣有型別的概念，並且可以使用 UML 的 Class diagram 中的 Attribute，Operation，Association 等等 Basic types 以下表示了 OCL 的基本型別，可以使用在 OCL 的表達式 context Airline inv: name.toLower = 'klm' context Passenger inv: age &gt;= ((9.6 -3.5)* 3.1).floor implies mature = true 這裡表現出了 name.toLower 必須為 String，而 age 則為 float，mature 則為 boolean Attributes OCL 中有專指 Object 實例的 Attribute，也有專指 Class 的 Attribute Class attribute 會在所有 Object 實例中共享 Object attribute 則是每個 Object 實例都有自己的 Attribute 以下是一個例子: -- Object attribute context Flight inv: self.maxNrPassengers&lt;= 1000 -- Class attribute context Passenger inv: age &gt;= Passenger.minAge 這裡的 maxNrPassengers 為專指 Flight Object 的 Attribute，而 age 則為所有 Passenger Class 的 Attribute The @Pre Keyword The @pre keyword indicates the value of an attribute at the start of the execution of the operation The keyword must be postfixed to the name of the item concernedsize = size@pre + 1 例如 size = size@pre + 1: size@pre 表示在執行這個 Operation 前的 size size 表示在執行這個 Operation 後的 size Example: 例如一個 Triangle 的完整 OCL 可能會類似以下，其中包含了: Class invariant: 三角形的三邊長必須符合三角不等式 Constructor(int sa, int sb, int sc): Triangle 的建構式 Pre-condition: 輸入值必須符合三角不等式 Post-condition: 建構後的 a, b, c 應該等於輸入的 sa, sb, sc Method: category(): 用來判斷三角形的類型 Post-condition: 回傳值應該為 Equilateral, Isosceles, Scalene context Triangle inv: a + b &gt; c and a + c &gt; b and b + c &gt; a context Triangle::Triangle(int sa, int sb, int sc) pre IllegealArgException: sa + sb &gt; sc and sa + sc &gt; sb and sb + sc &gt; sa post: a = sa and b = sb and c = sc context Triangle::category(): String post: result= if a@pre = b@pre then ifa@pre = c@pre then 'Equilateral' else 'Isosceles' endif else if a@pre = c@pre then 'Isosceles' else if b@pre = c@pre then'Isosceles' else 'Scalene' endif endif endif 3. Collection in OCL OCL 中有已經定義好的關於 Collection 的操作，語法上使用: collection -&gt; operation 來表示對 Collection 的操作 3.1 Four Subtypes of Collection 這裡有四種型態的 Collection 可以宣告: Set: arrivingFlights(from the context Airport) Non ordered, unique elements OrderedSet: passengers(from the context Flight) Ordered, unique elements Bag: arrivingFlights.duration (from the context Airport) Non ordered, non unique elements Sequence: passengers.age (from the context Flight) Ordered, non unique elements Basic Collection Operations Boolean isEmpty(): 如果 Collection 為空則回傳 True notEmpty(): 如果 Collection 不為空則回傳 True includes(object): 如果 Collection 中包含 object 則回傳 True excludes(object): 如果 Collection 中不包含 object 則回傳 True includesAll(Collection): 如果 Collection 中包含所有 Collection 中的元素則回傳 True excludesAll(Collection): 如果 Collection 中不包含所有 Collection 中的元素則回傳 True Integer size(): 回傳 Collection 的大小 count(object): 回傳 Collection 中符合條件的元素個數 sum(): 回傳 Collection 中所有元素的總和 Coolection including(object): 回傳一個新的 Collection，包含了 object excluding(object): 回傳一個新的 Collection，移除了 object union(Collection): 回傳一個新的 Collection，包含了原本的 Collection 和另一個 Collection intersection(Collection): 回傳一個新的 Collection，包含了原本的 Collection 和另一個 Collection 的交集 -(Collection): 回傳一個新的 Collection，從一個 Collection 中移除另一個 Collection 的元素 symmetricDifference(Collection): 回傳一個新的 Collection，包含了原本的 Collection 和另一個 Collection 的差集 Operations for Ordered Collection first(): 回傳第一個元素 last(): 回傳最後一個元素 at(index): 回傳 index 的元素 indexOf(object): 回傳 object 在 Collection 中的 index insertAt(index, object): 將 object 插入到 index 的位置 append(object): 將 object 插入到最後一個位置 prepend(object): 將 object 插入到第一個位置 subSequence(lower, upper): 返回一個包含從較低 index 到較高 index 的元素的新 Sequence subOrderedSet(lower, upper): 返回一個包含從較低 index 到較高 index 的元素的新 OrderedSet Loop Collection Operations NOTE Last edit 11-20-2023 00:11"
  },"/jekyll/2023-08-06-gdb_introduction.html": {
    "title": "Note | GNU Debugger Quick Notes",
    "keywords": "software tool Jekyll",
    "url": "/jekyll/2023-08-06-gdb_introduction.html",
    "body": "記錄如何使用 GDB 進行開發與除錯，以前都是用直覺、測試的方式來進行除錯，也很少用 debug mode 不是一件好事也很沒效率， 藉此建立 Debug 的好習慣。「實際上在我職涯中 95% 的 Bug 都能透過 debug mode 解掉」－PTT鄉民 Reference: GDB: The GNU Project Debugger, GDB User Manual jasonblog: 通過 GDB 學習 C 語言 Jserv: 你所不知道的 C 語言: 開發工具和規格標準 1. GNU Debugger(GDB) Introduction GDB 是 GUN 系統下的標準除錯工具，使我們能查看程式在運行中內部發生了什麼，或是崩潰時正在做什麼。 此外 GDB 經過移攜需求的調修與重新編譯，現在許多的 UNIX Like 系統上都可以使用 GDB。 GDB 最主要的四項功能有: Start your program, specifying anything that might affect its behavior. Make your program stop on specified conditions. Examine what has happened, when your program has stopped. Change things in your program, so you can experiment with correcting the effects of one bug and go on to learn about another. GDB 目前所能支援的語言: Ada Assembly C C++ D Fortran Go Objective-C OpenCL Modula-2 Pascal Rust 2. Using GDB to development 2.1 Debuging Program 2.2 Setting Breakpoints 2.3 Check status 「一旦你已經習慣於在 REPL 環境下進行探索性的編程，必須進行「編寫-編譯-運行」這樣循環實在有點令人生厭。」—jasonblog: 通過 GDB 學習 C 語言 REPL(Read-Eval-Print Loop) 環境可以讓我們方便的了解當下在做什麼，如果透過類似 REPL 的方式來進行對需要編譯後運行的程式進行開發、 追蹤代碼、除錯都非常有幫助。 2.1 Debuging Program 使用 GDB 就需要 debugging information 給 GDB 使用，這裡 gcc 要帶入參數 -g，gcc Option for Debugging， 然後就可以啟動 gdb 進行 debug，之後輸入 run 指令就可以運行程式。如果程式需要帶入參數的話， 就直接在 run 後面加入參數就好。 -g Produce debugging information in the operating system’s native format (stabs, COFF, XCOFF, or DWARF). GDB can work with this debugging information. gcc -g main.c -o main gdb main # Reading symbols from main... (gdb) run # Program output. (gdb) run ${argv} # Program output. 如果想對已經運行中的程式進行 Debug 就要使用 attach，並且要知道該 Process pid， 但注意對運行中的程式進行 Debug 有可能干擾正在運行的程式，有更多的細節可以查看 Debugging an Already-running Process。 ps -ef | grep ${program_name} gbd ${pid} 2.2 Setting Breakpoints 使用 info breakpoints 可以查看已經設置的中斷點，與中斷點的 Type, Address, What。中斷點有多種形式， 這裡主要講幾種常用的中斷點，更多內容可以看 Setting Breakpoints: Breakpoint and Continue 使用 break 或 b 設置，可以設置在目標的行數或是函數名稱， 當然如果遇到斷點後想要繼續程式，使用 continue 就可以了: # set breakpoint at line 23 (gdb) break main.c:23 # set breakpoint at function main (gdb) break main Breakpoint with condition 也可以設定當條件出現時才會中斷，例如懷疑是程式中出現非期望的值，就可以在這裡設置斷點觀察。並且也可以透過 condition 修改斷點的條件。 (gdb) break test.c:23 if b==0 # if this break point number is 1, change condition. (gdb) condition 1 b==1 Breakpoint with rule 依照規則來設定斷點，例如函數名稱，檔案，等等… # break all function (gdb) rbreak . # break all prefix is printNum* function (gdb) rbreak printNum* # break all function in test.c (gdb) rbreak test.c:. # break all function in test and prefix is print (gdb) rbreak test.c:^print Skip breakpoints multiple times 我們也可以設置跳過某的斷點幾次，例如一個函數前 10 次都沒出現問題要跳過前 30 次的中段，之後可以透過 info breakpoints 看到設置。 (gbd) ignore 1 30 Watchpoint 觀察點是設置當某個變數或類型產生變化時進行觀察，有 wathc, rwatch, awatch， # break if a changes (gbd) watch a # break when a is read (gbd) rwatch a # break when a is write (gbd) awatch a Clean and disable/enable breakpoint Disable/Enable # disable all point (gbd) disable (gbd) disable ${break_num} # enable all point (gbd) enable (gbd) enable ${break_num} (gbd) enable delete ${break_num} Clear/Delete # clean all point clear clear ${function_name} clear ${file_name}:${function_name} clear ${line} clear ${file_name}:${line} # delete all point delete delete ${break_num} 2.3 Check status 在 GDB 中有多種查看變數、記憶體區塊、記憶體內容的方法，這裡我們可以使用 print 印出變數的內容。 (gdb) print 1 + 1 # $1 = 2 # $1 is a temp variable, only live in this debug session. 例如如果有以下程式，然後去設置中斷點觀察變數。 int main() { int x = 10; return 0; } (gdb) break main (gdb) run (gdb) print x # $1 = 0 (gdb) next (gdb) print x # $2 = 10 (gdb) set x = 20 (gdb) print x # $3 = 20 查看變數在記憶體的地址，區塊大小，記憶體內容，在 GDB 中一個數字的低位元在前高位元在後，所以要從左往右讀， x 是從一個位置開始讀取 Memory，4b 代表 4 byte。 (gdb) print &amp;x # $4 = (int *) 0x7fffffffe37c (gdb) print sizeof(x) # $5 = 4 (gdb) print sizeof(double) # $6 = 8 查看記憶體的內容，格式為 x/[n][f][u] addr，其中 n 為顯示的單元數，f 是要顯示的格式，u 是單元長度。 (gdb) x/4xb &amp;x # 0x7fffffffe37c: 0x6f 0x00 0x00 0x00 (gdb) set x = 0x12345678 (gdb) x/4xb &amp;x # 0x7fffffffe37c: 0x78 0x56 0x34 0x12 (gdb) x/4tb &amp;x # 0x7fffffffe37c: 01111000 01010110 00110100 00010010 可以用 ptype 來檢查給定變數或類型的詳細類型定義。 (gdb) ptype x # type = int (gdb) ptype &amp;x # type = int * (gdb) ptype main # type = int (void) Note 以上大概就是常會用到的主要指令，還有更多細節可以看 GDB 手冊，GDB User Manual。"
  },"/jekyll/2023-08-05-linux_kernel_complie.html": {
    "title": "OS | Linux Kernel Compilation",
    "keywords": "OS linux Jekyll",
    "url": "/jekyll/2023-08-05-linux_kernel_complie.html",
    "body": "記錄一下如何編譯與更換 Linux kernel 版本，以用來進行開發測試 Linux kernel module。 Reference: Jserv: Linux 核心模組運作原理, 鳥哥私房菜: Linux 核心編譯與管理 如果想要開發 Linux kernel module，就可能會碰到要更新 Kernel 的情況，這裡紀錄一下如何進行 Kernel 的編譯與安裝。 1.取得 Source Code 有以下三種方法可以使用: 使用 Distribution 提供的核心原始碼檔案。 取得 www.kernel.org 上所提供的核心原始碼。 保留原本設定：利用 Patch 升級核心原始碼。 其中使用 Patch 的話需要將間隔中的每個 patch 都進行 patch，想要由 3.10.85 升級到 3.10.89 的話， 那麼就得要下載 patch-3.10.86, patch-3.10.87, patch-3.10.88, patch-3.10.89 等檔案。 這裡使用 www.kernel.org 所提供的原始碼，其中: mainline: 最新的、積極開發的版本，因此有較高的變化與更新頻率。 stable: 穩定版本，這些版本會包含 Mainline 的一些修復和改進，但不是全部都會收入。 longterm: 長期支持的版本，可以在生產環境內長時間穩定運行。 下載後解壓縮放置，這裡選擇放在 /usr/src: tar -Jxvf ${linux_kernel} -C /usr/src/ 2.前處理與配置核心功能 開始配置前先記得刪除檔案中可能有的 Object file(.o) 和 Config，mrproper 會清除過去曾經配置的核心配置文件。 cd /usr/src/${linux_kernel} make mrproper make clean 詳細配置與說明看 鳥哥私房菜: Linux 核心編譯與管理，這裡直接複製原來的 Kernel config，這個 .config 就是核心的配置檔案。 也可以使用 make menuconfig 來進入文字圖形介面來進行配置，但是我們這裡使用原本的 Kernel config。 cp /boot/config-${old_linux_kernel} /usr/src/${linux_kernel} 3.編譯核心檔案 這裡使用多核心數去進行編譯，因為編譯 Kernel 是一個很長時間的工作，這幾個核心可以同時進行編譯的行為，這樣在編譯時速度會比較快。 最後製作出來的資料是被放置在 /usr/src/${linux_kernel} 這個目錄下，之後才會進行安裝。 make -j ${core_number} clean bzImage modules bzImage: 編譯核心 modules: 編譯模組 Tip: 在這一步我遇到幾次編譯錯誤，要回頭去修改 .config 等設定。記錄一下最後的解決方法: 主要是一些系統簽章檢查需要解決，參考中有正確配置的方法，還有編譯時的依賴項沒有安裝，遇到就安裝就好。 4.實際安裝核心 4.1 安裝模組與核心 首先安裝 modules，會放置在 /lib/modules/$(uname -r) 目錄下，直接使用以下指令就好: make modules_install # check modules ll /lib/modules/ 核心則會放置在 /boot 下，並且檔名為 vmlinuz 開頭，這裡可以去看 vmlinuz 的歷史。這裡鳥哥有講如何配置多個內核模塊， 下面是不進行配置多模塊直接安裝的方法: cp arch/x86/boot/bzImage /boot/vmlinuz-${linux_kernel_version} chmod a+x /boot/vmlinuz-${linux_kernel_version} # Backup config file cp .config /boot/config-${linux_kernel_version} cp System.map /boot/System.map-${linux_kernel_version} gzip -c Module.symvers &gt; /boot/symvers-${linux_kernel_version} restorecon -Rv /boot 4.2 編輯開機選單 (grub) grub2-mkconfig 是用來生成 grub 文件的，因為預設較新版本的 Kernel 會放在最前面作為預設的開機選單項目， 所以這裡應該會看到剛剛安裝的核心放在第一位出現才對，否則等等可能會用舊核心開機。 # CentOS grub2-mkconfig -o /boot/grub2/grub.cfg # Ubuntu sudo update-grub # Generating grub configuration file ... # Found linux image: /boot/vmlinuz-${linux_kernel_version} # Found initrd image: /boot/initramfs-${linux_kernel_version} 最後就是重新開機然後查看 Kernel 是否有成功更新，可以使用 uname -a 或 uname -r。 Note Last edit 08-05-2023 22:20，如果使用 VM 編譯前多開核心給機器，我是單核心編譯花了大約三小時。"
  },"/jekyll/2023-07-28-UML_structure_diagrams.html": {
    "title": "Note | UML Structure Diagrams Introduction",
    "keywords": "software software_development Jekyll",
    "url": "/jekyll/2023-07-28-UML_structure_diagrams.html",
    "body": "本篇主要介紹 UML 的分類中的結構圖(Structure Diagrams) 1. Structure diagrams 1.1 Class diagram 1.2 Component diagram 1.3 Deployment diagram 1.4 Object diagram 1.5 Package diagram 1.6 Composite structure diagram 1.1 Class Diagram Class diagrams(類圖)是使用最廣泛的 UML 圖，他是所有 Object-Oriented Software Systems 的基礎。 透過顯示系統的 Class(類)、Mthods(方法)、Properties(屬性)來描述系統的靜態結構。 從下圖我們可以看出: Abstract Class: 看到 Shape 是一個 Abstract Class 他以斜體顯示。 Class diagram relationships: Generalization: 空心箭頭，同時 Shape 也是一個 Superclass，Circle、Rectangle、 Polygon 都是由 Shape 所衍生，也就是說 Circle 是一個 Shape，這是一種 generalization(一般化)/inheritance(繼承) 的關係。 Association: Class 間的連線，DialogBox 和 DataController 之間有一個關聯。 Aggregation: 空心菱形箭頭，Shape 和 Window 是一種聚合關係，Shape 可以存在而不依賴 Windows。 Composition: 實心菱形箭頭，Point 是 Circle 是一種組合關係，沒有 Circle 就不能存在 Point。 Dependency: 空心箭頭，Window 依賴於 Event，但 Event 不依賴於 Window。 Attributes: Circle 的屬性是 radius、center 後面是他的型別，這是一個 Entity class。 Methods: Circle 的方法是 area()、circum()、setCenter()、setRadius()。 Parameter: Circle 中的 area(in radius: flot) 代表參數是一個名為 radius 型別為 float 的傳入參數。 Return: Circle 中的 area(): double，代表返回一個 double 的值。 Hidden: Rectangle 的 Attributes，Mehtods 是隱藏的，圖中的其他一些 Class 也隱藏了他們的 Attributes，Mehtods。 1.2 Component diagram Component diagram(元件圖)描述一個軟體系統時，將軟體系統裡的元素給予模組化(Modularity)，即成為一個元件。將元件與元件間的關係做描述時， 對於軟體系統的運作可以比描述類別關係更加得清楚。 Component 通常比 Class 有更高的抽象層級，可能由一個或多個 Class 組成，可能是包、可執行檔案、資料庫等。 每個元件通常封裝特定的功能並公開一個明確定義的介面，元件與類最大的差別在於元件強調的是介面的溝通。 下圖是一個 Composite Component，也就是元件中包含著元件=的範例，Composite Commponent 中的主要元素有: Component(元件): 以一個長方形所顯示，右上角繪製一個 1.x UML 版本的元件圖(非必須)。 Provided interface(提供接口): 以一個圓形顯示，代表為提供給 Client 端所使用的接口。 Request interface(所需接口): 以一個半圓顯示，代表元件所需求的介面。 Port(端口): 以一個正方形表示，以表示元件公開的端口。 Relationship: Dependency: 空心箭頭，表示一個元件依賴其他元件或接口才能實現。 1.3 Deployment diagram Deployment diagram(部屬圖)是顯示運行時處理節點的配置，用於物件導向系統的物理方面進行建模， 通常用於對系統的靜態部署視圖(硬件拓撲)進行建模。 拓撲(Topology): 在系統架構中用於描述不同組件或模塊之間的關係和連接方式。 與 Component diagram(元見圖)的不同點在於，部屬圖主要用於展示一個系統或軟體是如何被部署在不同的硬體和運行環境中， 他著重於系統的物理組成、硬體資源的配置、節點之間的通訊通道。 下圖是一個部署圖的範例，有這些主要元素: Node: 3D 矩形，代表一個節點，無論是硬體或是軟體。 可以使用 &lt;&lt;stereotype&gt;&gt; 來註明節點，以區分節點的類型。 Node 內可以包含另一個 Node。 Association: Node 間的連線。 可以使用空心箭頭來代表之間的依賴性。 TCP/IP Client / Server Example: 1.4 Object diagram Object Diagram(物件圖)用於展示系統或軟體在特定時間點內物件實例(Object instance)，還有物件彼此間的關聯。 它也被稱作記憶體的快照(memory snapshot)。 Object(物件): 物件是特定 Class 的實例，物件顯示的是實例與屬性。 通常物件圖用於開發的後期階段，用來展示特定時間點內的物件實例與相關的關係，有助於理解系統在特定時間內的狀態， 確保物件間的關係符合設計需求。 當 Class diagram 非常複雜時，Object diagram 對於解釋系統的細節部分很有用， 說明 Object diagram 的最佳方式是透過相應的 Class diagram 對照產生的 Object diagram。 下圖說明了一個大學系所的可以有很多其他的延伸系所，將 Class diagram 實例化後的 Object diagram: 一個訂單管理系統的 Class diagram 與其 Object diagram: Object diagram 主要的元素有以下這些: Object: 矩形方框，提供了 Object Name : Class，並以下劃線註記。 Anonymous Object: 匿名物件，僅使用 Method 而不建立實例的物件。 Object attributes: 與 Class 相似，但 Object attributes 必須分配具體的 Value。 Links: Object 之間的關聯，可以使用 Class diagram 相同的箭頭來表示關係。 1.5 Package diagram Package diagram(封裝圖)用於建模高層級系統元素，Package 用於組織包含圖表、文件和其他關鍵的大型系統。 When to use Package diagram? Package diagram 可以用來簡化複雜的 Class diagram，可以將 Class 分到 Package 中。 Package 是邏輯上相關的 UML 元素的集合。 Package 被描繪為文件夾，可以在任何 UML 中使用。 一個 Package diagram 具有以下元素，下圖是一個訂單子系統的範例: Subsystem: 註明了這個子系統的名稱。 Package: 一個矩形，右上角帶有選項卡。Package name 位於矩形內或選項卡上。 Concrete package: 具有實現功能的包，其中有具體的程式碼，可以被執行、編譯或部署。 Abstract package: 沒有實際的程式碼實現，通常是為了邏輯上的分類，用於組織和分類包。 Dependency on external package: 不在系統內部直接實現或編寫的依賴包。 Dependency: 代表 Package 之間的依賴關係。 Example: 如果一個類想要在一個不同的包中使用一個類，它就必須為該包提供一個依賴。 Generalization: 一個包可以包含多個子包，每個子包都可以包含一系列元素。 1.6 Composite structure diagram Composite structure diagram(組合結構圖)是 UML 2.0 中添加的新圖形，其中包含 Class, Port, Package。 組合結構圖的作用與 Class diagram 類似，但是允許更詳細的描述多個 Class 的內部結構並顯示他們之間的交互。 Note Last edit 08-01-2023 00:13, Reference"
  },"/jekyll/2023-07-28-UML_behavior_diagrams.html": {
    "title": "Note | UML Behavior Diagrams Introduction (Unfinished)",
    "keywords": "software software_development Jekyll",
    "url": "/jekyll/2023-07-28-UML_behavior_diagrams.html",
    "body": "本篇主要介紹 UML 的分類中的行為圖(Behavior Diagrams)，與其子集交互圖(Interaction diagrams)。以此為分賴再依照廣泛使用程度介紹。 1. Behavior diagrams 1.1 Activity diagram 1.2 Use case diagram 1.3 State diagram 1.1 Activity diagram Activity diagram(活動圖)是 UML 中第二被廣泛使用的圖形，用於描述系統的動態方面，活動圖本質上是流程圖的進階版本， 它對從一個活動到另一個活動的流程進行建模。 When to Use Activity diagram 通過檢查業務工作流程(Business workflows)來確定候選用例(Use case)。 確定 Use Case 的前置與後置條件(Context)。 對於 Use Case 之間/內部的 Workflows 建模。 對於 Object 的複雜操作流程進行建模。 使用不同層級的活動圖對複雜的活動建模。 下圖分別解釋一個基本的活動圖，與一個訂單流程的範例: Pre/Post-condition: Black circle: 代表該 Workflow 的開始(init node)。 Encircled black circle: 代表該 Workflow 的結束(final node)。 Stadia: 代表動作(actions)。 Diamonds: 代表決策(decisions)。 Bars: 代表並行(Concurrent)活動的開始(split) 或結束(join)。 1.2 Use case diagram Use case diagram(用例圖)是第四使用廣泛度的圖形，表示 Actor 與系統交互的最簡表示形式，展現了 Actor 和與他相關的 Use case 之間的關係。 下圖是一個基本的 Use case diagram，Use case diagram 主要由以下元素所組成: Actor: 與系統進行互動的各種參與者，可以是使用者或外部實體。 Use case: 系統如何反應外界請求的描述，描述了一個特定的操作或任務。 Associate: 以虛線連接 Actor 與 Use case，表示 Actor 與 Use case之間的互動。 include: 包含關係，使用該用例時一定會執行的相關用例。 extend: 擴展關係，使用該用例時不一定會執行的相關用例。 Boundary: 以一個方框定義 System boundary，並在最上方寫出系統名稱。 下面是一個汽車銷售系統的 Use case diagram，可以發現即使是一個汽車銷售系統的用例也不超過十個: 1.3 State diagram State diagram (狀態圖)與用例圖並列第四廣泛使用，顯示實體的不同狀態，狀態圖還可以顯示實體如何通過從一種狀態更改為另一種狀態來響應各種事件。 When to use State diagram? 用於描述系統的狀態轉換，系統在不同時間點處於不同的狀態，以及在特定條件下如何從一個狀態轉換為另一個。 用於建模具有多種狀態的實體，實體如何根據事件改變狀態，如物件的生命週期、狀態機、事件驅動的系統等。 下圖用來說明狀態圖的符號: Initial Pseudo State/Final State: 黑色圓點，開始/結束節點。 State: 圓角矩形，內部標有狀態名稱，是一個實體所處的一個具體狀態。 Transition: 以剪頭指向下一個狀態，在上方可以標明指定觸發轉換的條件或事件。 下圖是一個具有 Substate(子狀態)的加熱器狀態圖，有子狀態的情況也可稱作 Nested state(嵌套狀態)/Compound state(複合狀態)。 Substate 可以有自己的進入狀態、結束狀態、以及在子狀態之間的 Transition。 History States: 除非有特別說明，不然進入一個子狀態都是以初始狀態重新開始。有註明 History state 就代表進入子狀態是以之前活動的最後一個子狀態開始。 延伸閱讀 UML 2 Tutorial - State Machine Diagram 2. Interaction diagrams 2.1 Sequence diagram 2.2 Communication diagram 2.1 Sequence diagram Sequence diagram(時序圖)是第三被廣泛使用的圖形，描述物件在時間序列中的交叉作用。序列圖會描繪在此情境下有關的物件， 以及此物件和其他物件交換訊息的順序。 下圖解釋了一個酒店預約的時序圖，從一個窗口(window) 開始啟動。 一個時序圖應該要注意的點有這些: Object/Actors: 方框，物件/參與者。 Lifeline: 垂直線，代表一個物件的開始與結束。 Activation bar: 發送和接收訊息的開始與結束。 Messages: 水平箭頭，上方寫有被調用的 Mehtod 與 Parameter、Return。 Self Message: 呼叫同一個物件的方法，例如一些 Private method。 Solid arrowheads: 實心箭頭，代表同步(Synchronous)訊息，發送方必須等待訊息完成。 Open arrowheads: 空心箭頭，代表異步(Asynchronous)訊息，發送方無須等待訊息完成。 Dashed lines: 代表回覆，可以是實心(同步)/空心(異步)箭頭上方帶有回覆的 Method。 Lost/Find: 使用一個圓點作為結束/開始，收件人未知/已知，發見人已知/未知。 Creat: 指向一個物件創造一條生命線。 Delete: 終止一條生命線。 Combined Fragment(組合片段): 大方框，用來標記複雜互動。 opt: 相當於 if 當條件為 true 就執行，false 不執行。 alt: 相當於 if else，條件為 true 執行否則執行 else。 loop: 條件為 true 就重複執行，可以使用 loop(n) 指明執行次數。 ref：參考其他的 Squence diagram，用於簡化圖表。 par: 併行(parallel)執行的片段。 2.2 Communication diagram Communication diagram(通訊圖)與 Seqence diagram 一樣是顯示對象如何傳輸信息，他們在語意上是等價的，也就是呈現相同的資訊。 Communication diagram / Seqence diagram 之間可以互相轉換，最主要的差別是通訊圖以空間進行排列元素，序列圖以時間排列元素。 如何在兩種圖型之間轉換的範例: 下圖是一個酒店預約系統的通訊圖，它的主要元素有: Object: 方框，代表物件或實體。 Links: Object 之間的連線，代表通訊通道。 Message: 實心/空心箭頭，各自代表同步/異步通訊，方向代表送出端與接收端，並且訊息帶有編號。 Note Last edit 07-30-2023 13:42, Reference"
  },"/jekyll/2023-07-26-unified_modeling_language.html": {
    "title": "Note | Unified Modeling Language Concepts",
    "keywords": "software software_development Jekyll",
    "url": "/jekyll/2023-07-26-unified_modeling_language.html",
    "body": "Unified Modeling Language(UML) 統一塑模語言。 雖然敏捷開發中不再投入過多時間在靜態建模上，但是 UML 還是有其價值，尤其是在大型複雜系統的設計，或在開發團隊需要更深入的技術和設計細節時。 1. Intorduction UML 是為了大型軟體系統的結構與行為的架構、設計、和實現建立 通用的可視化建模語言。它提供了用於傳達設計決策和系統架構的通用語言和符號。 它包括許多類型的圖，例如 Case diagram、Class diagram、Sequence diagram。他也支援一些 Advanced concepts 像是 Stereotypes(型別標記)、 Profiles(設計資料集)、Constraints(約束條件)、Packages(包)，由此可以對軟體系統進行更精確的定製建模。UML 是為了幫助改善溝通、協作和軟體系統的整體品質。 1.1 History UML 由 OMG(Object Management Group) 負責管理，2005年被 ISO 發布為認可的 ISO 標準，之後該標準定期修訂涵蓋 UML 的最新修訂版。 目前最新的版本是 UML 2.5.1 在 2017 年發布。 1.2 Why use UML in software development? Standardization: 用於描述軟體系統的標準可視化語言。使不同的利益相關者更容易交流設計決策與理解系統架構。 Clarity: 通過提供清晰、簡潔的表示來幫助減少軟體系統的歧義，防止軟體開發過程中的誤解和錯誤。 Collaboration: 促進不同利益相關者(如開發人員、架構師和項目經理)的溝通和協作。確保每個人都朝相同的目標努力。 Effciency: 提供軟體系統的可視化表示，可用於儘早識別潛在問題和設計缺陷，從而簡化軟體開發過程。 Reusability: 用於記錄軟件系統和設計模式，可以在未來的項目中重複使用。在軟件開發過程中節省時間和資源。 1.3 Key Object-Oriented Concepts in UML UML 已經取代了傳統的 Object-oriented (OO) 分析方法。 An object is made up of data and methods that control it. The data represents the object’s current status. A class is a type of object that has a hierarchy that can be used to mimic real-world systems. The hierarchy is expressed by inheritance, and classes can be linked in a variety of ways depending on the needs. Object 是我們周圍存在的現實世界實體，UML 能表示像抽象、封裝、繼承和多型這樣的基本原則，能表示物件導向分析和設計中的所有概念。 因為 UML 圖表中僅表示物件導向的概念。在開始學習之前，充分理解物件導向概念非常重要。 2. UML Hierarchy 根據 UML 提出的層次結構如下圖，分為結構圖(Structure diagrams)與行為圖(Behavior diagrams): Structure diagrams: 用於描述系統的靜態特徵或結構，由於代表結構，所以它們更廣泛的用於紀錄軟體系統的軟體架構。 如 Component diagrams 代表了如何描述軟體系統被拆分為 Components 並顯示這些 Components 之間的 dependencies。 Behavior diagrams: 描述了系統的動態特徵和行為，由於說明系統行為，因此主要用來描述系統的功能。如 Activities diagrams 用來描述系統中組件的業務與逐步操作的活動。 Interaction diagrams(交互圖): Interaction diagrams 是行為圖的 subset，強調被塑模系統中事物之間的控制流程與資料流程(flow of control and data)。 如 Sequence diagram 顯示對象如何在彼此之間就一系列訊息進行通訊。 2.1 UML Survey* States Grady Booch, one of the most important developer of UML, stated that “For 80% of all software only 20% of UML is needed”. UML 一共有 14 張圖表，但在一個軟體開發中並不是每個 Diagram 都會被使用，下圖表示了使用該 Diagram 的廣泛程度: 之後會再另外兩篇以結構圖、行為圖分類，再以使用廣泛度來依序介紹。 Note Last edit 07-24-2023 2201, Reference: Unified Modeling Language (UML) Introduction"
  },"/jekyll/2023-07-22-property_based_testing_entropy_guided_backbox_REST_API-_fuzzer.html": {
    "title": "Paper | BenFuzz: A Property Based Testing and Entropy Guided Blackbox REST API Fuzzer",
    "keywords": "software software_qualitiy Jekyll",
    "url": "/jekyll/2023-07-22-property_based_testing_entropy_guided_backbox_REST_API-_fuzzer.html",
    "body": "Benjamin Chen, “BenFuzz: A Property Based Testing and Entropy Guided Blackbox REST API Fuzzer”, 2022. 陳睿瑜, 基於特性測試與資訊熵之黑箱 REST API 模糊測試, 2022. 1. Intorduction REST API 為大多數前端後端與伺服器溝通型態，REST API 有著相當大的使用群體但其安全性或品質， 還是需要經由軟體測試，但背後實作的方式各有不同，所以對於 REST API 有蠻多工具可以去做測試。 論文提出以 REST API 回應之訊息熵作為變異參考依據，以及 Schemathesis[17] 中提到的基於特性的測試方法做結合， 檢測出相關的弱點，讓黑箱測試有了具有方向性的變異策略，使其能有更好的測試覆蓋率與弱點偵測， 去找到規格上與實作出的服務內容不符合的點及伺服器對於請求的處理是否能正確處理，得以找到 500 伺服器崩潰或錯誤問題， 論文也提供完整的前後端讓測試者能操作網路頁面就能做測試，達到開箱即用，也將其容器化方便做成 CI/CD 測試的一環。 2. Background 2.1 REST API REST API（Representational State Transfer Application Programming Interface）是一種基於 HTTP 協定的設計風格， 用於設計和開發網路應用程式的接口，支援資源的狀態和操作的表達，促進了不同系統之間的資訊交換和通訊。 2.2 OPEN API and Swagger Swagger（現在稱為OpenAPI）是一種用於OpenAPI 是用於描述 API 資訊的文件，包括 API 的端點、參數、輸出入格式、說明、認證等， 本質上它是一個 Json 或 Yaml 文件，而文件內的 Schema 則是由 OpenAPI 定義。它允許開發人員、團隊和企業在設計、 開發和使用 API 時更加方便和有效地進行溝通、理解和測試。 openapi: 3.0.0 info: version: 1.0.0 title: Sample API description: A sample API to illustrate OpenAPI concepts paths: /list: get: description: Returns a list of stuff responses: '200': description: Successful response 以上 OpenAPI Yaml 描述了一個名為 “Sample API” 的 API，該 API 具有一個 /list 的路徑可使用 get 來獲取回應。 2.3 Fuzz Testing 模糊測試 (Fuzz Testing) 是基於產生隨機且非預期的輸入，進而觸發目標程式非預期錯誤的一種自動化軟體測試技術。 模糊測試在多個領域都有相應的實作項目，能夠有效地發現程式異常、邏輯錯誤、開發人員設計的瑕疵及非預期的記憶體錯誤， 進而提高程式可靠度及軟體品質。模糊測試目前分為三大類白箱、灰箱、黑箱，依是否能獲得原始碼來分類。 2.4 Property Based Testing 基於範例的測試(Example Based Testing)，基於範例的測試通常要人工寫出測試範例，這樣就會有局限性， 因為要人工產生的範例會受限於思考範例的盲區跟上限。 基於特性測試(Property Based Testing)，我們可以基於要測試的函式，對其特性或運作邏輯，去做特性測試。以下將舉例加法函式在傳統測試與 特性測試的差異性。 Example，假設測試一個加法函數: 傳統測試: 給定測資跟解答，才能驗證是否可行，如 1+1=2 為一個測試點。 特性測試: 用交換律 A+B=B+A、結合律(A+B)+C=A+(B+C)來做到邏輯上的驗證，這些特性是已知的數學性質， 如果函數是正確實現的，它們應該始終成立。測資的部分透過隨機產生，這樣不但測試資料更加多元， 也能減少降低人工產生測試資料的時間成本，從而使得測試資料更具多樣性和覆蓋性。 特性測試包含了三個架構: Arbitrary 亂數產生器 是一種亂數產生的策略，當我們指定資料型別時，我們可以定義符合這個資料型別的亂數產生方式 Generator 測試產生器 測試時依據亂數產生器產生出測試資料的值，這時候我們可以拿到值去帶入函數做測試 Shrinker 誤區識別器 當測試結果不符合預期的時候，我們可以透過他找到錯誤的邊界 2.4.1 Hypothesis Hypothesis[7] 是基於特性測試在 Python 上實作的函式庫，開發者可以設計自己的特性測試，其中提供了各類型的 隨機產生器(Strategies)，如 Int, Float, String，也可以由開發者能加入自己的隨機產生器， 也可以由 Hypothesis-jsonschema，將 jsonschema 轉換成隨機產生器，通過解析型別與內建的隨機產生器組織再一起。 [7] DRMacIver. “Hypothesis”. URL: https://github.com/HypothesisWorks/hypothesis 2.5 Entropy Entropy(資訊熵)，是用來計算資訊量的一個方法，可以根據其計算判斷出其資訊量或亂度的一個量衡，使開發者便於以其作為依據， 去判別一段文字或是位元組所代表的資訊量。 $Entropy (H(X)) = - \\sum_{x} P(x) \\log_{b} P(x)$ 如果有一個系統 S 內存在多個事件 S={E1,…,En}，每個事件的機率分布 P={p1, …, pn}，則每個事件本身的資訊本體為: $I_{e} = -\\log_{2}{p_{i}}$ (對數以2為底，單位是 bit) 如英語有 26 個字母，假如每個字母在文章中出現次數平均的話，每個字母的訊息量為: $I_{e}=-\\log_{2}{1 \\over 26}=4.7$ 以日文五十音平假名為相對範例，假設每個平假名在文章中出現的機率相等，每個平假名日語文字可攜帶的資訊量為: $I_{e}=-\\log_{2}{1 \\over 50}=5.64$ 每個平假名所能攜帶的資訊量比英文字母更大，因為它有更多的字符選擇。這也意味著相比於英文字母， 使用平假名需要更多的 bit 來表示，因為它的字符集更無序。 3. Methodology and Implementation 3.2 Entropy 這裡作者提出一個假設: 「當經過 API 處理後的資訊熵提升代表其背後處理運算是更加複雜的，所以說資訊熵在此假設下與實際覆蓋度是有正相關的。」 因此作者可以以此指標來進行評估當前變異的策略，以此提高程式的覆蓋度，讓模糊測器可以測試到更深成的程式邏輯， 進而提高找到弱點的所在或是觸發錯誤。 3.3 Property Based Testing 作者挑選了 表.3 的四條準則，因為這四條可以通過最少的運算去檢核出來，在不犧牲模糊測試效率的情況下去實作。 利用 Hypothesis 這個 Python 編寫的函式庫，在本篇論文中作為變異（Mutation）及模糊測試的主要架構，通過實時變更函式庫的設定值， 讓其依照資訊熵的變化調整參數或是去更新其種子。使用 表.3 的規範方式作為特性測試之邏輯條件，進而調整變異方式及偵測 API 的弱點。 3.4 Parallel Request Sending 這裡作者使用 Async 進行發送請求與處理回應，來增加同一時間段內發送測試請求的效率與進行的測試數量。 使用 Python Library Aiohttp，是基於 Async 發送請求機制的 Library。 每個請求的生成機制為: 使用 Hypothesis 測試產生器所產生出的測試實例，URL 要通過 yarl 進行 hex 編碼後符合 RFC3986[12]。 若該 API 需要 Token 授權，將其放入 OpenAPI 所指定的授權位置以及其授權格式(bearer、JWT 或其他)。 依照 OpenAPI 所定義的請求格式送出測試請求，透過 Async 方式接受回應與計算熵(Entropy)與後續的調整變異。 3.5 - 3.7 Technology 3.5, 3.6 主要講述作者建立一個前後端系統可用來進行對目標的測試, 可以在前端上傳 OpenAPI 規格與相關授權 Token 後進行測試。 3.7 則是 Docker image 建立實作，這裡不再贅述。 4. Results Evaluation 後續作者以 RQ1 - RQ3，分別去驗證 Entropy(資訊熵)與 Coverage(覆蓋率)之間的關係，常用服務的 API 測試， 以及非自行架設的 API 進行測試，詳細步驟可以見論文 Section 4。 Other Section Section 5 為相關研究, 6 為結論, 7 為未來發展這裡不再說明。 NOTE 這篇論文主要是針對使用 REST API 服務的框架進行測試，以找出框架中不符合 REST API 規範的回應，或會對伺服器產生 500 Status 的錯誤， 其中關於 Property Based Testing, Fuzz Testing 的部分可以作為參考。 Last edit 07-27 14:07"
  },"/jekyll/2023-07-19-MVVM_modeling_methodology_user_interface.html": {
    "title": "Paper | A MVVM Modeling Methodology for Information Systems User Interface Design",
    "keywords": "software web_development web_framework Jekyll",
    "url": "/jekyll/2023-07-19-MVVM_modeling_methodology_user_interface.html",
    "body": "Yuan-Kai Hou, “A MVVM Modeling Methodology for Information Systems User Interface Design”, 2019. 侯元凱, MVVM 模式資訊系統使用者介面塑模方法論, 2019. Lab meeting study: 2 Review frontend framework, Design pattern(MVC, …), UML, PAC, 3-5 Method, 6 Conclusion. Section 2. 文獻回顧的部分放到其他筆記中講，這裡主要看作者的研究方法與方式，提出了一個基於 PAC 模式所擴展的 Enhanced-PAC 模式， 可用來表達 MVVM 模式下的 Frontend Component 的邏輯運作與轉換關係。 1. Introduction 1.1 Research Background and Motivation 傳統網頁大部分都是依照 MPA(Multi-Page Application) 模式開發，這裡作者點出幾個傳統網頁的問題: 並沒有規定一定要將 View, Business logic, DB logic, 分開撰寫，只要方法之間能互相通訊、執行即可。 因此在一份專案中能看到多種語法參雜並且高度耦合(coupling)，導致程式的維護性降低。 傳統網頁中有以 MDA(Model-driven architecture) 配合 CASE Tool 生成代碼的開發方法， 但對於程式邏輯與訊息傳遞上沒有可供依循的參考文件(reference document)，因此在開發過程中難以維護或理解功能對應程式碼的區段。 Model Driven Architecture(MDA) 模型驅動開發: 一種軟設計方法(Software design approach)該方法強調要在軟體開發中的每個步驟均須建構出模型，且最好應表達為電腦可理解的正規模式(Formal Model)。 MDA 將重點放在正向工程(forward engineering)上，從抽象的、人工詳細的建模圖生成代碼。 1.2 Research Objective 使用 DSRM(Design Science Research Methodology) 研究方法，以 Angular2 為前端框架開發一個名為 「便當王系統」， 並提出MVVM 模式資訊系統使用者介面塑模方法論(MVVM Modeling Methodology for Information Systems User Interface)。主要方法有: 使用 MVVM 開發模式，開發前端系統架構。 因為 MDA 開發中只在程式文件中有方法名稱，但並沒有對程式邏輯進行詳細的塑模。因此撰寫可使用於 MVVM 模式的塑模文件， 以提升程式設計師開發與維護時對於功能面的了解。 用 Angular2, PHP, MySQL 進行設計系統。但主要討論的是前端 Angular2 的部分，後端不在研究範圍中。 這裡所說的 「便當王系統」 請參考: 吳仁和,物件導向系統分析與設計―結合 MDA 與 UML, 5thEdition, 台北市: 智勝文化, February 2017, ch5 2. Literature Review 文獻探討章節講述: SPA(Single Page Application) 單一頁面程式: 詳情可看 Vue, React, Angular2 進行了解，主要取代過往 MPA 需要多個 HTML 進行建置網頁，當跳轉頁面時需要再次發出 Request 取回整個 HTML 造成的問題。 Software architectural pattern 軟體開發架構: 可參考 Note | Architectural Patterns Compare MVP, MVC, MVVM UML(Unified Modeling Language) 統一塑模語言: 是由 OMG(Object Management Group, OMG) 物件管理組織歷經多年的版本演化擴充，提出的物件導向塑模工具。 PAC(Presentation–abstraction–control) 表示-抽象-控制模式: 一種 Software architectural pattern，是常見的介面(interface) 結構表達工具，將使用者介面分為多個子介面，每個子介面可視為一個物件。 並有 Net-PAC 等擴展成網狀結構使其能表達 Web-base 的系統。 Shuen-Jen Tsai, Modeling the User Interfaces:A Component-based Interface Research for Integrating the Net-PAC Model and UML, 2002. 3. Research Method 3.1 Design Science Research Methodology 這裡使用該研究方法論步驟來進行論文之研究，其內容為下: Peffers, K., Tuunanen, T., Rothenberger, M. A., and Chatterjee, S., A Design Science Research Methodology for Information Systems Research 3.2 Research Method and Step Angula2 是已經強制以 MVVM 進行開發，而對於程式邏輯未有明確的塑模文件規範。作者使用 UML 的溝通突來建構一份符合 Angular2 框架的系統分析與設計的開發文件。 確認問題與動機: 透過文獻研究來了解當前傳統網頁所存在的問題，以及 MDA 系統開發方法下對於程式邏輯的描述文件不足。 定義解決方法的目標: 解決研究背景中傳統網頁(非 SPA、MVVM)的程式無法將使用者介面、程式邏輯、資料庫邏輯等語法分開撰寫，與不易分割進行分工的問題。 MDA 系統開發方法為 PSM 轉換為傳統網頁程式碼的開發方式，未涉及程式邏輯的塑模與轉換，本研究將以 UML 對其程式邏輯塑模，並產出一份可與 MVVM 模式對應之 UML 文件。 設計與發展: 由於 Angular2 已經強制使用 MVVM 進行開發，因此這裡會以生產率(Productivity)、可攜性(Portability)、互通性(Interoperability)、耦合力(Coupling)作為此系統開發框架的績效指標。 在 MDA 中進行塑模的部分則使用 Enhanced-PAC 建構溝通圖塑模系統的程式邏輯，與資訊傳遞，並以維護與文件(Maintenance and Documentation )做為此塑模方法論的衡量指標。 展示: 「便當王系統」中有一個新增項目活動圖做為來源，分別找出 Model、View、ViewModel，以及對這三個部分以資料詞彙做更詳細的說明，並建立出代表系統介面結構的 Enhanced-PAC 模型， 再以 Enhanced-PAC 模型, View model 資料詞彙作為來源，建構 UML 溝通圖(Communication Diagram)，這樣「便當王系統」就能透過 UML 來使用 Angular2 框架進行開發。 評估: 使用 Angular2 完成 SPA 的「便當王系統」後就能使用生產率(Productivity)、可攜性(Portability)、互通性(Interoperability)、耦合力(Coupling)作為系統評估此四項指標的問題， 維護與文件(Maintenance and Documentation)則以 Enhanced-PAC 模型與溝通圖的塑模方法論來評估。 6. Conclusion 作者提出一個新的 Enhanced-PAC pattern 用於表達 MVVM 架構下 Forntend Model，並以此 Model 進行開發與評估， 是否達到以 Enhanced-PAC 使開發者可以依循的參考文件。 NOTE 剩下的部分為論文的 Section 4-5. 為 Enhanced-PAC 建立方法，與使用 Enhanced-PAC 建立 Communication Diagram 與程式碼對應。並使用作者提出的指標來進行評估。 Section 6. 為結論與未來發展。 Last edit 07-19-2023 19:07"
  },"/jekyll/2023-07-18-software_arch_pattern.html": {
    "title": "Note | Architectural Patterns Compare MVP, MVC, MVVM",
    "keywords": "software software_development Jekyll",
    "url": "/jekyll/2023-07-18-software_arch_pattern.html",
    "body": "Design pattern compare for MVP, MVC, MVMM. 這是三種最常用的軟體架構模式，從這篇來了解這三種架構的差異與比較。 這三種 Architectural Patterns 的目的都是為了將 Business logic(業務邏輯)與 View(視圖)實現代碼分離，使一個程式能有不同的表現形式。 Architectural Patterns vs Design Patterns Architectural Patterns 是一種通用可重複使用的解決方案，用於解決特定 context 中的軟體架構中的常見問題。架構模式是處理系統中的主要組件如何協同工作， 訊息(message) 與資料(data) 如何在系統中流動以及其他結構性的考量因素。架構模式使用一些組件類型，每種組件會由更小的模塊所組成。 Design Patterns 則是常見問題的推薦解決方案與實踐，關注於如何建構應用程式中的組件。 Example: MVC 是一種 Architectural Pattren 他將程式分為三個主要部分, Model, View, Controller, 這部分之間的資訊交換有嚴格的規則，以實現更好的代碼組織與可維護性。 Observer Pattern(觀察者模式) 是一種 Design Pattren，定義了對象之間的一對多依賴關係，以便當一個對象改變狀態時，所有依賴於他的對象都會收到通知。 Model View Controller(MVC) MVC 最早來源於一篇論文，該論文對於 Model-View-Controller 三個模塊以其他們之間的通訊都講述了一些設計細節。 Steve Burbeck, Ph.D., Applications Programming in Smalltalk-80 (TM): How to use Model-View-Controller (MVC), 1979. MVC 將程式分為三種組件，Model, Viewm Controller, 三者有各自的用途和職責: Model: Model = data + business logic 也就是說 Model 既負責了資料的儲存也負責處理開資料的邏輯，因此處理業務邏輯是 Model 的責任而不是 Controller。所以從資料來看， 還可以分為資料的獲取、儲存、資料結構，因此在設計時 Model 會再次細分為更多 layer 如業務邏輯、網路、存儲等… View: View 負責顯示 Model 的資料，並負責最終如何在用戶介面中顯示數據以及終端用戶之間交互。View 是以網格、表格、圖表等等類似可以顯示數據的可視化表現。 Controller: Controller 負責 Model-View 之間的橋樑，用於控制程式的流程。所以 Controller 負責接收來自用戶的輸入、驗證輸入數據，解析用戶輸入後並交由對應的 Model 去處理。 所以理論上 Controller 是很輕的。 MVC Diagram, Reference to here. MVC and its variants 其實我們能發現最初版本的 MVC View 還是依賴於 Model，實際上降低了 View 的可用性，那變種的 MVC 就將 View 與 Model 完全分開，那就可以提高 View 的可重複性， 因此就有以下這種 MVC: MVC variant Diagram. View 與 Model 就不要進行通訊了，所有的通訊都基於 Controller，Model 將結果告訴 Controller，再由 Controller 更新 View。這種變種最早是由 Apple.Inc 所提出， 很多 Web 框架也是基於這種變種 MVC 所設計，如 SpringMVC。 另外在設計時 Controller 去跟 Model 發出請求時通常會比較耗時，因此一般都是非同步(async) 通知 Controller。 Model View Presenter(MVP) MVP 最早的解說來自於，是針對 MVC 再改良出的 Pattern: Mike Potel, MVP: Model-View-Presenter The Taligent Programming Model for C++ and Java, 1996. 可以看出最早的 MVP 其實與我們現在所看到的 MVP 不太一樣，該 MVP 是從數據管理與用戶介面兩個方向的問題出發，將 Smalltalk 的 MVC 在分解而成， 多了幾個中間組件: Interactor: 定義 View 的交互事件，也就是將使用者的輸入轉為適當的操作。 Commands: 定義對 Model 數據的操作如，儲存或更新資料庫、呼叫 API、進行演算法運算等。 Selections: 定義為從 Model 中篩選資料，並準備好相關的查詢或過濾條件。 MVP Diagram, Reference to here. 因此我們從上圖來看，如果去忽略中間組件，會發現與 MVC variant 幾乎一樣，在論文中就提到。 Presenter 其實就是 Controller，只是為了與 MVC 區分開才稱作 Presenter。在這個 MVP 中三個組件各自的職責: Model: 儲存數據(與資料庫溝通、請求網路資源等)，負責處理業務邏輯。 View: 顯示資料，將使用者的輸入傳給 Presenter。 Persenter: 從 Model 中獲取資料，並決定 View 中顯示什麼。 可以發現其實與 MVC variant 中的依賴是一樣的，只是 MVP 之間要透過介面(interface) 來實現，Model, View, Persenter 各自有各自的 interface， 針對 interface coding，自然就會去 decoupling，提高可重複性，以及容易進行單元測試。 Model-View-ViewModel(MVVM) MVVM 最早由 John Gossman 在他的 Blog 上所發表: John Gossman, Introduction to Model/View/ViewModel pattern for building WPF apps, 2005. View-Model 就如字面所述View-Model = Model of View，也可以看作是 Abstraction of the View，簡單來說在 MVVM 中 View 不負責維護數據， View 負責與 View-Model 同步數據，View-Model 也用於管理 View 的狀態和操作模型的方法與命令， 因此 View-Model 中封裝了 View 的屬性(Property)與命令(Command)。 因此在 MVVM 中最重要的一個特性就是數據綁定(data binding)，通過綁定使兩者之間鬆耦合(Loose coupling)，這樣就不用在 View-Model 中去寫 Update， 這裡綁定有兩種類型: 單向綁定(View-Model -&gt; View): 當 View-Model 發生變化後，View 才會更新。 雙向綁定(View-Model &lt;-&gt; View): 當 View-Mode, View 中任何一方發生變化，另一方都會更新。 一般情況下，只需在 View 中顯示但無須編輯的數據使用單向綁定，反之。同時 View-Model 封裝的是 View 的屬性與命令， 因此綁定也分為 Property Binding, Command Binding。 要實現綁定通常使用 Publish–subscribe pattern，這部分通常各大框架中都有自己的實現，Vue、React 中都實現了數據綁定。 Conclusion MVP, MVVM 都是為了實現 View 與 Business logic 的分離問題，只是兩者使用不同的實現。MVP 透過 interface 實現，缺點就是要編寫大量的 interface， 而 MVVM 則透過 Bind，但就要依賴框架工具來開發。 Last edit 07-21-2023 11:33"
  },"/jekyll/2023-07-18-automated_gen_test_case_using_UML.html": {
    "title": "Paper | Automated-generating test case using UML statechart diagrams (Unfinished)",
    "keywords": "software software_qualitiy generate_test_case UML Jekyll",
    "url": "/jekyll/2023-07-18-automated_gen_test_case_using_UML.html",
    "body": "Supaporn Kansomkeat, Wanchai Rivepiboon, “Automated-generating test case using UML statechart diagrams”, SAICSIT ‘03: Proceedings of the 2003 annual research conference of the South African institute of computer scientists and information technologists on Enablement through technology, September 2003, Pages 296–300. 本篇論文提出一種測試用例產生器基於 UML 狀態圖自動產生 Testing Case 的方法。 作者將 UML 轉為一個中間圖，稱作 TFG(Testing Flow Graph)，TFG 會明確識別 UML 狀態圖的流程，並針對測試的目的來進行增強。 從 TGF 中使用測試標準(Testing Criteria)來生成測試用例，包括覆蓋了 UML 圖中的狀態(State) 與轉換(Transition)。 最後使用突變分析(Mutation Analysis)來評估生成測試用例的錯誤揭示能力。 1. Introduction Specification-based testing(基於規格的測試) 是從規格中獲取的信息來幫助測試和開發軟體。測試活動包含: 設計測試用例(Testing Case)，這些測試用例是一系列的輸入。 執行測試用例和檢查執行結果 測試在開發過程的早期進行，開發人員通常會在規格中找到不一致和模糊之處，進而改進規格，然後再進行程式編寫。 Unified Modeling Language, UML(統一建模語言) BOOCH, G., RUMBAUGH, J., AND JACOBSON, I. 1998. The Unified Modeling Language User Guide. Object Technology Series. Addison Weysley Longman, Inc 是一種可視覺化建模語言，包含九種圖形。目前有許多研究專注於從 UML 規格中生成測試用例，詳細可見論文中 Reference，本論文提出一種從 UML 狀態圖中自動生成的測試用例。 NOTE Last edit 07-19-2023 23:02"
  },"/jekyll/2023-06-23-Intergrated_environment_sdd.html": {
    "title": "Paper | An Integrated Environment for Specification Driven Development (Unfinished)",
    "keywords": "software software_qualitiy software_development Jekyll",
    "url": "/jekyll/2023-06-23-Intergrated_environment_sdd.html",
    "body": "CHANG CHUNG-YEN, “An Integrated Environment for Specification Driven Development”, 2022. 張崇彥, 一個支援規格驅動開發的整合環境, 2022. 透過本論文了解一個整合 Junit, XML Model, Testing case, 的規格驅動開發環境架構。 本論文結合黑箱測試案例自動產生及測試驅動開發流程，提出規格驅動開發流程。在 Eclipse 上結合 XML 模型工具 Papyrus、OCL 處理器、 黑箱測試案例產生工具 CBTCG、JUnit，開發一個支援規格驅動開發的整合環境 CBSDD。 這個整合環境以專案的形式，完整支援軟體規格的制定，根據軟體規格自動產生測試案例，並透過自動執行測試案例，來驅動程式的實作與重構。 1. Intorduction 1.1 Motivation TDD 先編寫測試案例來進行規範，從未能夠通過測試案例的紅燈狀態開始進行程式開發，直到程式能夠通過測試的驗證達到綠燈狀態， 再經由重構。這些 TDD 所提倡的優點可見 TDD Concepts. 其中公認的優點為： 程式上開發與實施所得到的功能間都存在差距，TDD 由小而快的迭代不斷將實施的功能回饋給開發者而縮小了差距。 強調自動的單元化測試，自動測試提供可靠的系統，增強測試品質並且降低測試成本。 TDD 創建了一個完整的回歸測試平台。運行這些自動化測試案例，可以輕鬆確定更改是否破壞了系統中的任何內容。 即使測試驅動開發帶來諸多好處，但 TDD 在開發設計上的層面依然有可以改進的地方。因此提出了 BDD、SDD、APDD 等方法來讓使用者可以建立可執行的規範來進行測試。 過去團隊開發 Test Case Generation System[2] 使使用者能以規格文件來得到 Testing case 但當時需要使用者透過一個文件選擇器來選擇對應的輸入文件， 這樣的方式繁瑣且 Specification file, Output file 的關聯性也不明確。 [2] C.-L. Wang and N.-W. Lin, “, “Supporting Java Array Data Typein ConstraintBased Test Case Generation for Black-Box Method-Level Unit Testing,” in International Computer Symposium , 2018. 因此本論文提出一種透過 Eclipse 插件建立的 SDD 整合環境，來提升使用者的操作體驗。 1.2 Method 本專案設計了一個基於 Eclipse 的整合環境, 並設計出一套專案架構來管理規格文件與產生測試案例, 對於架構的詳情見論文 P3 這裡不詳細敘述，並且開發了兩個 Wizard 來幫助使用者建立專案。 Wizard: In Eclipse, a wizard is commonly used for the creation of new elements, imports or exports. fig 3. 規格驅動開發專案檔案目錄 在規格文件(Specification file)上需要 User 提供一個描述類別圖與精簡狀態圖的 UML 和 OCL file，使用 Eclipse 的 Papyrus 來繪製類別圖與精簡狀態圖， 並使用一個基於 Xtext 所製作的 OCL 編輯器來編輯 OCL。Testing Case 是經由團隊開發的 OCL 語法經由 Antlr 來產生，所以才需要製作一個 OCL 編輯器來進行高亮、語法檢查等功能， 讓使用者能確保語法正確並可以讓 OCL 分析器來進行分析。 fig 17. 規格驅動開發整合環境，其中 CBTCG 是團隊所開發的測試案例產生器。 NOTE Last edit 07-18-2023 12:32 我目前對 Eclipse 與相關環境開發研究到這裡就好，更有興趣的是 OCL 所產生的測試案例，這篇日後再回來讀。"
  },"/jekyll/2023-05-28-characteristics_of_bdd.html": {
    "title": "Paper | A Study of the Characteristics of Behaviour Driven Development",
    "keywords": "software software_qualitiy software_development Jekyll",
    "url": "/jekyll/2023-05-28-characteristics_of_bdd.html",
    "body": "C. Solis and X. Wang, “A study of the characteristics of behaviour driven development”, Software Engineering and Advanced Applications (SEAA) 2011 37th EUROMICRO Conference on, pp. 383-387, 2011. 本論文詳細定義了 BDD 相關文獻與工具而確定的 BDD 特徵，為理解和擴展 BDD 工具包或開發新工具提供了基礎。 Section 2 回顧現有 BDD 研究，3 採用的研究方法，4 介紹了確定的 BDD 概念模型，5 為總結。 1. Intorduction BDD 最初是由 Dan Northp[3]作為對 TDD 中存在的問題的回應而開發的。 [3] D. North, Introducing BDD, 2006. Available at: http://dannorth.net/introducing-bdd [Accessed December 13, 2010]. TDD 的介紹看這裡，Acceptance Test Driven Development (ATDD)(驗收測試驅動開發)[1][2]是 TDD 的一種種類，其中開發過程由代表利益相關者需求的驗收測試驅動。 驗收測試的通常是非開發者也能閱讀的文件，也可以是可執行的自動化測試，以確保利益相關者可以閱讀與理解，做為開發中的驗收標準。 利益相關者: 例如項目負責人、用戶、業務代表等。 但許多開發者在使用 TDD 和 ATDD 時會開始困惑，“程式設計師想知道從哪裡開始，應該測試什麼以內以及不應該測試什麼，一次測試多少，如何命名以測試，以及如何理解測試失敗的原因”[3]， TDD 和 ATDD 也存在一些問題，如他們專注的是系統的狀態而不是系統的期望行為，而且測試代碼與實際系統的實現高度耦合[18][20]。並且這些方法中非結構化且無限制的自然語言描述測試案例， 使它們很難被理解[3]。 BDD 被視為上面兩種方法的演進，BDD 的重點是可以自動化的方式定義目標系統行為的 fine-grained specifications(細粒度規範)， BDD 的主要目標是獲得系統的可執行 specification(規範)[3][20]。因此 BDD 中測試寫得很清楚與易理解，BDD 提供一種特定的共通語言， 有助於利益相關者指定其測試。還有各種支持 BDD 的工具如 JBehave [4]、Cucumber [5]和RSpec [6]。 Specification: 在 BDD 中指的是一種以人類可讀的方式表達系統行為期望的描述，用於指導軟件開發和測試。 2. RELATED WORK Carvalho等人[8][9]認為 BDD 是一種規範技術，“通過將這些需求的文本描述與自動化測試相連接，自動確認所有功能需求在源代碼中得到適當處理”。 他們主要關注於 BDD 中形成一種簡單共通語言，使用預先標記的集合來描述需求和測試，使需求轉化為可執行的測試案例。 Tavares等人[7]將焦點放在 BDD 作為一種更廣泛的設計技術或方法論上，強調將驗證和驗收整合到設計階段中，在進入構成功能的每個部份的設計之前， 要先考慮客戶的驗收標準。它們也認為 BDD 很大程度上基於規範任務和測試的自動化，需要適合的工具支持。 Keogh[10]對 BDD 則有更廣泛的觀點，主張 BDD 在軟體開發生命週期的重要性，利用 Lean thinking(精益思想)的概念如: value stream(價值流), pull(拉動), PDCA (Plan-Do-Check-Adapt) cycle(PDCA循環), 來揭示 BDD 的價值，他有力的證明 BDD 比 TDD 對軟體開發過程有更廣泛的影響。 Lean thinking: 一種管理和生產力提升的方法論，旨在減少浪費、增加價值和改進流程來實現組織和業務的成功。 Lazăr等人[11]也強調 BDD 在業務領域和軟體開發的交互中的價值，聲稱 BDD 使開發人員與領域專家能使用相同的語言。他們指出 BDD 的兩個核心原則: 業務和技術人員應該以相同的方式對待同一系統。 任何系統都應對業務具有確定可驗證的價值。 基於這觀點，他們分析了 BDD 方法並將概念作為 Domain model和 BDD profile (BDD 配置文件)來呈現。 Domain model: 用於表示系統的概念模型，描述系統中的各個實體與其之間的相互關係。 BDD profile : 描述 BDD 概念和規範的結構化文件。通常用於描述系統行為與驗收標準的關鍵字、語法和語意。 3. RESEARCH APPROACH 因為論文發表時的 BDD 文獻如第二節所示非常有限，因此作者以包含 TDD 和 Domain Driven Development(DDD) 的文獻回顧後， 以 TDD 作為基準界定 BDD 的具體特徵，因此作者認為的 BDD 具體特徵是那些沒有被做為 TDD 報告的特徵。 作者根據 BDD Wikipediap[13]列出的 40 個 BDD 工具包中選擇。並諮詢了 BDD mailing lists [23]，最後得出七個常用的工具包來分析: Cucumber [5,18]、Specflow [14]、RSpec [6,18]、JBehave [4]、MSpec [15]、StoryQ [12]和NBehave [16]。 Table 1 簡述了分析的七個工具包與其版本。 文獻回顧和工具包分析將交織再一起進行，回顧研究後確立 BDD feature sets(特徵集)，然後逐一分析工具包。當發現一個不在特徵集中的特徵後，會回到文獻中了解他是否可被視為 BDD 特徵， 直到分析完每個工具包。 4. THE CHARACTERISTICS OF BDD 作者透過以上的研究方法，確定了六個 BDD 的主要特徵。 A. Ubiquitous Language Ubiquitous Language(通用語言)是 BDD 中的一個重要概念，它的結構基於Domain model。它包含了定義系統行為所使用的術語，是產品團隊的所有成員和利益相關者共享的一組明確的詞彙。 在設計和實施階段，開發人員將使用該語言來命名 classes &amp; methods. 一個簡單的範例可見 What is Ubiquitous Language? Examples?。 BDD 本身包含一個預設的簡單 Ubiquitous Language 但與特定領域無關，是為了提供一種統一的方式來描述系統行為，用於結構 User Story(用戶故事)和 Scenario Templates(場景模板)。 User Story: 描述系統的功能需求或用戶期望的行為。 Scenario Templates: 定義具體的測試場景的模板，描述特定用戶故事的具體情境，行動和預期結果。 作者所分析的工具包都不支援為項目創建特定的通用語言。 B. Iterative Decomposition Process Iterative Decomposition Process(迭代分解過程)，在收集需求過程中，開發者往往很難找到與客戶溝通的起點，尤其是客戶所需實現的商業價值，Business value(商業價值)往往難以明確與識別。 因此 BDD 中，分析從識別系統的預期行為開始。系統的行為將從它打算產生的 business outcomes(商業成果)中得出。商業成果進一步細化為 feature sets(特徵集合)， 一個特徵集合將一個商業成果分割成一組抽象的特徵，這些特性指明了為了實現商業成果應該做什麼。 假設正在開發一個購物網站，其中一個商業成果就是用戶能下訂單並購買商品，就會包含幾個抽象特徵集如下 註冊和登錄: 用戶註冊新帳號、登錄現有帳號以及管理個人資料的功能。它是實現訂單和支付的前提。 訂單和支付: 用戶下訂單並完成支付的流程。它包括選擇適當的付款方式、填寫運送地址和付款信息等。 瀏覽和搜索: 用戶可以瀏覽網站上的商品列表，並提供搜索功能來快速尋找特定商品。 購物車管理: 用戶將他們感興趣的商品加入購物車，管理購物車中的商品數量、移除商品或更新數量。 每個特徵集和代表一個高層次的抽象，而其中的特徵則是進一部細分的具體功能與行為。 考慮到商業成果是 BDD 過程的起點，因此客戶需要明確指定商業成果的優先級，以使開發人員知道應首先開發哪些特徵集。 特徵隨後可以通過 User Story(用戶故事)來實現，提供了特徵的上下文。用戶故事是以用戶為導向的，描述用戶與系統之間的互動，其中應該澄清三個問題: 用戶在用戶故事中的角色是什麼? 這有助於明確指定使用者的身份和角色。 用戶希望有哪些特性? 這描述了用戶對系統功能的期望和需求。 如果系統提供了該特性，用戶可以獲得什麼好處? 這闡述了系統功能對用戶帶來的價值和利益。 在不同的情境下，一個用戶故事可能有多個版本，這些具體的實例就被稱作 Scenario(場景)。這些情境與結果通常由客戶提供，BDD 中場景被用作驗收標準， 用於驗證系統是否按造用戶故事的要求正確運作。 這種分解過程應該是 iterative(迭代)的，也就意味著對於每個不同的層級進行初步的分析即可，再隨著開發逐步詳細每個層級的細節，這樣就能使團隊有一個高層級的視圖再開始工作， 這樣可以使團隊更快的開始進行實現工作，同時保留靈活度與可調整性。 同樣以購物網站為例 初步分析：首先進行初步分析，識別出購物網站的高層級特徵集，如商品展示、購物車、付款和訂單管理等。 迭代分解：接下來可以選擇一個特徵集，如商品展示進行分解。識別出更具體的特徵，如分類展示、搜索功能、評論和評分等。 細化特徵：在這個特徵集中可以進一步細化特徵。如搜索，可以定義更具體的需求，如關鍵字、過濾器和排序等。 雖然作者所分析的工具包(2011)當下都不支援迭代分解，但現在已經有一些工具包支援迭代分解中的需求分析與設計。 C. Plain Text Description with User Story and Scenario Templates Plain Text Description with User Story and Scenario Templates(純文本描述的用戶故事與場景模板)， BDD 用簡單的通用語言與模板描述 features, user stories, scenarios, 如 User stories 以下用 Dan North 的模板來說明[3]: [StoryTitle] (One line describing the story) As a [Role], I want a [Feature], So that I can get [Benefit] StoryTitle 與 Role 描述給定角色下用戶執行的活動，Feature 則能確保開發人員知道應該實現哪些特徵與系統行為，為什麼要有這個功能以及該與誰討論與分析該功能。 也能清楚的說明 Feature 能帶給用戶什麼 Benefit，為什麼需要這些 Feature. Scenario 的撰寫模板則如下: Scenario 1: [Scenario Title] Given [Context] And [Some more contexts]…. When [Event] Then [Outcome] And [Some more outcomes]…. Scenario2: [Scenario Title] …. 場景描述了當系統在特定狀態下發生事件時應該如何行動，場景的結果是改變系統狀態或輸出的動作。對於上面兩種模板中括號中的描述應該使用專案中定義的 Ubiquitous Language 來撰寫。 此外要將它們直接映射進專案中，也意味著 Class 的命名和方法也應該使用 Ubiquitous Language 來撰寫。 作者分析的四個工作包使用的模板都與[3]的略有不同，但都定義了 User story 中的 role, feature, benefit。 在當下已經有 BDD 穩定且廣泛應用的通用語言模板格式，詳情可以見 Gherkin，被 Cucumber 所支援。 D. Automated Acceptance Testing with Mapping Rules Automated Acceptance Testing with Mapping Rules (使用映射規則的自動化驗收測試)，BDD 繼承了 ATDD 的自動化驗收的特點。 在 BDD 中驗收測試是一個可執行的規範，驗證對象之間的 interactions (交互)或 behavior(行為)，而不只是狀態[3][20]。 Automated Acceptance testing: 使用自動化測試工具來執行對系統行為的驗證，這些測試通常是從 User story, Scenario 中產生的。 開發者將從一個迭代分解的過程中生成的 Scenario(場景)開始，場景將被轉化為測試驗證實現。場景的每一個步驟是一個表示場景中的抽象元素，這些元素包括: contexts, events, actions。例如在 User story 或 context C 的特定情況下，當 event X 發生，系統地回答應該是 Z。每個步驟都被映射到一個測試方法。 每個步驟都將遵循 TDD 的流程即 “red, green, refactoring” 以使其通過。 Mapping rules(映射規則)則提供了一個場景到 Test code(測試代碼)或 Specification code (規範代碼)的標準映射。作者所研究的工具中映射規則有不同的變化: JBehave: 一個 User story 是一個包喊一組 Scenario 的文件，文件的名稱被映射到 Class 的命名，每個 Scenario 的步驟都被映射到一個 Test Method(測試方法)。 通常測試方法的名稱與 User story 文本相同。包含測試方法的 Class 則不須與場景相同。詳情可見 JBehave Writing Textual Stories。 Cucumber: Cucumber使用正則表達式進行映射，映射方法可見 Cucumber Expressions。 E. Readable Behaviour Oriented Specification Code Readable Behaviour Oriented Specification Code (可讀的行為導向的規範代碼)，BDD 建議代碼應該是系統文檔的一部份，與敏捷的價值觀一致。 代碼應該是可讀的，規範應該是代碼的一部分。因此 Method name 應該指出 Method 應該執行的操作，Class, Method name 都應該以句子形式撰寫。 Mapping rules 有助於生成可讀的行為導向的代碼，它確保 Class, Method name 與 User story 和 Scenario 的標題相同。 目前大部分的 BDD 工具都支持將 Scenario 中的規範轉為代碼的方法。 F. Behaviour Driven at Different Phases Behaviour Driven at Different Phases(不同階段的行為驅動)，這裡討論了行為驅動在軟體開發中的不同階段。 計畫階段: 定義商業成果，描述系統應該實現的期望行為。例如: 用戶能夠完成購物並順利支付訂單。 分析階段: 商業成果被分解為一組特性，這些特性捕捉目標系統的行為。例如: 用戶註冊，商品管理，訂單處理等等.. 這些特性將會被轉化為 User stoies。 實施階段: Automated Acceptance testing 中 Testing Clases 是根據 Scenario 所產生的。 因此 Class name 指明了該 Class 該做什麼或行為是什麼，這使開發者能考慮它們開發中的組件的行為，以及與之交戶的其他對象的角色與責任。 在作者研究的當下還沒有針對定義商業成果也就是計畫階段的支持，大部分的工具包都專注於 User stoies, Scenario 的撰寫與測試自動化。 Table 2 總結了作者分析的七個工具的對於這些特徵的支持情況 Fig 1 是一個以 UML class diagram 來呈現的六個特徵之間的概念與關係模型 V. CONCLUSIONS BDD 是多種方法的結合，如 ubiquitous language, TDD, automated acceptance testing。作者通過文獻回顧與工具包的分析來確認了六個 BDD 的主要特徵。 並透過逐一分析研究表明了這些特徵之間的互相關聯。並提出了一個 BDD 的概念模型 Fig. 1.。 而對於新的研究方向則指出， 2011 的當下工具包都主要關注於軟體開發的實踐階段，對於分析的支持有限，而計劃階段則根本沒有，這是一個可擴展的研究方向。 而另一項則是可以擴展 BDD 的映射規則，2011 現有的工具包都關注於 User story, Scenario 映射到代碼，此外 feature sets(特徵集) 也可以被映射到命名空間中， 在此之下再加入場景的測試[10]。 [10] E. Keogh, BDD: A Lean Toolkit. In Processings of Lean Software &amp; Systems Conference, Atlanta, 2010. NOTE Last edit 06-18-2023 22:56"
  },"/jekyll/2023-05-20-analysis_mutation_testing.html": {
    "title": "Paper | An Analysis and Survey of the Development of Mutation Testing",
    "keywords": "software software_qualitiy mutation_testing Jekyll",
    "url": "/jekyll/2023-05-20-analysis_mutation_testing.html",
    "body": "Yue Jia, Mark Harman, … (2011), An Analysis and Survey of the Development of Mutation Testing. 用這篇論文熟悉什麼是突變測試(Mutation Testing). 突變測試是用來檢測測試集合能否發現故障的能力， 這篇文章全面分析了一系列突變測試的方法、工具、發展和驗證結果，本文引用眾多且詳細，閱讀時要搭配原文的索引閱讀。 Section 2 是突變測試的幾本理論，3 ~ 5 詳細介紹突變技術與應用，6 是總結研究經驗，7 是突變工具的開發工作，8 討論目前的證明，9 則是尚未解決的問題與障礙，10 為總結。 Section 3 以後是詳細技術介紹，如果只想簡單理解什麼是突變測試讀 1、2、10 即可。 1. Intorduction 突變測試(Mutation Testing)是一種基於故障的測試技術，可以提供一個稱為 Mutation adequacy score(突變適應性分數)的測試準則來評估測試集合在檢測故障方面的有效性。 突變測試使用的故障代表程序員常會犯的錯誤，我們也可以模擬任何 Test adequacy criteria(測試適當性準則)。透過簡單的語法變化崁入原始程式中，稱為突變體。這些突變體會執行輸入輸出測試集合，如果一個突變體的結果與原始程式不同就表示檢測到一個故障突變體，亦稱為killed mutant(殺死突變體)。 Mutation adequacy score： 突變測試的測試準則，是檢測到的突變數量與生成突變體的總數的比例，公式為(檢測到的突變/生成突變數量)。 Test adequacy criteria: 一組可用於判斷是否進行了充分測試的規則，也可指導測試數據的選擇，明確的說明如何選擇測試數據。 Mutation score = (number of killed mutants/total number of mutants killed or surviving) x 100 突變測試的概念可以追溯到 1971, 當時 Lipton 在一篇學生論文中提出了這一概念。Mutation testing 可以應用在軟體測試的單元層、集成層和規範層。已經在許多編程語言中得到應用，是一種白盒單元測試技術。同時也可以在設計層次用於測試程式的規範或模型， 如 Mutation Testing已經應用於有限狀態機、Statecharts、Estelle規範、Petri網絡、網絡協議、安全策略和Web服務等領域。 本文作者整理了一系列的突變測試論文，將重要的論文註記於 Table. 1.，及關於增長趨勢的圖表 Fig. 1.。 2. The theory of mutation testing 2.1 Fundamental Hypotheses 突變測試希望能夠有效的識別足夠的測試數據，用於發現真正的故障。[96]但是潛在的故障是巨大的，不可能代表所有的突變體，這是一個針對故障的子集，希望接近模擬所有故障。 這個理論建立於兩個假設 the Competent Programmer Hypothesis(CPH) 和 Coupling Effect(CE)： CPH(合格程序員假設)[3][66]： 假設編程人員是有能力的，他們盡力去更好地開發程序，達到正確可行的結果。因此可能有程式中有錯誤，但只是一些小的語法更改修正的簡單錯誤。 並且在部分論文中也引入了 Program neighborhoods(程序鄰域)的概念[37] CE(耦合效應)[66]： [66]提出後， Offutt 進行了擴展 Coupling Effect Hypothesis， Mutation Coupling Effect Hypothesis[174][175]。 Coupling Effect Hypothesis：測試數據集可以檢測到由簡單錯誤，同時也能隱含地檢測更複雜的錯誤。 Mutation Coupling Effect Hypothesis: 因此複雜突變體與簡單突變體之間也存在著耦合關係。 因此傳統 Mutation testing 中僅僅會使用簡單突變體進行測試。 [3] A.T. Acree, T.A. Budd, R.A. DeMillo, R.J. Lipton, and F.G. Sayward, “Mutation Analysis,” Technical Report GIT-ICS-79/08, Georgia Inst. of Technology, 1979. [66] R.A. DeMillo, R.J. Lipton, and F.G. Sayward, “Hints on Test Data Selection: Help for the Practicing Programmer,” Computer, vol. 11, no. 4, pp. 34-41, Apr. 1978. 關於 CEH 的有效性已經有很多研究驗證[145], [164], [174], [175]。其中證明了從一階突變體生成的測試及對於 kth oredr 的突變體同樣能有效指出錯誤 (k = 2; … ; 4)， 能殺死一階突變體的有效數據同樣也能殺死 99% 以上的二、三階突變體。[242], [243], [244], 提出了一個簡單的理論模型 the q function model，將程式視為一組有限函數。 將測試集用於一階與二階模型，存活比例分別為 $\\frac{1}{n}$ 和 $\\frac{1}{n^2}$ n 是階數。[125]中可以找到關於 the boolean logic faults 的耦合效應的正式證明。 [125] K. Kapoor, “Formal Analysis of Coupling Hypothesis for Logical Faults,” Innovations in Systems and Software Eng., vol. 2, no. 2, pp. 80-87, July 2006. 2.2 The Process of Mutation Analysis Fig. 2. 是突變分析的傳統流程圖，將原始程式進行單一的語法改變，產生有缺陷的程式 P’，稱作 The mutant(突變體)，如 Table. 2. 僅將 &amp;&amp; 改變為 ||。 這種轉換規則稱作 mutation operator(突變運算符)，典型的突變運算符用於替換、插入或刪除運算符來修改變數和表達式。 Table. 3. 是 Fortran 的第一套正式突變運算符， 在 Mothra mutation system 上被實現。 [123] 使用了一種腳本語言 Mutation Operator Constraint Script (MOCS)，提供兩種類型的約束: Direct Substitution Constraint: 允許用戶選擇特定的轉換規則來執行簡單的變更，如將一種運算符轉換為另一種。 Environmental Condition Constraint: 指定適用於突變的特定環境條件，例如突變操作只在特定的作業系統下生效。 [217] 提出一種轉換語言 MUDEL，用於指定突變操作符的描述，可以定義為捕捉某種程式中的語法規則進行修改。更詳細的說明可在 Offutt 等的工作中找到[177] 然後在下一步中，我們要先確保原始程式 p 能通過測試集合 T，以驗證測試集合的正確性。 T 中的每個測試用例運行 p’ 與 p 的結果相同則稱為 survived(存活)，否則則稱為 killed(殺死)。 killed: 代表能夠找出 p’ 的錯誤，因此也代表能找出 p 的錯誤。 survived: 無法檢測出 p’ 的錯誤，代表這個測試用例也無法找出 p 的錯誤。 因此我們會希望一個測試用例能殺死盡可能多的突變體，因為這樣才代表這個測試用例是有效的能檢測出多個錯誤情況。 為了改進 T，測試者可以提供額外的輸入來殺死存活的突變體。但有些突變體是無法被殺死的，他們稱為 Equivalent Mutants(等效突變體)， 它們在語法上有所不同但功能等同於原始程式。自動檢測是不可能的[35]，[187]，因 program equivalence 是無法判定的， 因此這是阻礙突變測試應用的障礙之一。 關於突變分析的最終目的就是 Adequacy Score(適應性分數)，即是 Mutation Score(突變分數)，他表示輸入測試集的品質。 突變分析的目標是將分數提高到 1，表示測試集合 T 足以檢測到突變體表示的所有故障。 2.3 The Problems of Mutation Analysis 阻礙突變測試成為實用測試技術的第一個問題是對測試集執行大量突變體的高計算成本。其他問題則與使用突變測試投入的人力成本有關， 如 Human oracle problem(人類預期問題)[247]和 Equivalent mutants(等效突變體問題)[35]。 Human oracle problem: 是指每個測試用例需要人類來驗證測試結果的問題，花費時間來檢查結果是否符合預期。 Equivalent mutants: 由於不可判定性，往往需要額外的人力投入。 現有的突變測試進展，雖然還未完全解決這些問題，但突變測試的過程已經可以自動化，並且運行時可以實現合理的擴展性。 3. Cost Reduction Techniques 傳統的 Mothra 中所有的突變體都要被考慮在內，為了使 Mutation testing 成為實用的測試技術，許多成本降低技術被提出，[191]的調查中分為三類: “do fewer”, “do faster”, “do smarter”. 在本文中將技術整理為兩類，並在 3.1 與 3.2 各自介紹: reduction of the generated mutants: 減少生成的突變體對應 “do fewer” reduction of the execution cost: 減少運算成本對應 “do faster”, “do smarter” Fig. 3. 是已發表關於降低成本的想法的時間發展與情況 作者整理的當下 Selective Mutation(選擇性突變), Weak Mutation(弱突變)是最廣泛使用的成本降低技術，見論文 Fig. 4. 3.1 Mutant Reduction Techniques 再不嚴重影響測試效果的情況下減少產生的突變體數量是一個熱門的研究問題。對於一組給定的突變體 M 和測試數據集 T，$MS_T(M)$ 表示 T 應用於 M 的突變分數。 突變體減少可以定義為在 M 中找到一個子集 M’ 而 $MS_T(M) = MS_T(M’)$。 3.1.1 Mutant Sampling 突變體抽樣是一種簡單的方法，從整個集合中隨機選擇一小部分突變體，最早由[2][34]提出。首先像傳統的突變測試一樣生成可能的突變體，然後從這些突變體中隨機選擇 x% 進行分析， 其餘的則丟棄。 在[159][248]中這種方法進行了驗證，從 10% 到 40% 之間，結果顯示 10% 與全選相比效果僅下較 16%，表明 x &gt; 10 時，抽樣是有效的，在 [64][131] 中也得到了驗證。 除了固定抽樣率，[207]提出一種基於 the Bayesian sequential probability ratio test(SPRT)(貝葉斯序列概率比檢驗) 的抽樣方法，突變體是隨機選擇直到達到統計學上的合適樣本量為止， 這種方法比固定抽樣更敏感，因為他們是基於可用的測試集自我調整的。 [207] M. Sahinoglu and E.H. Spafford, “A Bayes Sequential Statistical Procedure for Approving Software Products,” Proc. IFIP Conf. Approving Software Products, pp. 43-56, Sept. 1990. 3.1.2 Mutant Clustering 突變體聚類最早在[116]提出，使用 clustering algorithms(聚類演算法)選擇一個突變體子集。先生成突變體，然後應用聚類演算法，根据可殺死的測試案例將一階突變體分類到不同的聚類中。 同一集群中的每個突變體都保證被一組類似的測試用例殺死，然後在每個集群中選擇少量的突變體用於測試，其餘的丟棄。 [116]中使用兩種聚類算法 K-means 和 Agglomerative clustering，並將結果與隨機和貪婪選擇策略做比較，結果表明聚類能選擇更少突變體並保持突變分數。後續發展可在[120]中找到， 使用了一個 domain reduction technique(領域縮減技術) 來避免執行所有的突變體。 3.1.3 Selective Mutation 選擇性突變透過減少應用的突變運算符來實現減少突變體數量。試圖找到一小部分突變運算符來產生可能突變體中的子集，而不會對測試效果產生重大損失。 最早可見於[156]提出的 “constrained mutation”，[190]隨後擴展了這個想法，稱為選擇性突變。 突變運算符生成的突變體數量各不相同，如在 Mothra 中，ASR 和 SVR 兩個運算符生成了約 30 ~ 40% 的突變體[131]。[156]建議省略這些產生大部分突變的運算子 ASR、SVR。 之後 Offutt[190] 將其擴展至四個和六個，在他們的研究中: 2-selective mutation: 99.99 的平均突變分數，減少 24% 的突變體。 4-selective mutation: 99.84 的平均突變分數，減少 41% 的突變體。 6-selective mutation: 88.71 的平均突變分數，減少 60% 的突變體。 [190] A.J. Offutt, G. Rothermel, and C. Zapf, “An Experimental Evaluation of Selective Mutation,” Proc. 15th Int’l Conf. Software Eng., pp. 100-107, May 1993. [248][252]則基於測試效果的選擇，被稱為 constraint mutation(約束性突變)，僅採用兩個操作符進行突變 ABS、RAR 因為殺死 ABS 需要 input domain(輸入域)的不同部分測試用例， 而殺死 RAR 需要檢查 mutated predicate(突變謂詞)的測試用例。結果表明可以將突變體數量減少 80%，而僅對突變分數減少 5%。 [252] W.E. Wong and A.P. Mathur, “Reducing the Cost of Mutation Testing: An Empirical Study,” J. Systems and Software, vol. 31, no. 3, pp. 185-196, Dec. 1995. [182] Offutt等人以此進一步擴展了它們的 6-selective mutation，將 Mothra 運算符分為三類：statements, operands, expressions 之後依次省略每一類的運算符， 最後發現來自 operands, expressions 兩類的 ABS, UOI, LCR, AOR, ROR 這些關鍵的運算符取得了 99.5 的變異分數。 基於之前的經驗，Barbosa 等人[19]定義了一個選擇足夠操作符的指南，它們將這個指南應用於 Proteum 的 77 個 C mutation operators [6]，得到一組 10 個選定的突變操作符， 其平均突變分數為 99.6% 並且減少了 65.02% 的突變體。 並與 Offutt 和 Wong 做比較得到最高的突變分數。 [19] E.F. Barbosa, J.C. Maldonado, and A.M.R. Vincenzi, “Toward the Determination of Sufficient Mutant Operators for C,” Software Testing, Verification, and Reliability, vol. 11, no. 2, pp. 113-136, May 2001. 而最新研究是 Namin 和 Anderws 進行[168][169][170]，將選擇性突變問題定義為統計問題，使用線性統計從 109 個 C mutation operators 中識別出 28 個操作符的子集， 目前他們減少了 92% 的突變體，是論文(2011)當下最高的減少率。 3.1.4 Higher Order Mutation 高階突變由 Jia 和 Harman (2008)提出[122]，基本動機是尋找那些罕見但有價值的高階突變，first order mutants (FOMs) 和 higher order mutants (HOMs)， HOMs 是通過多次應用突變操作符來生成的。 使用了 subsuming HOMs 的概念，一個 subsuming HOMs 比建構她的 FOMs 更難被殺死。因此使用單一的 HOM 來取代 FOM 以減少突變體的數量。 除此之外他們還使用了 strongly subsuming HOM(SSHOM) 的概念，他只被能夠殺死構成他的 FOM 的測試用例的交集的子集才能夠殺死他。 Polo等人[199]部分證明了這個想法，他們提出了不同的算法將一階段組合成二階突變。應用二階突變可以減少 50% 的測試工作量，而測試效果幾乎沒有損失。 Langdon等人[136][137]應用 multi-object genetic programming(多目標遺傳編程方法) 生成高階突變體， 他們發現了比任何一階突變體更難殺死的 realistic higher order mutants(現實高階突變體)。 multi-object genetic programming(MOGP): 一種進化計算技術，結合 Genetic Programming(遺傳編程)，Multi-Objective Optimization(多目標優化)的概念與方法。 realistic higher order mutants: 這些高階突變體可以更好地模擬真實世界中的軟件錯誤，並提供更有挑戰性和現實性的測試用例。 3.2 Execution Cost Reduction Techniques 本節介紹三種優化執行過程的技術類型 3.2.1 Strong, Weak, and Firm Mutation 根據分析突變體在執行過程中是否被殺死的方式，突變測試技術可分為三種類型: Strong(強突變)、Weak(弱突變)和 Frim(穩固突變) 強突變通常被稱為傳統的突變測試。最初由DeMillo等人提出[66]。對於給定的程序 p，如果突變體 m 與原始程序 p 的輸出不同，則認為突變體 m 被殺死。 NOTE Last edit 05-26-2023 20:52 本篇論文先閱讀完基本的突變測試，之後再來補齊"
  },"/jekyll/2023-04-21-tdd_concepts.html": {
    "title": "Paper | Test-driven development concepts, taxonomy, and future direction",
    "keywords": "software software_qualitiy software_development Jekyll",
    "url": "/jekyll/2023-04-21-tdd_concepts.html",
    "body": "John Estdale &amp; Elli Georgiadou, (2005) Test-driven development concepts, taxonomy, and future direction. 這篇論文可以用來快速了解什麼是 TDD(Test-driven development) 測試驅動開發 Intorduction 測試驅動開發策略要求在小型、快速的迭代(Small, rapid iterations)中先編寫自動測試，然後再開發功能代碼。這種開發策略作為極限編程(XP, Extreme programming)核心實踐之一而持續受到關注。 XP, Extreme programming 是敏捷軟體開發(Agile software development)的一種方法，強調非常短的迭代進行軟體開發。 小型快速的迭代(Small, rapid iterations)是一種敏捷開發方法論，將軟體開發過程劃分為一系列小模塊，每個模塊都有各自的開發與測試。 例如: 一個團隊開發一個線上商店，可將功能分為各個小的模塊: 用戶註冊、商品搜索、購物車管理、訂單處理等等，然後透過迭代週期來完成和測試各個模塊。 The test aspect 除了 High-level 的測試之外，TDD要求編寫單元的自動化測試。 在軟體中什麼是確切的單元有一些爭議，即使在 OOP 中 Classes 和 Method 都被建議作為合適的單元。 無論如何 Method 和 Procedure 都是最小的可測試單元組件。 開發者需要實現測試驅動(Test drivers)與模擬函數(function stubs)，可以經由自動化(JUnit, …)或手動化測試。 Test drivers: 一個可以執行單元測試的程式，已確定代碼是否正確通過測試。 Function stubs: 一個虛擬的函數，通常只有命名、輸入與輸出參數。 因此在TDD中開發者需要先寫好單元測試，然後代碼完成後就可以立即執行測試。 The driven aspect TDD是一種測試策略，強調測試先行，通過測試來引導軟體開發中的分析、設計、和編寫決策。 在 XP 中客戶也被視為開發者的一員，提供更清楚的需求，以此來更清楚的撰寫測試，測試就是決定程式應該做什麼的第一步。 為了促使測試成為分析和設計的一部分，需要使用重構(refactoring)的作法。 就是不斷改變現有程式碼的結構，但依然要通過測試，使程式碼得到改進。 因此TDD更多著重的是分析和設計，而不是測試，測試是用來幫助開發者決定程式或程式介面應該是什麼樣子。 The development aspect TDD旨在協助構建軟件開發，但它不是一種軟件開發方法論或過程模型。相反，它是一種實踐方法，可以與其他方法相結合。 如結合其他開發方式，在 DevOps 中 TDD, BDD 都被強調為測試過程的重要方法，可以參考[1]。 同時測試不是為了做出設計決定後就被拋棄的，而是成為開發過程中的一個重要步驟。 如果有一個變化導致測試失敗，當測試還在開發者的腦海中時，開法者可以立刻知道哪裡出錯。缺點就是在開發時必須同時維護測試與生產代碼。 [1] Pulasthi Perera, …, (2017) Improve Software Quality through Practicing DevOps, IEEE Software development methodologies 在軟體開發過程或方法論定義了建立軟體的基本任務(base task)的順序(order)、控制(control)、評估(evaluation)。在這些方法論中的複雜度與控制範圍從非正式的到高度結構化不等 方法論分為兩大類: Prescriptive(規定型), Agile(敏捷型) 具體可分為: Waterfall(瀑布式), Spiral(螺旋式), Incremental(增量式), Evolutionary(演進式) 但在開發中通常是組合使用這些方法，例如: 一個組織可能使用增量式開發模型，逐步建構項目的累積片段。而在每個增量中開發者可以應用瀑布或線性方法進行開發。那麼根據增量的大小我們就能將整個方案標記成不同的方法論。 假設方程式 $\\sum_{i=1}^{N} I_i$ 代表整個專案，$I_i$則是每次的增量，我們可以去預想如果$N$大於一定量則是一個增量項目，若$N\\leq2$則使用瀑布項目。如果每個增量需要修改大量重複的軟體部份，我們就可以說具有迭代性，例如： 如果$C_i$是$\\sum_{i=1}^{N} I_i$的項目$P$中的每次增量被影響的代碼部分，並且項目$P$有迭代性則$C_i \\cap C_{i+1} \\neq \\emptyset$ Prescriptive(規定型)通常會希望有一份正式的文件，如規範文件來記錄增量的需求。 Agile(敏捷型)則文件通常是非正式的，如白板圖或一套不完整的UML圖，並且生成是快速的。 建設任務的順序對一個項目來說至關重要，傳統的順序是： Requirements elicitation(需求徵詢), Analysis(分析), Design(設計), Code(編碼), Test(測試), Integration(集成), Deployment(部署), Maintenance(維護) 在開發過程中我們能發現在Design, Code, Test階段都有不同種類的測試，如：單元測試, 整合測試, 回歸測試 TDD’s historical context Test-driven development(測試驅動開發)是與敏捷模型的興起一起出現的，都起源於20世紀50年代的迭代，增量，進化的過程模型。 將測試移動到編碼的前方並不是什麼新鮮事，在1980’s的 Cleanroom 軟體工程方法就已經包含了使用 Formal methods 對早期設計元素進行驗證。 在1998 XP(極限編程)後開始推崇先寫測試在寫程式，但在那之前就可能有非正式的測試先行的方法。 [2] K. Beck, Extreme Programming Explained: Embrace Change, Addison-Wesley, 1999. “learned test-first programming as a kid while reading a book on programming. It said that you program by taking the input tape … and typing in the output tape you expect. Then you program until you get the output tape you expect.” - Kent Beck [2] TDD就是將這種做法做到極端，總是先寫測試再編程，增量式、迭代式和演進式過程模型的發展對它的出現至關重要。 將測試分解成更小、更簡單、更具體的單元測試，這樣做的好處是能夠更快地找到問題，更容易進行測試和調試。 永遠不使 Code 退化，不允許代碼質量下降或出現新的錯誤。這可以通過不斷地運行測試來實現。 XP takes the known best practices and “turns the knobs all the way up to ten.” - Kent Beck [2] TDD developed within the context of iterative, incremental, and evolutionary models. Iterative 涉及重複一組開發任務，通常是在逐漸擴展的需求集上進行。 Incremental 則產生一系列的版本，每一個增量都提供更多的功能。 Evolutionary 方法涉及自適應和輕量級迭代開發。 Adaptive(自適應): 強調利用過去迭代的反饋來改進軟體。 Lightweight(輕量級): 減少過度的規範和流程，利用反饋進行改進，在最短時間內交付可用的版本。 Spiral Model(螺旋模型): 結合原型(prototyping)和迭代(iterative)的循環，並加入風險驅動和錨點里程碑。 that to implement XP, developers must apply all of the incumbent practices—leaving some out weakens the model and can cause it to fail. - Kent Beck [2] TDD要求將設計決策延遲(design decisions be delayed)和靈活(flexible)以影響軟體設計 設計決策延遲: 在這之前先編寫測試用力，這些測試用力會指導開發者在編寫代碼之前考慮好應該如何設計代碼才能使其滿足需求。這樣開發者可以通過通過測試用例更好的理解問題和需求。 靈活: 在設計過程中，保持靈活性以便能夠適應可能出現的變化。這意味著編寫足夠通用的測試用例，以便代碼可以更容易地進行修改，而不會破壞之前的測試。 一個自動化的測試將給予開發者勇氣重構程式碼，也給了他們需要的訊息，使他們能在改變程式碼後迅實現修改，而實現開發人員對代碼的集體所有權(Collective ownership)。 集體所有權(Collective ownership): 所有的開發人員都有對代碼的共同責任和掌控權，因為TDD自動化單元測試可以使開發者迅速檢查是否有破壞其他開發者的工作。 Automated testing Software tools 已經成為現代軟體開發的重要因素。Compilers, Debuggers, IDE, Modeling, Computer-aided software engineering tool, 這些工具提高了開發者的生產力。 TDD假定存在自動化的單元測試框架，這樣的框架提供了測試驅動(test driver), 存根(stub), 子系統的接口(interfaces to other subsystem)的測試組合。 Erich Gamma 和 Kent Beck 開發了 JUnit, 一個 Java 的自動化單元測試框架，JUnit 在很大程度上促進了 TDD 和 XP 的廣泛普及。類似 JUnit 的框架也已實現於多種不同的程式語言中，創建了一系列的框架，被稱為 xUnit。 xUnit 一般來說 xUnit 允許開發者撰寫一系列自動化單元測試，從初始化(Initialization), 執行(Execution)並對測試代碼進行斷言(Assertion)。 各個測試都是獨立的，所以測試順序並不重要。xUnit 測試是用與被測試代碼相同的語言編寫的， 同時測試也可以作為文檔(docs)，開發者通過閱讀測試程式碼來了解被測試程式碼的行為和功能。 JUnit 也提供了一個可移植的 GUI，已經被集成到流行的開發環境中，如 Eclipse。 JUnit 中一些工具簡化了 Mock object(模擬對象)、Stub(存根) 的創建，這些可以取代真實的協作對象，因此開發人員就能專門測試一個特定的對象。 同時也可以使用其他工具如 Cactus, Derby 和 JUint 一起實現涉及 J2EE 組件或 Database 的自動化測試。 支持 TDD 的工具不斷增加顯示出了 TDD 受到的支持，並且使開發人員能輕鬆開發單元測試並通過自動化執行大型測試套件，以此迅速獲得有關系統狀態的結果。 Early testing in academia 大學的計算機科學和軟體工程課程可以作為一個指標，來評估軟體實踐的廣泛接受程度。有時學界會領先於實踐，有時則會跟隨，而軟體工程，迭代開發和 TDD 則是後者的模式。 這些軟體工程課程往往落後於業界的普遍實踐，因此往往是實際軟體開發使用了新的開發過程模型，再由學者研究、最終成為課程的一部分。 1991 ACM 課程指南中迭代開發和驗證只分配了不到八小時的講座與實驗時間。而在 2001 年則分配到了更少的時間僅有五小時。 在文章撰寫的當下 2005 TDD 並還沒有被學界廣泛接受。 Recent context (2005) XP 是當時最著名的敏捷方法，經常與其他敏捷方法(例如: Scrum) 結合使用，XP 提出使用 TDD 作為開發高品質軟體的一個組成部分。 TDD 的潛在使用者常常對於編寫和維護測試單元感到擔憂， Beck 承認自動化單元測試並不是對所有事情都必要的， 但是他也堅稱 XP 無法在沒有 TDD 的情況下運作，因為它是作為將整個流程黏合在一起的黏合劑。 盡管 XP 的流行但並不代表使用時會採用它的所有實踐，或者他們不連貫的使用這些實踐。 ThoughtWorks 的一個專案中，J. Rasmusson 是一個早期的 XP 使用者，但約有 1/3 的代碼是使用 TDD 開發。[3] 在這個專案中，開發者在 37,000 行代碼中有 16000 行是用於自動化單元測試，許多測試都是在測試優先和測試最後的迭代中寫的。[3] 因此口頭證據表明，即便只採用 XP 的部分實踐，TDD也通常會包含在內。 [3] J. Rasmusson, “Introducing XP into Greenfield Projects: Lessons Learned,” IEEE Software, May/June, 2003, pp. 21-28 “If I could only recommend one coding practice to software developers, those who use XP or otherwise, it would be to write unit tests.” - J. Rasmusson [3] 在當時流行的 IDE Eclipse 中，JUnit, XP 這樣的流行結合也可以意味著 TDD 被廣泛採用的一部分證明。 Evaluative TDD research 在論文當下(2005)大部分文章都是關於應用 TDD 所寫，對於 TDD 的效果與好處的研究相對較少。 關於 TDD 的研究大致可依背景分為業界與學術的研究: 業界研究往往更注意實際應用與實用性，並著重於現有實踐進行評估與改進。 學界則更注重基礎理論與科學方法，著重於深入研究問題背後的原因與機制。 TDD in industry NCSU(North Carolina State University) 的研究者在四家不同公司進行了三項關於 TDD 的實證研究[8][9][10]，參與者是一些相對小的團退。這些研究以缺陷密度(Defect density)作為衡量品質的指標。 使用 TDD 進行開發的控制組比對照組多通過 18% - 50% 的外部測試用例。 同時使用 TDD 開發的除錯(debugging) 時間花費更少。 Table 1 summarizes these studies and labels each experiment as either a case study or a controlled experiment. [8] B. George and L. Williams, “A Structured Experiment of Test-Driven Development,” Information and Software Technology, vol. 46, no. 5, 2004, pp. 337-342. [9] E.M. Maximilien and L. Williams, “Assessing Test-Driven Development at IBM,” Proc. 25th Int’l Conf. Software Eng. (ICSE 03), IEEE CS Press, 2003, pp. 564-569. [10] L. Williams, E.M. Maximilien, and M. Vouk, “Test-Driven Development as a Defect-Reduction Practice,” Proc. 14th Int’l Symp. Software Reliability Eng. (ISSRE 03), IEEE Press, 2003, pp. 34-45. TDD in academia 盡管在學術環境中很多 TDD 相關的研究都是軼事，但在 Table 2. 列出的五個研究則有具體的實證結果。 除了 [11] 其他都聚焦於 TDD 的早期缺陷檢測能力。 [11] [12] 報告了軟體品質與程式開發者生產力的明顯改善。 [13] 報告撰寫的測試數量與生產力之間的相關性。 [14] [15] 則報告了缺陷密度和生產力方面並沒有明顯改善 以上報告都相對較小，並且沒有或很少有具有 TDD 經驗的開發者參與。 Factors in software practice adoption 多種因素影響是否採用軟體實踐其中包含: Motivation for change(改變的動機), Economics(經濟), Availability of tools(工具的可用性), Training and instructional materials(訓練和培訓工具), A sound theoretical basis(紮實的基礎理論), Empirical and anecdotal evidence of success(成功的經驗和軼事), Time(時間), The practice by highly regarded individuals or groups.(權威個人或團體的做法) Motivation for change: 軟體開發實務上有明確的改變動機，開發涉及人員、流程、技術和工具等複雜的組合，TDD 為一種嘗試改進並看似有效的方法。 Availability of tools: TDD 的工具支持很強，並且正在不斷改進，像 JUnit, MockObjects, Cactus, 這樣的工具已經成熟並廣泛可用。 Economic: 經濟模型上也指出了 XP 和 TDD 在軟體開發上的潛力，但需要進一步研究 TDD 和 XP 帶來的優缺點，如: TDD 在配合代碼使用時需要研究它在速度上是否會慢於傳統開發，與對缺陷密度(defect density)的影響。 A sound theoretical basis &amp; Empirical and anecdotal evidence of success: 研究表明，學術開發需要 5 - 15 年時間才能在商業實踐中取得成功，反之亦然。TDD 可以改善開發教育。 TDD 的普及面臨許多挑戰，首先開發者要有良好的紀律，因為 TDD 需要開發者遵從他的流程與步驟，因此需要使開發者充分理解 TDD 的好處，才能使開發者嘗試使用。 TDD 也被廣泛的誤解，很多人錯誤的認為 TDD 只關注測試，而不是設計。TDD 不僅僅是關注測試，它同時也要求開發人員在編寫程式碼之前，必須先清楚地設計出程式的架構和功能。在這個過程中，開發者必須思考如何編寫最小程式碼，減少重複代碼，設計清晰的介面等，要求開發者在設計和開發過程中相互交替地進行，從而可以促進高品質、易於維護和可擴展的程式碼產生。 TDD 並不適用於所有開發場景，開發者和管理者必須決定何時用 TDD 何時不用。 Understanding TDD’S effects 在 2005 論文發表時，TDD 對於品質的影響都將焦點放在缺陷密度(defect density) 上，應該有其他方式評估軟體品質。 未來的研究應該考慮在課程和程式設計師成熟度不同的情況下，TDD的有效性。還可以研究 TDD 與先設計後測試的方法、迭代方式進行的測試最後方法的效果差異。 也需要研究 TDD 與其他實踐結合的效果，如配對編程(Pair programming), 代碼檢查(Code inspection) 結合的效果，並且研究 TDD 是否可以納入大學教育中， 以提高學生的設計和測試能力。 XP-EF, 一個持續進行評估 XP 專案案例研究的框架[16] 配對編程(Pair programming): 兩名程式開發人員共同在同一臺電腦上工作，一人負責寫代碼，另一人負責檢查。 代碼檢查(Code inspection): 軟體開發中對於原代碼進行審查以檢查其品質和可靠性。 L. Williams, L. Layman, and W. Krebs, Extreme Programming Evaluation Framework for Object-Oriented Languages, v. 1.4, tech. report TR-2004-18, North Carolina State Univ., 2004. Even if Xp fades in popularity 即使未來 XP 逐漸失去流行，但是 TDD 可能仍然持續存在。如果TDD進入學術領域，學生們可以帶著更好的紀律和更好的軟體設計和測試技能進入軟體開發組織， 從而提高軟體工程社區可靠地生產、重用和維護高品質軟體的能力。 NOTE 這篇論文很好的介紹了 TDD 的概念與歷史、工具、在 2005 年時的未來展望，同時結尾的論述也非常準確，2023 年的今天 XP 已經不再像 2002 那樣流行， 但是 TDD 仍然是重要的開發方式，並也發展了後續的 BDD 等。 Last edit 04-29-2023 18:24"
  },"/jekyll/2023-04-18-applying_isoiec25010.html": {
    "title": "Paper | Applying the ISO/IEC 25010 Quality Models to Software Product",
    "keywords": "software software_qualitiy standard Jekyll",
    "url": "/jekyll/2023-04-18-applying_isoiec25010.html",
    "body": "John Estdale &amp; Elli Georgiadou, (2018) Applying the ISO/IEC 25010 Quality Models to Software Product, IEEE. 1. Introduction 軟體開發過程中，我們著重的是如何交付軟體產品，但是潛在的購買者其實更在意軟體產品能為他們帶來的未來服務價值，也就是軟體能夠在實際使用中帶來的效益。ISO/IEC 25010: 2011 是評估軟體產品質量的重要標準，可以幫助我們確定軟體流程的交付表現以及提出改進建議。論文從以生命週期為導向的服務觀點出發，探討 ISO/IEC 25010 模型的範圍和解釋，也確認了購買者關注的其他重要軟體產品方面，並提出了可能需要的品質要求和評估方式。 2. ISO/IEC 250xx series: SQuaRE SQuaRE 標準的目標是幫助開發和獲取系統和軟件產品的人員規範和評價產品的質量要求，並建立一個支持質量測量的過程。過去的 ISO/IEC 9001 標準主要關注是否符合先規定的規格或要求，而現再更包含了客戶上的需求與期望。 在軟體品質特徵中可分為兩個模型： 使用品質(Quality in Use) 是指產品能夠在特定使用情境下，由特定用戶滿足其需求並達成其特定目標的程度。 產品品質(Product Quality) 則是指與軟體的靜態特性和電腦系統的動態特性相關的品質特徵，這些特徵包括了例如可靠性、效能、可維護性、可移植性等。 詳細的敘述與子項可見Note - ISO/IEC 在論文中將 ISO/IEC 25010: 2011 分為兩種類型的模型 3. Applying the ISO/IEC 25010: 2011 Quality Models 不像物理世界，有明確獨立的尺寸和明確定義的度量（如長度、質量、時間、電流、熱力學溫度、光強度等）。 軟件品質概念則有點模糊例如兼容性，重新定義和重組這些概念就是 ISO/IEC 25010 工作的一部分。 “a conceptual framework and not a ready-to-use solution” from Biscoglio and Marchetti (2015) define software quality evaluation plans in a conference paper. 應用品質管理中的 7Ms Model 來涵蓋軟體開發中的行為 7Ms Model 是品質管理中常使用的工具，其中的 M 可視為是品質管理中的基本要素，其中包含: Machine, Method, Material, Manpower, Management, Milieu, Measurement 4. Quality in Use 第4、5節則對 Table 1. 的結構來依序查看各個特性的定義，並分析與評估各個特性，若有需要則進一步評估子特性。 4.1 Achievement of Needs Effectiveness(效果), Efficiency(效率) ISO/IEC 25010 不包含對應用程式的具體功能與特色的評估，它們的價值僅能由採購方的組織的需要來評估。 因此這裡的效果與效率僅為當用戶實現特定目標時的指標。 Example: Apple Inc. 在定義 App store 時將這一需求描述為: “If your App doesn’t provide some form of lasting entertainment value, or is just plain creepy, it may not be accepted.” ; 「如果你的應用程式沒有提供持久的娛樂價值，或者很令人不舒服，它可能不會被接受。」 這就與 ISO/IEC 標準所討論的 stated and implied needs 有所偏差。 4.2 Freedom from Risk 講述對於不同行業上產品應該尊崇法律和法規的要求，尤其是它們可能在不同的時間，由不同的機構所制定因此經常是複雜並易發生衝突的。 但是產品供應者應該要了解在銷售區域內使用其產品的所有限制，並明確說明使用限制與假設。 論文中認為應該回復 ISO/IEC 過去的 Compli-ance 子項目已涵蓋對於法律、監管與合同相關的合符規範性要求。 5 Product Quality 這些關於 Product Quality(產品品質)的特正主要是開發者/供應商或在技術上有更多參與的購買者所關心的。 Functional suitability(功能適用性) 其子項 Completeness(完整性), Correctness(正確性), Appropriateness(適合性)這些都是開發者所進行的先行評估，但是最終評估還是在購買方的手上。 Performance efficiency(性能效能) 對於軟體開發上所進行的評估與測試可能不是最終交付後的實際運行環境，因此在開發期間進行的軟體產品測試可能無法完全反映最終交付後的表現。 5.1 Compatibility(兼容性) Co-existence(共存性)指產品能否與其他在同一平台上運行的產品協調共存，並使用所提供的服務共享資源，並且不要影響其他服務。 Interoperability(互相操作性)指產品與其他系統、產品或組件之間是否能夠交換信息。如在作業系統檔案區域中將檔案以滑鼠拖拉至產品檔案上傳區域。 5.2 Usability(易用性) 易用性下的子特性可以被理解為用戶如何識別一個產品是否適合它們的程度，所以包含了供應者如何描述系統，說明文件等等。 5.3 Reliability(可靠性) 可靠性關注的是產品的穩定性和持續性，需要考慮多種因素，包括硬件故障、人為錯誤、系統崩潰、網路中斷等等。 5.4 Security(安全性) 產品要確保如控制存取, 確保資料完整性, 保護機密等等的機制，例如 Apple store 上因為 APP 大多經由批准過因此基本保證使用者不會在商店中遭遇惡意軟體。 5.5 Maintainability(可維護性) 可維護性定義為能被預期的使用者有效的修改與維護的程度 Analyzability(可分析性)包括可預期的評估變化的影響，判斷缺失和故障，也就是在工作前的能做的評估。與現場工作中能注意到的問題，這需要有效的識別錯誤機制。 Testability(可測試性)可測試性是很重要的，其中包含 low-level 中的組件測試、單元測試。與 high-level 的測試(通過開發和維護回歸測試套件來確認服務沒有下降級別， 即性能下降至比之前更差的狀態)。 Extensibility(可擴張性) 是在先前[20]曾提出過一個應該增加的主要特性，提出模塊化和物件導向的軟體可以更容易的擴展， 並且 Tested classed 可以直接集成到現有系統中，無須重頭開始建構或測試整個系統。 [20] Siakas, K.V., Georgiadou, E.: PERFUMES: A Scent of Product Quality Characteristics. In: International Software Quality Management Conference, pp. 211-20. BCS, London (2005) 5.6 Portability(可移植性) 傳統上軟體的可移植性是指開發者對於 Source code 的再編譯、建構等。對於指定平台購買指定的版本，當轉移到新的平台時需要購買新的實現，並將數據、配置等轉移到新的環境中。 ISO/IEC 25010 的可移植性包含購買的產品能在預期的平台上運行，包含插件兼容，虛擬的或基於雲的環境。 Replaceability(可替換性)則指現有產品的一個版本被另一個版本替換，開發者應該考慮更廣泛的服務問題，如現有數據如何遷移與減少使用者介面的改面。 商業慣例是向前兼容就是任何新版本應該保持所有以前的功能與特性，沒有倒退。 5.7 Supportability(可支援性) 作者認為應該在 ISO 中加入的特性，與5.5 Maintainability類似是預測未來的一個因素，但與可維護性不同的是這代表的是一些開發方對購買者所需要的支援， 如: 緊急求救電話，遠程診斷不接觸 Source code 或其他私密訊息的支持; 信息或工具。這是開發方在競爭市場上提供高品質產品和服務的關鍵之一。 6. Other Characteristics of the Product at Delivery 作者認為在 SQuaRE 中其他重要的特性與影響 6.1 Honesty and Openness 有明確的要求要進行開放和誠實的溝通，既針對考慮購買的買方，也針對下載應用的使用者。在SQuaRE品質模型中，誤導性的文件並未被考慮進去。 如蘋果公司拒絕某些應用的原因包括: 「應用未能如開發人員所宣稱的那樣運作」 「應用包含未公開或與描述不一致的隱藏功能」 「旨在提供詐欺或假功能的應用，但未明確標示」。 6.2 Product Maturity 產品的成熟度生命週期: initial development(初期開發), active evolution(積極發展), servicing(維護與服務), phase out(淘汰). 追求可靠性的購買者更喜歡第三階段的產品，而更關注產品特性與功能的購買者可能更喜歡第二階段的產品。 一個產品會隨著時間的變得越來越複雜，除非進行減少複雜度的維護工作。在[23]中顯示了過去產品維護對於未來品質的強烈影響。 [23] Lehman M.M., Perry D.E. Ramil J.F.: Implication of Evolution Metrics on Software Maintenance. In: International Conf on Software Maintenance, pp. 208-17, IEEE (1998). doi: 10.1109/ICSM.1998.738510 7. In-life Experience, post-Deployment 最終對購買者來說重要的是產品在實際使用上的表現，但這個表現必須等到部屬或購買後才能知曉。 SQuaRE 目前沒有涉及這些方面，這部分都是上線後流程，如果買家要求產品供應商以協議的價格提供所有這些支持服務，則需要建立全面的額外服務品質指標。 7.1 Customer Support by Supplier 儘管支援服務是一種部署後活動，但供應商的支援能力仍然是軟體產品評估中重要的一個方面，即使它被明確地排除在 ISO/IEC 250xx 系列的範圍之外因為這只受到產品技術的輕微影響，應該至少要被提及。 7.2 In-life Maintenance 購買者需要知道供應商的政策和流程，以便預測和規劃維護產品的工作與成本。例如: 承諾修復錯誤，發布錯誤修復與新版本的品質與變化等。 8. Conclusions &amp; 9. Relation to SPI Manifesto 作者最主要的貢獻是討論了 ISO/IEC 25010 的範圍，並進一步的討論: 合法，法規與合約遵循性、可擴展性、支持性、誠實描述、產品成熟度和包括供應商客戶支持和部署後的維護在內的活動。 以此使 ISO/IEC 25010 成為更全面的品質模型，可用來評估規範是否應該改進、維持或忽略某些維度的基礎。 SPI Manifesto: Base improvement on experience and measurements. Pries-Heje J., Johanson J., (eds.): SPI Manifesto, http://www.iscn.com/Images/SPI_Manifesto_A.1.2.2010.pdf last accessed 2018/05/28 NOTE Last edit 04-21-2023 12:32"
  },"/jekyll/2023-04-13-software_standard.html": {
    "title": "Note | Standard - ISO/IEC",
    "keywords": "software software_qualitiy standard Jekyll",
    "url": "/jekyll/2023-04-13-software_standard.html",
    "body": "ISO/IEC 9126 Software engineering — Product quality was an international standard for the evaluation of software quality. Intorduction 如果想得到高品質的軟體，就要先能夠對於軟體產品的品質能夠進行完整的描述，因此國際標準組織發布的ISO/IEC就是常被引用的軟體產品品質(Software Product Qualitiy)標準。 這個標準的目標是去應對一些已知的人類偏見，這些偏見將可能會對軟體開發專案的交付和認知產生不利影響。這些偏見包括在項目開始後更改優先順序[1]或沒有任何清楚的成功[2]定義。 所以通過明確說明與確定項目的優先順序，將抽象的概念轉化為可衡量的數值或指標。並且輸出數據可以根據架構X(Schema X)[3]進行驗證，不需要任何干預。 目前最新的標準是 ISO/IEC 25010:2011 使用兩個模型來描述分別為 使用品質(Quality in Use) 與 產品品質(Product Quality)。 優先順序: 優先順序指的是軟體開發中各個任務、功能、需求等進行排序，以確定其相對重要性和優先級。 成功: 就是對於項目目標的清晰定義和明確衡量標準的確立，以便能夠確定項目是否已經達到預期的成效與目標。 架構X(Schema X): 任何特定的數據架構或格式，以驗證輸出數據是否符合標準。 Product Quality 功能適用性（Functional suitability） 功能完整性（Functional completeness）: 產品是否完整涵蓋了所有指定目標與任務的程度。 功能正確性（Functional correctness）: 產品提供具有所需精度或者相符的結果的程度。 功能適合性（Functional appropriateness）: 產品是否具有適當的功能完成指定工作的程度。 性能效能（Performance efficiency） 時間行為（Time behavior）: 指一個產品或系統在執行其功能時的反應與吞吐率。 資源利用率（Resource utilization）: 產品在執行功能時使用的資源數量與類型。 容量(Capacity): 指產品或系統參數的最大負載或工作量限制。 相容性（Compatibility） 共存（Co-existence）: 指一個產品在與產品共享環境與資源時，能有效執行其功能的程度，不會對其他產品產生負面影響。 互相操作性（Interoperability）: 指兩個或多個系統，產品間可以互相交換訊息並使用的程度。 可靠性（Reliability） 成熟度（Maturity）: 產品在正常運行下滿足可靠性要求的程度。 可用性（Availability）: 產品在運行下可操作與可訪問性的程度。 容錯能力（Fault tolerance）: 盡管存在硬體或軟體故障，但軟體系統、產品或組件依然按造預期的運行的程度。 可恢復性（Recoverability）: 當發生中斷或故障時，軟體或系統能夠直接回覆受影響的數據並重新建立系統所需狀態的程度。 易用性（Usability） 被識別的適當性（Appropriateness recognizability）: 用戶能夠識別產品或系統是否符合它們需求的程度。 易學習性（Learnability）: 產品或系統能夠使用戶在緊急情況下學習如何有效的使用他的程度。 吸引力（Operability）: 產品使用者能容易操作，控制與使用的程度。 用戶錯誤保護（User error protection）: 產品保護用戶不出錯的程度。 用戶介面美觀(User Interface aesthetics): 產品提供用戶介面美觀和滿意的程度。 可訪問性(Accessibility): 產品能使所有使用者都能使用的程度，如聽障、視障等特殊需求等。 安全性（Security） 保密性(Confidentiality): 確保產品能確保資料只能被授權的人員訪問的程度。 完整性(Integrity): 產品、系統資訊在處理的過程中不被未經授權就修改，保持完整與正確的程度。 抵抗賴性(Non-repudistion): 確保產品在能證明已經發生的事件，以便未來行為不能被當事人抵賴的情況。 可追溯性(Accountability): 確保系統能追蹤與記錄所有活動與事件，以便追蹤與追責。 真實性(Authenticity): 確保資訊、系統、用戶的真實性與可信性，避免假冒、詐欺、冒用等問題的程度。 可維護性（Maintainability） 模塊性(Modularity): 產品或系統由離散組件所完成，當組件產生更改對其他組件產生最小影響的程度。 可重複性(Reusability): 產品中的元件(類別、函數等)可被多次使用降低開發時間的程度。 可分析性(Analyzability): 產品可被容易的分析，發現潛在的問題或缺陷的程度。 可修改性(Modifiability): 產品可以容易地進行修改，並且這些修改可以不影響系統的其他部分。 可測試性(Testability): 產品可以容易地進行測試，以確保它的正確性和穩定性。 可移植性（Portability） 適應性(Adapatability): 產品是否能適應不斷發展的硬體；軟體或使用環境的程度。 易安裝性(Installability): 產品能否在不同環境下安裝與配置並進行運行的程度。 可替換性(Replaceability): 在相同環境中用戶能找到其他相同目的的指定軟體的程度。 Quality in Use 效果（Effectiveness） 用戶實現特定目標的準確性和完整性。 效率（Efficiency） 相對於用戶實現目標的準確性和完整性，所耗費的資源。 滿意度（Satisfaction） 實用性（Usefulness）: 指用戶對實現實用目標的實現與使用結果和使用後果的滿意度。 信任性（Trust）: 用戶或其他相關者對於產品或系統按預期行為的信任程度。 愉悅性（Pleasure）: 指用戶從滿足個人需求中獲得的愉悅程度。 舒適性（Comfort）: 使用者在使用過程中，對於其身體上的舒適感受的滿意程度。 無風險性（Freedom from Risk） 減輕經濟風險(Economic Risk Mitigation): 產品或系統在預期使用環境中減輕財務狀況、有效運營、商業財產、聲譽或其他資源的潛在風險的程度。 降低健康和安全風險(Health and Safety Risk Mitigation): 產品或系統在預期使用環境中降低對人體的潛在風險的程度。 降低環境風險（Environmental Risk Mitigation）: 產品或系統在其預期使用環境中減輕與環境相關的潛在風險的程度。 涵構覆蓋（Context coverage） 涵構完整性（Context Completeness）: 在所有指定的使用環境中，產品或系統可以有效、高效、無風險和滿意地使用的程度。 靈活性（Flexibility）: 產品或系統在需求中最初指定的範圍以外的環境中可以有效、高效、無風險和滿意地使用的程度。 Advanced John Estdale(2018), Applying the ISO/IEC 25010 Quality Models to Software Product, IEEE 這篇論文則對標準中的各項特性做了說明與更進一步的探討，閱讀筆記 Note Link NOTE Last edit 04-17-2023 12:32"
  },"/jekyll/2023-02-04-algorithm_kmp.html": {
    "title": "Leetcode | Algorithm - KMP",
    "keywords": "algorithm string Jekyll",
    "url": "/jekyll/2023-02-04-algorithm_kmp.html",
    "body": "Notes: KMP (Knuth Morris Pratt). reference here. Introduction 字串尋找演算法 (KMP), 可在一個字串中尋找另一個字串的出現位子, 使用這個算法的平均時間複雜度是 O(n+m), 其中 n 為字串的長度, m 為模式串的長度. Brute Force 我們先看看暴力解如何去解，假設有兩個字串: text: “aabaabaaf” pattern: “aabaaf” 最值觀的寫法就是從 text 開始做 for loop, 接下來逐字匹配 text, 如果 pattern 找到匹配失敗的 text 才往前移動 1 直到能將 pattern 匹配完或者 traverse text 結束. 在這種寫法下時間複雜度為 O(m*n), 因為至少每個 text 的 element 都要經過一次 pattern 的匹配過程. 也可以想像成一個滑動的窗口, 窗口的大小為 pattern size. 28. Find the Index of the First Occurrence in a String 中的暴力解法. func strStr(haystack string, needle string) int { var result int = -1 if len(haystack) &lt; len(needle) { return result } for i := 0; i &lt; len(haystack); i++ { if checkStr(haystack, needle, i) { result = i return result } } return result } func checkStr(haystack, needle string, index int) bool { for i := 0; i &lt;= len(needle); i++ { if i == len(needle) { return true } if index &gt;= len(haystack) { return false } if haystack[index] == needle[i] { index++ } else { return false } } return false } KMP Algorithm Diagram reference 因此我們可以將暴力解用圖解來展開： 那我們就能想像如何減少不必要的搜尋, 首先如下兩張圖: 如果右移後的 overlap 都無法比對, 下次比對時我們都可以先跳過這些 overlap. 但是看下面這個例子, 這次比對到的目標中含有符合的子串. 因此我們就有必要進一步的比對, 那這段 overlap 要如何找出來? 假設已經比對成功的部分是 p’, 他是 p 的一個子串, 那重疊部分就是 p’ tail 和右移 p’ 的head. 因此我們可以說 結合以上的兩種方式, 我們可以看到這個算法最終的樣子: Last Edit 04-02-2023 19:28"
  },"/jekyll/2023-01-12-gomoku_ai.html": {
    "title": "Note | Gomoku AI - Game Tree",
    "keywords": "algorithm game Jekyll",
    "url": "/jekyll/2023-01-12-gomoku_ai.html",
    "body": "練習基礎AI演算法的入門題目，博弈樹搜尋與啟發式演算法，順手完成一個 GUI 小遊戲。 Introduction 傳統的AI演算法題目，以非神經網路的方式來嘗試AI撰寫。睡不著乾脆更新下Blog，至少忙起來能讓我轉移一些注意力，順便做個筆記。 博弈樹(Game Tree)其實就是一種將賽局中所有可能展開後的 Tree，那其中的每個 Node 就代表著遊戲進行中的某個狀態， 那我們以 Tic-Tac-Toe (井字棋)來看，就會有 26830 種遊戲過程。 Tic-Tac-Toe 的棋盤大小不過才 3x3 就有 26830 種可能，那五子棋的棋盤為 15x15 一個簡易的算法是 15! 幾近是天文數字。 如果想以全部遍歷的方式來找出最佳解幾乎是不可能做到的事，那這時就需要一些策略去找出最佳路徑，所以我們會用到幾種算法。 Maximin algorithm. (對抗性的搜尋算法) Alpha Beta pruning. (剪枝算法) Heuristic. (啟發式算法) Maximin algorithm Maximin algorithm (極大極小值演算法)，是一種找出失敗的最大可能性下的最小值的演算法，這是 wikipedia 上的解釋。 簡單來說在一個五子棋博弈樹中，必然有兩個不同的玩家互相下棋，故博弈樹我們也可以分成玩家層與AI層來看待，因此在搜尋的過程中我們要去找尋每一層的最有利狀態， 最後去找到玩家下的最好的狀態下，AI能下出的最佳步驟。 the source picture 其實看上圖就能發現其實就跟 Max-Min-Heap 很相像，只是今天 Heap 是從下往上， 但 Tree 是從上往下，一個中序遍歷的搜尋方式。在搜尋過程中我們要去找到都是極大或極小的路徑， 但是如果我們真的去遍歷整個棋盤，時間複雜度將為 O(b2)， 在棋盤為15*15，深度為4的情況下，2254是一個非常慢的演算法。 Alpha Beta pruning 但是即便我們能做到棋盤的對抗搜尋，但是搜尋時間還是很慢，因此我們需要去做到剪枝的動作， 放棄不可能的分支做搜尋，把希望放在更有可能的分支上以做到更深的深度。在最糟的情況下， 時間複雜度將為 O(b2)，而在最佳的情況下有機會達到 O(sqrt(bd))， 實作上我們就是在遞迴時，如果當前層數為極大層，但是下下層出現一個比最大值還要小的節點我們就直接剪掉， 因為我們要找的是極大值，就不用去考慮更小的值了，同理運用在極小層。 Wikipedia中圖的第三層中的 7 節點其實也是要剪掉的，圖有錯誤。 Heuristic 但是即使我們能做到剪枝搜索時間還是平均需要 O(b3d/4)，但同時我們也知道初始的搜尋節點很重要， 如果一個棋盤如下圖，我們以一個2D Array來實現這個棋盤狀態，像是 x, y = 0, 0這樣的位置如果不會影響大局完全沒有搜尋的必要。 State Operate 五子 直接返回 活四 直接返回 活三 … 死三 … 因此我們可以在尋找搜尋節點以有鄰居的空位為主，並且加入一些評分機制來對這些位置排序： 把每次搜尋的位置 print 出來來觀察搜尋的順序與影響。除去五子活四這種直接將軍的局面， 剩下的依照分數排序來做搜尋，這樣我們能更快的找到最佳的路徑， 分數不符的路徑就將會被剪掉，把時間用來做到更深的搜尋層數。 要記住即便使用了啟發式函數，但是只要局面變得複雜，搜尋速度同樣會下降很多， 畢竟每做一個搜尋就是需要付出 Pd 的時間消耗，因此排序並消除不重要位置的步驟就很重要。 Note 寫輔助函數時的注意事項 在寫這些輔助函數的時候，要盡量減少做整個棋盤的搜尋動作，同時可以嘗試將棋盤視為字串， 這樣就能以 Regex 來處理連線確認將會大幅加快執行速度，尤其是如果你的棋盤 評估函數要仔細設計 評估函數；尋找的棋形；搜尋深度；是最直接的影響棋力的選項，如果AI的落子有問題， 通常應該往這幾個方向做修改，例如搜尋深度與評估函數設計不好就有可能使AI自殺的情形發生。 NOTE 之後更新，目前已完成一個簡易的算法在Github，希望能在過年前把它做到一個更滿意的狀態。"
  },"/jekyll/2022-11-24-git_commit.html": {
    "title": "Note | Commit Message Format",
    "keywords": "github tool Jekyll",
    "url": "/jekyll/2022-11-24-git_commit.html",
    "body": "說明 Git 提交時應注意的 Commit message format，建立良好的規範才能有效的合作 做 issue 的時候不應該一次 Commit 所有變動！應該獨立每個 Commit 不同意義的異動，這樣才能使 Commit 快速閱讀與程式碼間的關係。 每次 Commit 都是針對異動做說明，Why &amp; What。 每次 Commit 加上 issue 編號，方便追蹤後續問題。 Basic format Header: type: 代表 commit 的類別：feat, fix, docs, style, refactor, test, chore，必要欄位。 scope 代表 commit 影響的範圍，例如資料庫、控制層、模板層等等，視專案不同而不同，為可選欄位。 subject 代表此 commit 的簡短描述，不要超過 50 個字元，結尾不要加句號，為必要欄位。 Body: Body 部份是對本次 Commit 的詳細描述，可以分成多行，每一行不要超過 72 個字元。 說明程式碼變動的項目與原因，還有與先前行為的對比。 Footer: 填寫任務編號（如果有的話）. BREAKING CHANGE（可忽略），記錄不兼容的變動，以 BREAKING CHANGE: 開頭，後面是對變動的描述、 以及變動原因和遷移方法。 type 需規範好有哪些類別： feat: 新增/修改功能 (feature)。 fix: 修補 bug (bug fix)。 docs: 文件 (documentation)。 style: 格式 (不影響程式碼運行的變動 white-space, formatting, missing semi colons, etc)。 refactor: 重構 (既不是新增功能，也不是修補 bug 的程式碼變動)。 perf: 改善效能 (A code change that improves performance)。 test: 增加測試 (when adding missing tests)。 chore: 建構程序或輔助工具的變動 (maintain)。 revert: 撤銷回覆先前的 commit 例如：revert: type(scope): subject (回覆版本：xxxx)。 Example 一個簡單的 feat 範例 #Feat Example feat: message 信件通知功能 因應新需求做調整： 通知和 message 都要寄發每日信件， 通知和 message 都用放在同一封信裡面就好， 不然信件太多可能也不會有人想去看。 調整項目： 1. mail_template.php，新增 message 區塊。 2. Send_today_notify_mail.php，新增 取得每日 Message 邏輯。 3. Message_model_api.php，新增 $where 參數，以便取得每日訊息。 4. Message_api.php、Message_group_user_model_api.php，新增 *取得訊息使用者* 邏輯，以便撈取每日訊息。 issue #863 NOTE 只會寫程式只能代表你是一個 Coder，能跟一群人一起寫程式才是一個 Engineer。 What’s the Difference Between a Programmer, Coder, Developer, and Software Engineer?"
  },"/jekyll/2022-11-08-ai_csp.html": {
    "title": "Note | Constraint Satisfaction Problem",
    "keywords": "ai algorithm Jekyll",
    "url": "/jekyll/2022-11-08-ai_csp.html",
    "body": "Introduction to Artificial Intelligence Week 6 Notes CSPs Introduction Constraint Satisfaction Problem 其定義為一組物件，而這些物件需要滿足一定的限制或條件，而這個問題可能有很多的解。CSP 問題經常表現出高複雜性，需要結合啟發式搜尋和搜尋方法來在一個合理的時間內解決問題。 主要解法：通過識別違反約束的變量與值的組合消除大規模的搜索空間。 CSP related Linear programming Nonlinear progaramming Numerical analysis CSP application Operations research Network flows optimization problems CSP Define 定義一個 3-tuple(X, D, C) 其中 X = {X1, …, Xn} Finite set of variables. D = {D1, …, Dn} Nonempty domain of possible values for each variable. C = {C1, …, Cn} Finite set of constraints, Each constraint Ci limits the values that variables can take. Example Map coloring 就是一個經典的 CSP。每個區域即是變數，顏色便是值域，相鄰區域顏色不同是約束。 其中關於 Constraint 可被劃分為: unary constraint: 只約束單個 variable 的取值。如 SA 不能被染成綠色。 binary constraint: 兩個 variable 相關的約束。如 SA 不能與 WA 的顏色相同。 global constraint: 全局約束。如 Alldiff, 即約束中所有 variable 皆需取不同的值。 Arc Consistency example Backtracking search for CSPs 回朔搜索，用於 DFS 中。他每次為 Single variable 進行取值，當沒有合法的值可給某個 variable 時就進行回朔。 為使 search 變的高效，須解決以下問題: 下一步應該給哪個 variable 取值? 按照什麼順序取值? 每步 search 應該做怎樣的推理? Variable select: Minimum remaining values (MRV) 最少剩餘值啟發式: 選擇合法取值最少的 variable 開始。這樣選擇的 variable 可能很快地導致失敗，從而進行回朔。 Degree heuristic 鄰接度啟發式: 選擇與其他未取值 variable 約束最多的 variable 來試圖降低未來可能的分支。 Value order Least constraining value (最少限制值): 選擇 value 時優先選擇給鄰居 variable 留下最多選擇的分支。 Inference in CSPs 在 CSP 問題中，Algorithm 可以進行搜索，也可以做約束傳播 (The constraint propagation) 的推理 約束傳播: 使用約束來減少一個 variable 的合法取值範圍，從而影響與此 variable 有約束關係的另一個 variable 的取值。 (局部相容性) Node Consistency (節點相容) Single variable (In CSP Single node) 值域中的所有取值滿足他的 unary constraint. if SA = NA = {Red, Blue, Green}; SA != Blue, NA != Red, SA = {Green, Red}, NA = {Blue, Green}; Arc Consistency (弧相容) CSP 中某 variable range 所有取值滿足該 variable 的所有 binary constraint. 最常用的算法為 AC-3, 算法流程大致如下: Get all the constraint and turn each one into two arcs. Expmale: A &gt; B becomes A &gt; B and B &lt; A. Add all the arcs to a queue. Repeat until the queue is empty: 3.1. Take the first arc (𝑥, 𝑦), off the queue (dequeue). 3.2. For every value in the 𝑥 domain, there must be some value of the 𝑦 domain. 3.3. Make 𝑥 arc consistent with 𝑦. To do so, remove values from 𝑥 domain for which there is no possible corresponding value for 𝑦 domain. 3.4. If the 𝑥 domain has changed, add all arcs of the form (𝑘, 𝑥) to the queue (enqueue). Here 𝑘 is another variable different from 𝑦 that has a relation to 𝑥. Forward Checking 向前檢驗是最簡單的推理形式，只要 X variable 被賦值了，就向前檢驗過程對他做 Arc Consistency. 檢查對於每個通過約束與 X 相關的, 未賦予值的 Y variable, 從 Y 的 range 中消去與 X 不相容的值。 NOTE 待更新"
  },"/jekyll/2022-11-07-network_urat.html": {
    "title": "Note | Basic UART Concept",
    "keywords": "protocol Jekyll",
    "url": "/jekyll/2022-11-07-network_urat.html",
    "body": "修網路實作的時候，給學弟妹講解 URAT 跟怎麼在 Arduino 上實作準備的一些教材，簡單介紹下 UART 怎麼在 Arduino 中進行資料傳遞，實驗使用單芯線進行通訊。 How to transfer data between two computers? 5 Volts logic: Signal on transmission medium Metal : Square wave &amp; Sine ware Optical fiber : Light square ware Wireless : Electromagnetic waves The Universal Asynchronous Receiver/Transmitter (UART) UART這種簡單的通訊方式已經存在了幾十年，依然廣受歡迎。 In a world where technology can become obsolete very quickly Still enjoys immense popularity. Asynchronous communication UART 是非同步通訊，所代表的是通訊中兩個byte之間的空隙是不固定的，而一個byte中的bit間隔是固定的。因此發送端與接收端都要有相同的 UART 設定。 Capabilities and Characteristics a basic UART system provides robust, moderate-speed, full-duplex communication with only three signals: Tx (transmitted serial data), Rx (received serial data), and ground. 一個基本收送的UART傳輸，僅需要三個端口Tx, Rx, GND。但在這之前的前提是 Rx, Tx, 在相同的數據傳輸頻率。 Key Terms Start bit: The first bit of a one-byte UART transmission. It indicates that the data line is leaving its idle state. The idle state is typically logic high, so the start bit is logic low. The start bit is an overhead bit; this means that it facilitates communication between receiver and transmitter but does not transfer meaningful data. Stop bit: The last bit of a one-byte UART transmission. Its logic level is the same as the signal’s idle state, i.e., logic high. This is another overhead bit. Start bit 代表空閒結束，Srart bit 僅代表開始，不具實際數據。 Stop bit 代表傳輸結束，電位拉高等待下一次 Start。 上圖可以發現發送 10 bit 的資料但其實真正的數據僅有 8 bit。 Baud rate: The approximate rate (in bits per second, or bps) at which data can be transferred. Example: 9600-baud system，即 1 bit 需要 1/(9600 bps) ≈ 104.2 µs，注意不是實際上每秒傳送 9600 資料， 實際上有開銷 bit 的消耗。 Check bit(Parity bit) Parity bit: An error-detection bit added to the end of the byte. Odd or Even 如果要設定校正位，就多傳送一個Bit，並預先設計好 Odd or Even，如今天要傳送 00001110 而 even 即 校正為 1 這樣就會有偶數個 1。 可以看到如果加入 Parity bit，想要發送一個 byte 的成本也增加了 1 個 bit。 Synchronizing and Sampling the UART interface does not use a clock signal to synchronize the Tx and Rx devices. So how does the receiver know when to sample the transmitter’s data signal? 因為是非同步通訊，所以接收器的 clock 完全獨立於發送器 clock，只需要知道 Start bit 的開始位置後就能依照固定的 clock 進行接收。 Conclusion 所以當今天要設計一個UART單方向傳輸，僅用一條線就可以完成，以預設的頻率收接下來的 8 bit，當電位拉高代表一次傳送的結束，等待下次開始。 這部分可以設計一個 Finite-State Machine 來完成發送與接收的程式設計。 Reference Back to Basics: The Universal Asynchronous Receiver/Transmitter (UART) Practice One-Line-Communication Last Edit 11-17-2022 12:07"
  },"/jekyll/2022-11-05-docker_jekyll.html": {
    "title": "Note | Docker Build Github Pages",
    "keywords": "github tool Jekyll",
    "url": "/jekyll/2022-11-05-docker_jekyll.html",
    "body": "說明如何使用 Jekyll docker image 在不用熟悉 Ruby 與相關套件管理下，生成靜態文件。 之前就想找一個能用 Markdown 寫筆記的地方，過去都是寫在 Github 的 Repositories 裡面但是檔案一多起來想整理也不方便，那就自己寫個 Blog 當作紀錄， 剛好就趁這個機會把這次的內容當作第一篇紀錄。 Jekyll 是一個用 Ruby 寫的簡單靜態網頁生成器，但是目前我幾乎都是用 Lab 的電腦做事，平時也是遠端到上面，所以很直覺的就想用 Docker 來處理環境， 之後跑腳本把生成好的文件在推上 Github 就可以做好一次更新了。 Required: Docker image Jekyll/Jekyll Html, Javascript, CSS 剛開始就先找個模板來用，Jekyll themes上就有很多可以用的模板來用，像我用的就是使用 GitBook 風格的模板，同時有搜尋功能之後文章的找尋也會比較方便。 找到模板之後去把他 forks 到自己的儲存庫，clone 下來就可以開始修改了。 Docker Jekyll Jekyll 官方有一個 Docker image 所以拉這個 image 就可以了，裡面 Readme 教學寫得還蠻詳細的，只要把模板 volume 到 container 裡面就可以執行 Jekyll 生成。第一次運行安裝套件等等會花一點時間，之後啟動容器速度就快很多了。 之後再簡單寫個 bash script 這樣一個能快速生成的 Jekyll 環境就搭建完成了。 jekyll build 直接生成網頁 jekyll serve 生成網頁後運行在 localhost:4000 docker run \\ -v $WD:/srv/jekyll:z \\ -v /etc/localtime:/etc/localtime:ro\\ -p 4000:4000 \\ --name jekyll \\ -it jekyll/jekyll \\ jekyll serve 2&gt; /dev/null || docker start jekyll &amp;&amp; docker attach jekyll; Customize 之後就等文件生成好，同時記得設定 Github pages publishing source, 把發布源改到生成的目錄。這樣 github.io 的內容就直接指向這個目錄。然後就是一些自定義的小修改，這裡只要會一點 Js, Html 就可以搞定。 像我用的模板本來是舊的 post 優先，稍微改排序，預設字體，加入時間註記，這樣一個簡單的靜態網頁就完成了。 Change default font size: gitbook-plugin-fontsettings \"pluginsConfig\": { \"fontsettings\": { \"size\": 1, } } Add date in post: _layout/post.html change sort method: _includes/toc-date.htnl Remote theme 如果想要使用其他人在 Github Repository 上的 theme，可以使用 jekyll-remote-theme 這個套件，只要在 _config.yml 裡面加入以下內容。 remote_theme: owner/name plugins: - jekyll-remote-theme 然後再 Gemfile 裡面加入下面，就可以使用遠端模板來進行 Build 了。 source \"https://rubygems.org\" gem \"jekyll-remote-theme\" NOTE 之後想到要修改的再更新吧，可能加入留言系統、標籤之類的，目前這樣的靜態網頁我就很滿意了。"
  },"/jekyll/1970-01-01-virtual_machine_tool.html": {
    "title": "Tool | Virtual Machine",
    "keywords": "linux tool Jekyll",
    "url": "/jekyll/1970-01-01-virtual_machine_tool.html",
    "body": "Notes How to using virtual machine tool. 1. ESXI 1.1 ESXI 5.5 import OVF Environment: VMware Player 16, ESXI 5.5 因為 5.5 是非常老舊的版本了，所以在匯入 OVF 檔案時會有很多問題，這裡記錄下如何解決 因為 5.5 不支援 SHA256，改用 SHA1 的方式匯出 OVF 檔案 ovftool.exe --shaAlgorithm=SHA1 /path/to/the/original/ova_file.ova /path/to/the/new/ova/file-SHA1.ova Unspported hardware family ‘vmx-18’ .ovf 找到 &lt;VirtualSystemType&gt;vmx-18&lt;/VirtualSystemType&gt; 改成 &lt;VirtualSystemType&gt;vmx-8&lt;/VirtualSystemType&gt; 檔案 ovf_file.ovf 未通過完整性檢查，可能在傳輸過程中已損毀。 .mf 找到 SHA1(ovf_file.ovf)= 5ba21a516c7b499e22c3463e4bd0596ab5336c4e 刪除 No support for the virtual hardware device type ‘20’ ESXI 5.5 不支援 NVME Controller，下面整段替換 &lt;Item&gt; &lt;rasd:Address&gt;0&lt;/rasd:Address&gt; &lt;rasd:Description&gt;NVME Controller&lt;/rasd:Description&gt; &lt;rasd:ElementName&gt;nvmeController0&lt;/rasd:ElementName&gt; &lt;rasd:InstanceID&gt;3&lt;/rasd:InstanceID&gt; &lt;rasd:ResourceSubType&gt;vmware.nvme.controller&lt;/rasd:ResourceSubType&gt; &lt;rasd:ResourceType&gt;20&lt;/rasd:ResourceType&gt; &lt;/Item&gt; &lt;Item&gt; &lt;rasd:Address&gt;0&lt;/rasd:Address&gt; &lt;rasd:Description&gt;SCSI Controller&lt;/rasd:Description&gt; &lt;rasd:ElementName&gt;scsiController0&lt;/rasd:ElementName&gt; &lt;rasd:InstanceID&gt;3&lt;/rasd:InstanceID&gt; &lt;rasd:ResourceSubType&gt;lsilogic&lt;/rasd:ResourceSubType&gt; &lt;rasd:ResourceType&gt;6&lt;/rasd:ResourceType&gt; &lt;/Item&gt; invalid configuration for device ‘0’ 找到 &lt;vmw:Config ovf:required=\"false\" vmw:key=\"videoRamSizeInKB\" vmw:value=\"262144\"/&gt; 改成 ` ` 這樣應該就能成功部屬了，但是詳細會有什麼問題還沒測試過 2. Virtualbox 2.1 Vboxmanage Virtual box指令操作 手動相關指令說明: 新建一個名為「New VM」的虛擬機器 vboxmanage createvm -name 「New VM」 -register 設定「New VM」的記憶體是128MB並開啟acpi 設定第一開機碟為dvd 以及新增一個網路介面 vboxmanage modifyvm 「New VM」 -memory 「128MB」 -acpi on -boot1 dvd -nic1 intnet 建立一個虛擬硬碟名為「newhd.vdi」 大小為 4000MB vboxmanage createvdi -filename 「newhd.vdi」 -size 4000 -register 將「New VM」的 hda 設定為「newhd.vdi」虛擬磁碟 vboxmanage modifyvm 「New VM」 -hda 「newhd.vdi」 將在\"/home/file/iso.iso\"的ISO映像檔 設定到 名為 dvd的光碟映像檔庫 vboxmanage registerimage dvd /home/file/iso.iso 設定名為「New VM」的 dvd裝置為 /home/file/iso.iso vboxmanage modifyvm 「New VM」 -dvd /home/file/iso.iso 設定「New VM」所使用的 VRDP 的連接Port為 3390 vboxmanage modifyvm 「New VM」 -vrdpport 3390 啟動 VRDP VBoxVRDP -startvm 「New VM」 ----- List function 查詢目前vbox上有設定多少個vm vboxmanage list vms 查看支援的 OS Type vboxmanage list ostypes 查看運行中的 VM vboxmanage list runningvms 其它可以list的指令 vboxmanage list hostdvds vboxmanage list hostinfo vboxmanage list hddbackends vboxmanage list systemproperties vboxmanage list dhcpservers vboxmanage list hdds vboxmanage list dvds 指令啟動vm vboxmanage startvm \"VM name\" --type headless (用背景啟動，不加上--type headless參數可能會有錯誤!!) 2.2 Autostart Virtualbox VMs How to autostart virtual machine by systemctl. reference Create a systemd service file. sudo vim /etc/systemd/system/$service-filename [Unit] Description=VBox Virtual Machine %i Service Requires=systemd-modules-load.service After=systemd-modules-load.service [Service] Restart=always RestartSec=3 User=%u Group=vboxusers ExecStart=/usr/bin/VBoxHeadless -s %i ExecStop=/usr/bin/VBoxManage controlvm %i savestate [Install] WantedBy=multi-user.target change %i to you virtual machine UUID or name. %u is you virtual machine manager username. reload systemd service file and enable service. sudo systemctl daemon-reload sudo systemctl enable $service-filename sudo systemctl start $service-filename check virtual machine is runing vboxmanage list runningvms Last Edit 10-01-2023 12:22"
  },"/jekyll/1970-01-01-linux_config.html": {
    "title": "Tool | Linux Config",
    "keywords": "linux tool Jekyll",
    "url": "/jekyll/1970-01-01-linux_config.html",
    "body": "Notes Linux config Change username 要改使用者名稱通常很麻煩，這個是一個我覺得比較好的做法，多了 Link 至少讓沒改到的地方能連結到新的 home How To Change Username On Ubuntu, Debian, Linux Mint Or Fedora Create a temporary user sudo adduser tempuser sudo usermod -aG sudo tempuser Login with tempuser sudo usermod -l ${newusername} -d /home/${newusername} -m ${oldusername} sudo groupmod -n ${newusername} ${oldusername} Create a symbolic link sudo ln -s /home/${newusername} /home/${oldusername} Login new username and delete temporary user sudo userdel -r tempuser Disk mount Ext4 with LVM Resize Reduce LVM size Check disk mount df -h df -h /home # Filesystem Size Used Avail Use% Mounted on # /dev/mapper/debian--vg-home 23G 1.9G 20G 9% /home Unmount disk umount, then check and fix disk e2kfsck umount /dev/mapper/debian--vg-home e2kfsck -f /dev/mapper/debian--vg-home Resize file system resize2fs resize2fs /dev/mapper/debian--vg-home 80G # resize2fs 1.42.9 (28-Dec-2023) # Resizing the filesystem on /dev/mapper/debian--vg-home to 28321400 (4k) blocks. # The filesystem on /dev/mapper/debian--vg-home is now 28321400 blocks long. Reduce LVM size lvreduce, then check and fix LVM e2kfsck lvreduce -L 80G /dev/mapper/debian--vg-home e2kfsck -f /dev/mapper/debian--vg-home Mount disk mount mount /dev/mapper/debian--vg-home Extend LVM size Check physical volume pvdisplay and volume group vgdisplay pvdisplay # --- Physical volume --- # PV Name /dev/sda5 # VG Name debian-vg # PV Size 100.00 GiB / not usable 2.00 MiB # Allocatable yes (but full) # PE Size 4.00 MiB vgdisplay # --- Volume group --- # VG Name debian-vg # Free PE / Size 2560 / 9.99 GiB Extend LVM size lvextend Using percentage lvextend -l +100%FREE /dev/mapper/debian--vg-home Resize file system ext4 or ext3 use resize2fs resize2fs /dev/mapper/debian--vg-home xfs use xfs_growfs xfs_growfs /dev/mapper/debian--vg-home Check disk mount df -h df -h /home Firewall ufw (Uncomplicated Firewall) # 如果要允許連線指定 Port 可以輸入指令: sudo ufw allow &lt;port-number&gt; sudo ufw deny &lt;port-number&gt; # 只允許特定 IP 才能連線的話，可以輸入以下指令: sudo ufw allow from &lt;IP&gt; to any port &lt;port-number&gt; sudo ufw deny from &lt;IP&gt; to any port &lt;port-number&gt; # 只允許 Subnet 可以連線到，可以輸入以下指令: sudo ufw allow from &lt;IP-with-mask&gt; to any port &lt;port-number&gt; sudo ufw deny from &lt;IP-with-mask&gt; to any port &lt;port-number&gt; sudo ufw allow from 159.66.109.0/24 to any port 22 # 刪除已經建立的規則: sudo ufw status numbered # 知道指定編號可以輸入以下指令來刪除規則: sudo ufw delete &lt;rule-number&gt; iptables iptables 是最基本的防火牆工具，並且還可以設定 NAT、Port Forwarding 等功能，這裡有一個基本的腳本可以使用 iptables-script.sh。 要保存 iptables 設定的話在 debian 可以使用 iptables-persistent 來保存設定 設定好當前 rules 後 sudo bash -c iptables-save &gt; /etc/iptables/rules.v4, sudo bash -c ip6tables-save &gt; /etc/iptables/rules.v6 延伸閱讀: DebianFirewall, iptables-essentials Network config ubuntu wifi config $ sudo vim /etc/netplan/50-cloud-init.yaml # This file is generated from information provided by the datasource. Changes # to it will not persist across an instance reboot. To disable cloud-init's # network configuration capabilities, write a file # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following: # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wl0: optional: true access-points: \"SSID-NAME-HERE\": password: \"PASSWORD-HERE\" dhcp4: true debian static ip config sudo vim /etc/network/interface # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug ens160 iface ens160 inet static address ${address} netmask ${netmask} gateway ${gateway} # This an autoconfigured IPv6 interface iface ens160 inet6 auto sudo systemctl restart networking.service fix Wifi Mt7601u driver How to fix wireless adapter Mt7601u not working, Can run correctly on kernel 5.15. Reference git clone https://github.com/jeremyb31/mt7601u.git sudo dkms add ./mt7601u sudo dkms install mt7601u/1.0 ZeroTier Route Config Install ZeroTier and add route to between vLan and Physical Lan curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join $NETWORK_ID Add to zerotier network, and Authorize it at https://my.zerotier.com/network/$NETWORK_ID. Configure the ZeroTier managed route: At my.zerotier.com/network/$NETWORK_ID -&gt; Settings -&gt; Managed Routes, adds another route to every device joined to the ZeroTier network. Destination (Via) $PHY_SUB $ZT_ADDR 192.168.100.0/23 172.27.0.1 Edit /etc/sysctl.conf to uncomment net.ipv4.ip_forward. This enables forwarding at boot. sudo sysctl -w net.ipv4.ip_forward=1 # Add you network interface to shell variables. PHY_IFACE=eth0; ZT_IFACE=zt7nnig26 Add ip forwarding rules in iptables. sudo iptables -t nat -A POSTROUTING -o $PHY_IFACE -j MASQUERADE sudo iptables -A FORWARD -i $PHY_IFACE -o $ZT_IFACE -m state --state RELATED,ESTABLISHED -j ACCEPT sudo iptables -A FORWARD -i $ZT_IFACE -o $PHY_IFACE -j ACCEPT # Save iptables rules for next boot. sudo apt install iptables-persistent sudo bash -c iptables-save &gt; /etc/iptables/rules.v4 Last using sudo iptables -L -v to check iptables rules. Add Static Route Best to add a static route to the device to be connected to the ZeroTier network. 要連上 ZeroTier 的裝置加上 Static route 可以讓連線更穩定，不然有時候會發生 ping 不到的情況， 後來用 traceroute 才發現 ping 的路徑跑到別的網路去了。 Windows route add $PHY_SUB mask $PHY_SUB_MASK $PHY_GATEWAY Linux sudo ip route add $PHY_SUB via $PHY_GATEWAY Other ulimit ulimit 可以用來限制 shell 執行程式所需的資源 ulimit [options] [limit] ulimit Man Page Last Edit 06-11-2024 03:38 Add static route config."
  },"/jekyll/1970-01-01-leetcode_guide.html": {
    "title": "Leetcode | Master Guide",
    "keywords": "leetcode algorithm Jekyll",
    "url": "/jekyll/1970-01-01-leetcode_guide.html",
    "body": "我的刷題筆記，主要用於記錄自己在刷題過程中遇到的問題與解法。 Reference: leetcode-master, Python-LeetCode 581, codeforces-go, 灵茶山艾府: 如何科学刷题？ 以上參考從基礎的 Array 到 Dynamic Programming 都有詳細的解釋。 Array Binary Search 704. Binary Search 2070. Most Beautiful Item for Each Query 2601. Prime Subtraction Operation Hackerrank. Climbing the Leaderboard Easy Array 26. Remove Duplicates from Sorted Array 27. Remove Element 34. Find First and Last Position of Element in Sorted Array 35. Search Insert Position 57. Insert Interval 69. Sqrt(x) 283. Move Zeroes 287. Find the Duplicate Number 367. Valid Perfect Square 844. Backspace String Compare 977. Squares of a Sorted Array 1995. Count Special Quadruplets 2161. Partition Array According to Given Pivot Easy Array Matrix 36. Valid Sudoku 840. Magic Squares In Grid 861. Score After Flipping Matrix 2373. Largest Local Values in a Matrix Hackerrank. Forming a Magic Square Hackerrank. Queens Attack II Martix Prefix Sum 525. Contiguous Array 974. Subarray Sums Divisible by K 2145. Count the Hidden Sequences Sort 4. Median of Two Sorted Arrays 33. Search in Rotated Sorted Array 75. Sort Colors 80. Remove Duplicates from Sorted Array II 540. Single Element in a Sorted Array 786. K-th Smallest Prime Fraction 945. Minimum Increment to Make Array Unique 1051. Height Checker 1122. Relative Sort Array 1508. Range Sum of Sorted Subarray Sums 1636. Sort Array by Increasing Frequency 2037. Minimum Number of Moves to Seat Everyone 2233. Maximum Product After K Increments 2785. Sort Vowels in a String Hackerrank. Organizing Containers of Balls Spiral Matrix 209. Minimum Size Subarray Sum Spiral Matrix Backtracking Backtracking 17. Letter Combinations of a Phone Number 22. Generate Parentheses 37. Sudoku Solver 39. Combination Sum 46. Permutations 51.N Queens 52.N Queens Ii 77. Combinations 78. Subsets 79. Word Search 93. Restore IP Addresses 131. Palindrome Partitioning 1255. Maximum Score Words Formed by Letters 2597. The Number of Beautiful Subsets Binary Tree Binary Tree 100. Same Tree 101. Symmetric Tree 110. Balanced Binary Tree 112. Path Sum 222. Count Complete Tree Nodes 226.Invert Binary Tree 257. Binary Tree Paths 513. Find Bottom Left Tree Value 559.Maximum Depth Of N Ary Tree 572.Subtree Of Another Tree 783. Minimum Distance Between BST Nodes 1038. Binary Search Tree to Greater Sum Tree 1325. Delete Leaves With a Given Value 1367. Linked List in Binary Tree 1530. Number of Good Leaf Nodes Pairs 2096. Step-By-Step Directions From a Binary Tree Node to Another 2196. Create Binary Tree From Descriptions Binary Tree Level Order Traversal Build Binary Tree for Array Iteration Binary Tree Traversal Iteration Binary Trees Unified Method Binary tree N-ary Tree Traversal Recursion Binary traversal Other Operations 623. Add One Row to Tree 979. Distribute Coins in Binary Tree Bit Manipulation Bit Manipulation 201. Bitwise AND of Numbers Range 2419. Longest Subarray With Maximum Bitwise AND 2429. Minimize XOR Bit Manipulation Summary Dynamic Programming 0/1 Knapsack 0/1 Knapsack Problem 416. Partition Equal Subset Sum 474. Ones and Zeroes 494. Target Sum 1049. Last Stone Weight II Dynamic Programming Stack with DP 62. Unique Paths 70. Climbing Stairs 96. Unique Binary Search Trees 300. Longest Increasing Subsequence 343.Integer Break 509. Fibonacci Number 931. Minimum Falling Path Sum 1105. Filling Bookcase Shelves 1749. Maximum Absolute Sum of Any Subarray 2370. Longest Ideal Subsequence 2684. Maximum Number of Moves in a Grid Unbounded Knapsack 139. Word Break 140. Word Break II 279. Perfect Squares 377. Combination Sum IV 518. Coin Change II Unbounded Knapsack Problem Graph Breadth First Search 752. Open the Lock 1654. Minimum Jumps to Reach Home 2045. Second Minimum Time to Reach Destination Graph 514. Freedom Trail 787. Cheapest Flights Within K Stops 1219. Path with Maximum Gold 1382. Balance a Binary Search Tree 1791. Find Center of Star Graph 1971. Find if Path Exists in Graph 2092. Find All People With Secret 2285. Maximum Total Importance of Roads 2497. Maximum Star Sum of a Graph 2812. Find the Safest Path in a Grid 2924. Find Champion II Graph Search 200. Number of Islands 778. Swim in Rising Water 797. All Paths From Source to Target 2368. Reachable Nodes With Restrictions Shortest Path 743. Network Delay Time 1976. Number of Ways to Arrive at Destination 2976. Minimum Cost to Convert String I Topological Sort 310. Minimum Height Trees 2192. All Ancestors of a Node in a Directed Acyclic Graph Greedy Algorithm Greedy Algorithm 55. Jump Game 452. Minimum Number of Arrows to Burst Balloons 455. Assign Cookies 621. Task Scheduler 763. Partition Labels 948. Bag of Tokens 1509. Minimum Difference Between Largest and Smallest Value in Three Moves 2411. Smallest Subarrays With Maximum Bitwise OR Hash Hash 15. 3Sum 18. 4Sum 41. First Missing Positive 128. Longest Consecutive Sequence 202. Happy Number 242. Valid Anagram 268. Missing Number 349. Intersection of Two Arrays 383. Ransom Note 454. 4Sum II 506. Relative Ranks 791. Custom Sort String 873. Length of Longest Fibonacci Subsequence 916. Word Subsets 966. Vowel Spellchecker 1002. Find Common Characters 1481. Least Number of Unique Integers after K Removals 2342. Max Sum of a Pair With Equal Sum of Digits 2395. Find Subarrays With Equal Sum 2657. Find the Prefix Common Array of Two Arrays 3541. Find Most Frequent Vowel and Consonant Linkedlist Linkedlist 21. Merge Two Sorted Lists 23.Merge K Sorted Lists 24. Swap Nodes in Pairs 25.Reverse Nodes In K Group 83.Remove Duplicates From Sorted List 141. Linked List Cycle 142. Linked List Cycle II 143. Reorder List 160. Intersection of Two Linked Lists 203. Remove Linked List Elements 206. Reverse Linked List 707. Design Linked List 725. Split Linked List in Parts 2058. Find the Minimum and Maximum Number of Nodes Between Critical Points 2181. Merge Nodes in Between Zeros 2816. Double a Number Represented as a Linked List Middle Node of Linkedlist Math Math 43. Multiply Strings 84. Largest Rectangle in Histogram 976. Largest Perimeter Triangle 1518. Water Bottles 2364. Count Number of Bad Pairs 2579. Count Total Number of Colored Cells 2971. Find Polygon With the Largest Perimeter Hackerrank. Extra Long Factorials Hackerrank. Non-Divisible Subset Queue &amp; Stack Priority Queue 502. IPO 703. Kth Largest Element in a Stream 1046. Last Stone Weight 1642. Furthest Building You Can Reach 2530. Maximal Score After Applying K Operations 3066. Minimum Operations to Exceed Threshold Value II Queue &amp; Stack 2197. Replace Non-Coprime Numbers in Array Stack &amp; Queue 20. Valid Parentheses 150. Evaluate Reverse Polish Notation 225. Implement Stack using Queues 232. Implement Queue using Stacks 239. Sliding Window Maximum 347. Top K Frequent Elements 735. Asteroid Collision 921. Minimum Add to Make Parentheses Valid 950. Reveal Cards In Increasing Order 1047. Remove All Adjacent Duplicates In String 1190. Reverse Substrings Between Each Pair of Parentheses 1249. Minimum Remove to Make Valid Parentheses 1700. Number of Students Unable to Eat Lunch 1717. Maximum Score From Removing Substrings 1910. Remove All Occurrences of a Substring 2116. Check if a Parentheses String Can Be Valid Sliding Window Sliding Window 3. Longest Substring Without Repeating Characters 395. Longest Substring with At Least K Repeating Characters 643. Maximum Average Subarray I 713. Subarray Product Less Than K 992. Subarrays with K Different Integers 1456. Maximum Number of Vowels in a Substring of Given Length 1839. Longest Substring Of All Vowels in Order 2134. Minimum Swaps to Group All 1’s Together II 2444. Count Subarrays With Fixed Bounds 2461. Maximum Sum of Distinct Subarrays With Length K 2958. Length of Longest Subarray With at Most K Frequency 3254. Find the Power of K-Size Subarrays I String String 9.Palindrome Number 28. Find the Index of the First Occurrence in a String 71. Simplify Path 151. Reverse Words in a String 344. Reverse String 394. Decode String 459. Repeated Substring Pattern 539.Minimum_Time_Difference.md 541. Reverse String II 796. Rotate String 1400. Construct K Palindrome Strings 1653. Minimum Deletions to Make String Balanced 1935. Maximum Number of Words You Can Type 2108. Find First Palindromic String in the Array 2109. Adding Spaces to a String 3042. Count Prefix and Suffix Pairs I 3223. Minimum Length of String After Operations Easy Hash Easy String Hackerrank. Encryption Hackerrank. Repeated String Trie 14. Longest Common Prefix 208. Implement Trie (Prefix Tree) Two Pointer Two Pointer 5. Longest Palindromic Substring 19. Remove Nth Node From End of List 42. Trapping Rain Water 125. Valid Palindrome 442. Find All Duplicates in an Array 845. Longest Mountain in Array 881. Boats to Save People 1358. Number of Substrings Containing All Three Characters 1365.How Many Numbers Are Smaller Than The Current Number 1395. Count Number of Teams 1721. Swapping Nodes in a Linked List 1750. Minimum Length of String After Deleting Similar Ends 2149. Rearrange Array Elements by Sign 2570. Merge Two 2D Arrays by Summing Values Other Other 1. Two sum 2. Add Two Numbers 7. Reverse Integer 11. Container With Most Water 13. Roman to Integer 16. 3Sum_Closest 49. Group Anagrams 50. Pow(x, n) 67. Add Binary 118. Pascal’s Triangle 989. Add to Array-Form of Integer 1523. Count Odd Numbers in an Interval Range 1979. Find Greatest Common Divisor of Array Last Edit Leetcode 2025-09-16 12:41:55"
  },"/jekyll/1970-01-01-editor_envirmnment.html": {
    "title": "Tool | Edirot Guide",
    "keywords": "tool Jekyll",
    "url": "/jekyll/1970-01-01-editor_envirmnment.html",
    "body": "Notes editor environment VScode for Java Add Java jdk in vscode in setting.json \"java.configuration.runtimes\": [ { \"name\": \"JavaSE-17\", \"path\": \"/usr/bin/java\" }, ] Stop Java auto build disable java.autobuild.enabled Hide Run | Debug in editor Editor: Code Lens Controls whether the editor shows CodeLens. Hide Inlay Hints Editor › Inlay Hints: Enabled Enables the inlay hints in the editor. on -&gt; off [Vscode Debugger for C/C++ using GDB] Install gdb in Linux { \"name\": \"C++ Launch\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/target.out\", \"stopAtEntry\": false, \"customLaunchSetupCommands\": [ { \"text\": \"target-run\", \"description\": \"run target\", \"ignoreFailures\": false } ], \"launchCompleteCommand\": \"exec-run\", \"linux\": { \"MIMode\": \"gdb\", \"miDebuggerPath\": \"/usr/bin/gdb\" }, \"osx\": { \"MIMode\": \"lldb\" }, \"windows\": { \"MIMode\": \"gdb\", \"miDebuggerPath\": \"C:\\\\MinGw\\\\bin\\\\gdb.exe\" } } Last Edit 09-29-2023 12:22"
  }}
